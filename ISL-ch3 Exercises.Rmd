---
title: "ISLR CH3 Applied Exercises"
output: html_notebook
---


```{r ISLR CH3, applied exercise# 8}
auto.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/Auto.csv", header=T, stringsAsFactors = F, na.strings = "?")

str(auto.df)

auto.lm = lm(mpg ~ horsepower, data = auto.df)
summary.lm = summary(auto.lm)
summary.lm
print("--------- Summry is S3 object of class summary.lm ------------")
print("attributes of summary")
str(attributes(summary.lm))
sprintf("type of summary: %s ", typeof(summary.lm))

sprintf("names of summary:")
names(summary.lm)
fstatistic <- summary.lm$fstatistic

"-------- Calculate overall p-value using F distribution-----"
str(attributes(fstatistic))
sprintf("Type of fstatistic: %s", typeof(fstatistic))
stopifnot( fstatistic["value"] == fstatistic[[1]])
fstatistic["value"]
fstatistic[[1]]

fstatisticValue <- fstatistic[["value"]]
fstatisticNumDegreesOfFreedom <- fstatistic[["numdf"]]
fstatisticDenDegreesOfFreedom <- fstatistic[["dendf"]]
overallPValue = pf(fstatisticValue, fstatisticNumDegreesOfFreedom, fstatisticDenDegreesOfFreedom,lower.tail = FALSE)

"-------- Type differences --------------"
typeof(summary.lm[["r.squared"]]) # a double 
typeof(summary.lm["r.squared"]) # a list 
stopifnot(summary.lm[["r.squared"]] == summary.lm$r.squared)

rsquared <- summary.lm$r.squared
rse <- summary.lm$sigma # Clearly RSE is an estimate of population vriance
coefficients <- summary.lm$coefficients
sprintf("Type of coefficients: %s", typeof(coefficients))
sprintf("type of r.squared in summary: %s", typeof(rsquared))
sprintf("R squared: %f", rsquared)

sprintf("RSE (Standard deviation from population regression line) = %f", rse)
typeof(summary.lm$sigma) # double
typeof(summary.lm ["sigma"]) # list
percentageError = summary.lm$sigma/mean(auto.df$mpg)
sprintf(" i) Yes: F-statistics %.4f > 1 and overall p-values %.4f < 0.05", fstatisticValue, overallPValue)
sprintf(" ii) Deviation from population regression line is: %.4f%% and variablity explained by horsepower: %.f%%", floor(percentageError*100), floor(rsquared*100))
sprintf(" iii) Reltionship is negative:")
coefficients

sprintf("Confedence interval for coefficients: ")
confint(summary.lm)

sprintf(" iv) Predict mpg associated with a horsepower of 98 and show predicted value, confidence interval")
predict(auto.lm, data.frame(horsepower = c(98)) , interval = "confidence")
sprintf(" Predict mpg associated with a horsepower of 98 and show predicted value, prediction interval")
predict(auto.lm, data.frame(horsepower = c(98)) , interval = "prediction")

sprintf("b) plot response and predictor:")
plot(auto.df$mpg ~ auto.df$horsepower, col="red",xlab="horsepower",ylab="mpg") + abline(auto.lm, lwd=3, col="blue")

sprintf("c) create diagnostic plots: ")
par(mfrow=c(2,2))
plot(auto.lm)
par(mfrow=c(1,1))
sprintf("Problems with the fit are : 1_ dependency is not linear, 2_ heteroscedastisity, 3_ high leverage, 4_ high leverage outlier")

# find which observation in dataframe has rstudent > 3
sprintf (" rows in dataframe with rstudent >= 3:")
which(rstudent(auto.lm)>=3)

#plot high leverage points (hatmatrix)
n <- nrow(auto.df)
p <- ncol(auto.df)
plot(hatvalues(auto.lm), col=ifelse(hatvalues(auto.lm) > (p+1)/n, "red", "blue") ) +
  text(hatvalues(auto.lm), labels=ifelse(hatvalues(auto.lm) > (p+1)/n, names(which(hatvalues(auto.lm) > (p+1)/n)), ""))

#plot outliers against student residual (rstudent)
plot(predict(auto.lm), rstudent(auto.lm), col=ifelse(rstudent(auto.lm) >= 3, "red", "blue")) +
  text(predict(auto.lm), rstudent(auto.lm), labels = ifelse(rstudent(auto.lm) >= 3, which(rstudent(auto.lm) >= 3), ""))

#plot hatvalues against outliers
plot(hatvalues(auto.lm), rstudent(auto.lm), col=ifelse( hatvalues(auto.lm) > (p+1)/n || rstudent(auto.lm) >= 3, "red", "blue") )

```

```{r ISLR CH3, applied exercise# 9}
auto.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/Auto.csv", header=T,  na.strings = "?")

sprintf("a) scatter plot matrix")
my_cols <- c("#00AFBB", "#E7B800", "#FC4E07")  
pairs(auto.df, pch = 19,  cex = 0.5,
      col = my_cols[auto.df$origin],
      lower.panel=NULL)
# pairs(auto.df, pch = 19, lower.panel = NULL)

sprintf("b) compute the matrix of correlations between the variables:")

# First remove "name" column from dataframe
drops <- c("name")
newDf <- auto.df [, !(names(auto.df) %in% drops)]
str(newDf)
# Now find a subset of records that have at least one NA 
dfSubsetWithNa <- newDf[rowSums(is.na(newDf)) > 0,]
head(dfSubsetWithNa)

# Check a particular column that has NA and include the record
dfSubsetWithNaInOneCol <- newDf[is.na(newDf$horsepower), ]
head(dfSubsetWithNaInOneCol)

# get a subset of records in dataframe with no NA in any column:
dfSubsetWithNoNa <- newDf[rowSums(is.na(newDf)) == 0, ]
head(dfSubsetWithNoNa)

cor(dfSubsetWithNoNa[,])

# use GGally to visualize the correlation between all predictors in 
GGally::ggcorr(dfSubsetWithNoNa)

# fit a model
df.lm <- lm (mpg ~ ., data = dfSubsetWithNoNa)
summary.ml <- summary(df.lm)

# overall p-value
fstatistic = summary.ml[["fstatistic"]]
fstatisticValue <- fstatistic[["value"]]
fstatisticNumDegreesOfFreedom <- fstatistic[["numdf"]]
fstatisticDenDegreesOfFreedom <- fstatistic[["dendf"]]
overallPValue = pf(fstatisticValue, fstatisticNumDegreesOfFreedom, 
                   fstatisticDenDegreesOfFreedom,lower.tail = FALSE)


sprintf("i) Yes because: F-statistics %.4f,  p-value: %.4f < 2.2e-16: ", fstatisticValue, overallPValue)
sprintf("ii) displacement, weight, year, origin")
sprintf("iii) miles per galon increases as year goes by")

# diagnostic plots for linear regression fit:
plot(df.lm)
sprintf("d) nonlinearity and heteroschedasticity")
sprintf(" High leverage points are rows 327 and 394 because there are above cook's line")
sprintf(" Also 323 and 327 are the outliers, thus 327 is high levrage and outlier")
# plot high leverage points
n <- nrow(dfSubsetWithNoNa)
p <- ncol(dfSubsetWithNoNa)
plot(hatvalues(df.lm), col=ifelse(hatvalues(df.lm) > (p+1)/n, "red", "blue"))+
  text(hatvalues(df.lm), labels=ifelse(hatvalues(df.lm) > (p+1)/n, names(which(hatvalues(df.lm) > (p+1)/n)), ""))
     
# plot outliers
plot(rstudent(df.lm), predict(df.lm), col=ifelse(rstudent(df.lm)>3, "red", "blue"))+
  text(rstudent(df.lm), predict(df.lm), labels = ifelse(rstudent(df.lm)>3, names(which(rstudent(df.lm)>3)), ""))

sprintf("e) find all possible interactions first")

"fit a model with all interactions"
df.lm2 <- lm (mpg ~ .^2, data = dfSubsetWithNoNa)
summary.ml2 <- summary(df.lm2)
summary.ml2

# Let's find statistically significant  coefficients with p-value less than 0.05
library(broom)
library(dplyr)
coeffs <- tidy(df.lm2) %>%
  select(term, estimate, std.error, statistic, p.value) %>%
  filter(p.value <= 0.05)

coeffs
sprintf("f) try different transformation on statistically significant variables")

"Fit a model with only those statistically significant variables"
df.lm3 <- lm (mpg ~ displacement + acceleration + origin + displacement:year + acceleration:year + acceleration:origin, data = dfSubsetWithNoNa)
summary(df.lm3)
# plot diagnostic graphs
plot(df.lm3)

# first regress mpg over displacement and plot fitted vs residual to see if there is any bend shape
mpg_displacement.lm <- lm(mpg ~ displacement, data = dfSubsetWithNoNa)
plot(mpg_displacement.lm)
# clearly we can see a curve shape and heteroscedasticity, thus let's use displacement^2 to see any improvement

mpg_displacementSquared.lm <- lm(mpg ~ I(displacement^2), data = dfSubsetWithNoNa)
mpg_displacementSqrt.lm <- lm(mpg ~ I(displacement^0.5), data = dfSubsetWithNoNa)
mpg_displacementLog.lm <- lm(mpg ~ log1p(displacement), data = dfSubsetWithNoNa)

displacementMin <- min(dfSubsetWithNoNa[["displacement"]])
displacementMax <- max(dfSubsetWithNoNa[["displacement"]])
displacementvalues  <- seq(displacementMin, displacementMax, 0.01)

newDisplacementDf <- data.frame(displacement=c(displacementvalues))

# Now predict new mpg for new displacement values for displacement squared
new_mpg_for_squared <- predict(mpg_displacementSquared.lm, newDisplacementDf)
new_mpg_for_sqrt <- predict(mpg_displacementSqrt.lm, newDisplacementDf)
new_mpg_for_log <- predict(mpg_displacementLog.lm, newDisplacementDf)

plot( dfSubsetWithNoNa[["displacement"]], dfSubsetWithNoNa[["mpg"]], xlab = "displacement", ylab="mpg", main = "displacement against mpg with new prediction")+
  lines(displacementvalues, new_mpg_for_squared, col="red") + 
  lines(displacementvalues, new_mpg_for_sqrt, col="blue",lty=2) + 
  lines(displacementvalues, new_mpg_for_log, col="green",lty=3)
  
# change displacement -> displacement^2 improve f statistics and Rsquered
df.lm4 <- lm (mpg ~ I(displacement^2) + acceleration + origin + displacement:year + acceleration:year + acceleration:origin, data = dfSubsetWithNoNa)
summary(df.lm4)


#change acceleration -> log(acceleration) reduce Rsquared and increase RSE thus it does not help
df.lm5 <- lm (mpg ~ I(displacement^2) + log1p(acceleration) + origin + displacement:year + acceleration:year + acceleration:origin, data = dfSubsetWithNoNa)
summary(df.lm5)

# change origin -> log(origin) does not change the result 
df.lm6 <- lm (mpg ~ I(displacement^2) + acceleration + log1p(origin) + displacement:year + acceleration:year + acceleration:origin, data = dfSubsetWithNoNa)
```


```{r ISLR CH3, applied exercise# 10}
carseats.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/Carseats.csv", header=T,  na.strings = "?", stringsAsFactors = F)

# summary(carseats.df)

# Now find a subset of records that have at least one NA 
carseats.dfWithNa <- newDf[rowSums(is.na(carseats.df)) > 0,]
sprintf(" Number of observations containing NA is %d ",nrow(carseats.dfWithNa))

# Check a particular column that has NA and include the record
# dfSubsetWithNaInOneCol <- newDf[is.na(newDf$horsepower), ]
# head(dfSubsetWithNaInOneCol)

# now that there is no Na, we can safely convert US and Urban columns into factor

carseats.df[["ShelveLoc_factor"]] <- factor(carseats.df[["ShelveLoc"]], 
                                            levels = c("Good", "Medium", "Bad"))
carseats.df[["Urban_factor"]] <- factor(carseats.df[["Urban"]], levels = c("Yes", "No"))
carseats.df[["US_factor"]] <- factor(carseats.df[["US"]], levels = c("Yes", "No"))

sprintf(" -------------- contrats carseats.df[['ShelveLoc_factor']] -------------------")
contrasts(carseats.df[["ShelveLoc_factor"]])
table(carseats.df[["ShelveLoc_factor"]])

sprintf(" -------------- contrats carseats.df[['Urban_factor']] -------------------")
contrasts(carseats.df[["Urban_factor"]])
table(carseats.df[["Urban_factor"]])

sprintf(" -------------- contrats carseats.df[[US_factor]] -------------------")
contrasts(carseats.df[["US_factor"]])
table(carseats.df[["US_factor"]])


carseats.lm <- lm(Sales ~ Price + Urban_factor + US_factor, data = carseats.df)
summary(carseats.lm)
sprintf("base is urbon = No and also Us = No" )
sprintf(" In urbon area sales increases by 2%% ")
sprintf(" outside US sales decreases by 12%% or equivalently in US increases by 12%%")

sprintf(" model that only uses the predictors for which there is evidence of association")

carseats.lm.smaller <- lm(Sales ~ Price + US_factor, data = carseats.df)
summary1 <- summary(carseats.lm.smaller)
sprintf("F-statisitcs improved , RSE slightly improved but Rsquared did not improve that much")

coeffsMatrix <- coef(summary1)
coeffsMatrix
originalConfInt <- confint(carseats.lm.smaller)
myConfInt <- cbind(coeffsMatrix[,1]-coeffsMatrix[,2]*2, coeffsMatrix[,1]+coeffsMatrix[,2]*2)
originalConfInt
myConfInt

plot(carseats.lm.smaller)
sprintf("Outliers are observations 69, 51, 377")
sprintf("High Leverage points are 26, 50 and 368")
sprintf("There is no high leverage point which is also outlier")

# "outliers" 
# plot(predict(carseats.lm.smaller),rstudent(carseats.lm.smaller),
#      col=ifelse(rstudent(carseats.lm.smaller)>3,"red","black"))
# 
# "High leverage"
# n <- nrow(carseats.df)
# p <- ncol(carseats.df)
# plot(hatvalues(carseats.lm.smaller), 
#      col=ifelse(hatvalues(carseats.lm.smaller) > (p+1)/n, "red", "blue"))+
#   text(hatvalues(carseats.lm.smaller), 
#        labels=ifelse(hatvalues(carseats.lm.smaller) > (p+1)/n, 
#                      names(which(hatvalues(carseats.lm.smaller) > (p+1)/n)), ""))
```


```{r ISLR CH3, applied exercise# 13}
# if (!require("pacman")) install.packages("pacman") 
# p_load(datasets, ggplot2, ggthemes, dplyr, RColorBrewer, grid)
# data(airquality)

set.seed(1)
x <- rnorm(100)
eps <- rnorm(100, 0, 0.25)
y <- -1 + 0.5*x + eps

plot(x, y, col="red")

sprintf("fit a least square model to model with noise variance 0.25")  
# first create a datafrme from x and y
data.df <- data.frame(x1 = x, y1 = y)
data.lm <- lm(y1 ~ x1, data = data.df)
summary.lm <- summary(data.lm)
summary.lm
coef(summary.lm)
confint(data.lm)
# draw least square line and population regression line
plot(x, y, col="black") + 
  abline(coef = c(-1, 0.5), col="red")+
  abline(coef = coef(data.lm), col="blue")

# redo the above with less noise in the data
sprintf("fit a least square model to model with noise variance 0.05")  
eps1 <- rnorm(100, 0, 0.05)
yy <- -1 + 0.5 * x + eps1

plot(x, yy, col="black")
# fit a least square model 
# first create a datafrme from x and y
data.df2 <- data.frame(x2 = x, y2 = yy)
data.lm2 <- lm(y2 ~ x2, data = data.df2)
summary.lm2 <- summary(data.lm2)
summary.lm2
coef(summary.lm2)
confint(data.lm2)
sprintf("RSE significantly reduced and F2 increased when we decreased error also coefficients gets much closer to real values")

plot(x, yy, col="black") + 
  abline(coef = c(-1, 0.5), col="red")+
  abline(coef = coef(data.lm2), col="blue")


# redo the above with more noise in the data
sprintf("fit a least square model to model with noise variance 0.8")  
eps2 <- rnorm(100, 0, 0.8)
yy2 <- -1 + 0.5 * x + eps2

plot(x, yy2, col="black")
# fit a least square model 
# first create a datafrme from x and y
data.df3 <- data.frame(x3 = x, y3 = yy2)
data.lm3 <- lm(yy2 ~ x3, data = data.df3)
summary.lm3 <- summary(data.lm3)
summary.lm3
coef(summary.lm3)
confint(data.lm3)
sprintf("RSE significantly increased and F2 decreased when we increased error also coefficients gets much closer to real values")

plot(x, yy2, col="black") + 
  abline(coef = c(-1, 0.5), col="red")+
  abline(coef = coef(data.lm3), col="blue")

sprintf("Confedence interval for noisier data: ")
confint(data.lm3)

sprintf("Confedence interval for less noisy data: ")
confint(data.lm2)

sprintf("For less noisy data has smaller confedence interval than noisier data")
```


```{r ISLR CH3, applied exercise# 14}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5*x1 + rnorm(100)/10
y <- 2+2*x1+0.3*x2+rnorm(100)

sprintf("corrolation between x1 and x2:")
cor(x1,x2)
plot(x1,x2)

sprintf("Fit a least square model y ~ x1 + x2: ")
lm1 <- lm(yc ~ xc1 + xc2, data = data.frame(xc1 = x1, xc2 = x2, yc = y))
summary(lm1)

sprintf("Fit a least square model y ~ x1: ")
lm2 <- lm(yc ~ xc1, data = data.frame(xc1 = x1, yc = y))
summary(lm2)

sprintf("Fit a least square model y ~ x2: ")
lm3 <- lm(yc ~ xc2, data = data.frame(xc2 = x2, yc = y))
summary(lm3)

sprintf("No contradiction: there is a correlation between x1 and x2, if one is there the other do not add much information (each one alone adds info)")
# add new information to existsing data
x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y1 <- c(y,6)

sprintf("Fit a least square model y ~ x1 + x2 with new data: ")
lm1 <- lm(yc ~ xc1 + xc2, data = data.frame(xc1 = x1, xc2 = x2, yc = y1))
summary(lm1)

sprintf("Fit a least square model y ~ x1  with new data: ")
lm2 <- lm(yc ~ xc1, data = data.frame(xc1 = x1, yc = y1))
summary(lm2)

sprintf("Fit a least square model y ~ x2  with new data: ")
lm3 <- lm(yc ~ xc2, data = data.frame(xc2 = x2, yc = y1))
summary(lm3)

"new data (observation 101) is only high leverage in y1 ~ x1 + x2 model"
plot(lm1)

"new data (observation 101) is outlier and high leverage in y1 ~ x1 model"
plot(lm2)

"new data (observation 101) is only high leverage in y1 ~ x2 model"
plot(lm3)

sprintf(" as before there is a correlation between x1 and x2, if one is there the other do not add much information (each one alone adds info)")


```

```{r ISLR CH3, applied exercise# 15}
boston.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/Boston.csv", header=T,  na.strings = "?", stringsAsFactors = T)
# summary(boston.df)
str(boston.df)

# count number of observations with NA
boston.df.WithNa <- boston.df[rowSums(is.na(boston.df)) > 0, ]
# str(attributes(is.na(boston.df)))
sprintf(" Number of observations containing NA is %d ",nrow(boston.df.WithNa))

# for each predictor fit a model
attach(boston.df)
lm1 <- lm(crim ~ zn, data = boston.df)
summary(lm1)
plot(crim, zn)+abline(coef = coef(lm1), col="blue")
lm2 <- lm(crim ~ indus, data = boston.df)
summary(lm2)
plot(crim, indus)+abline(coef = coef(lm2), col="blue")
lm3 <- lm(crim ~ chas, data = boston.df)
summary(lm3)
plot(crim, chas)+abline(coef = coef(lm3), col="blue")
lm4 <- lm(crim ~ nox, data = boston.df)
summary(lm4)
plot(crim, nox)+abline(coef = coef(lm4), col="blue")
lm5 <- lm(crim ~ rm, data = boston.df)
summary(lm5)
plot(crim, rm)+abline(coef = coef(lm5), col="blue")
lm6 <- lm(crim ~ age, data = boston.df)
summary(lm6)
plot(crim, age)+abline(coef = coef(lm6), col="blue")
lm7 <- lm(crim ~ dis, data = boston.df)
summary(lm7)
plot(crim, dis)+abline(coef = coef(lm7), col="blue")
lm8 <- lm(crim ~ rad, data = boston.df)
summary(lm8)
plot(crim, rad)+abline(coef = coef(lm8), col="blue")
lm9 <- lm(crim ~ tax, data = boston.df)
summary(lm9)
plot(crim, tax)+abline(coef = coef(lm9), col="blue")
lm10 <- lm(crim ~ ptratio, data = boston.df)
summary(lm10)
plot(crim ,ptratio)+abline(coef = coef(lm10), col="blue")
lm11 <- lm(crim ~ black, data = boston.df)
summary(lm11)
plot(crim, black)+abline(coef = coef(lm11), col="blue")
lm12 <- lm(crim ~ lstat, data = boston.df)
summary(lm12)
plot(crim, lstat)+abline(coef = coef(lm12), col="blue")
lm13 <- lm(crim ~ medv, data = boston.df)
summary(lm13)
plot(crim, medv)+abline(coef = coef(lm13), col="blue")

sprintf("a ) There is no strong relationship between chas and crime")

boston.lm <- lm(crim ~ ., data = boston.df)
summary(boston.lm)

sprintf("b_) For all predictors except: indus, chas, nox, rm, age, tax, ptratio, lstat we can reject null hypothesis")

xs <- c(coef(lm1)[2],coef(lm2)[2],coef(lm3)[2],coef(lm4)[2],coef(lm5)[2],coef(lm6)[2],coef(lm7)[2],coef(lm8)[2],
  coef(lm9)[2],coef(lm10)[2],coef(lm11)[2],coef(lm12)[2],coef(lm13)[2])

coefs <- coef(boston.lm)
ys <- coef(boston.lm)[-1] # drop intercept
length(ys)
plot(xs, ys)

sprintf(" Fit a model of the form b0 + b1*X + b2*X^2 + b3*X^3 ")

lm1 <- lm(crim ~ zn + I(zn^2) + I(zn^3), data = boston.df)
summary(lm1)
lm2 <- lm(crim ~ indus + I(indus^2) + I(indus^3), data = boston.df)
summary(lm2)
lm3 <- lm(crim ~ chas + I(chas^2) + I(chas^3), data = boston.df)
summary(lm3)
lm4 <- lm(crim ~ nox + I(nox^2) + I(nox), data = boston.df)
summary(lm4)
lm5 <- lm(crim ~ rm + I(rm^2) + I(rm^3), data = boston.df)
summary(lm5)
lm6 <- lm(crim ~ age + I(age^2) + I(age^3), data = boston.df)
summary(lm6)
lm7 <- lm(crim ~ dis + I(dis^2) + I(dis^3), data = boston.df)
summary(lm7)
lm8 <- lm(crim ~ rad + I(rad^2) + I(rad^3), data = boston.df)
summary(lm8)
lm9 <- lm(crim ~ tax + I(tax^2) + I(tax^3), data = boston.df)
summary(lm9)
lm10 <- lm(crim ~ ptratio + I(ptratio^2) + I(ptratio^3), data = boston.df)
summary(lm10)
lm11 <- lm(crim ~ black + I(black^2) + I(black^3), data = boston.df)
summary(lm11)
lm12 <- lm(crim ~ lstat + I(lstat^2) + I(lstat^3), data = boston.df)
summary(lm12)
lm13 <- lm(crim ~ medv + I(medv^2) + I(medv^3), data = boston.df)
summary(lm13)
```




```{r rowSum, and row cherry picking from a dataframe}
x <- cbind(x1 = 3, x2 = c(4:1, 2:5))
x
rowSums(x); colSums(x)

# choose rows of dataframe using an array od logicals
df <- data.frame(x = c(1,2,NA, 3, 4, NA), y=1:6)
df[c(F,T,T,F,F,T), ]
```

