---
title: "ISL Ch5 Lab and Exercises"
output:
  pdf_document: default
  html_notebook: default
---

```{r sample()}
library(tidyverse)
set.seed(1)
x <- 1:12
# a random permutation
is.integer(sample(x))
# bootstrap resampling -- only if length(x) > 1 !
sample(x, replace = TRUE)

# 100 Bernoulli trials
sample(c(0,1), 100, replace = TRUE)

# create divide a dataframe into folds using samples:

(theDf <- tibble(x=1:56))
k = 10
folds <- sample(1:k, size = nrow(theDf), replace = T) #10 fold CV
table(folds)

# folds with same size
sameSizefolds <- sample(rep(1:k, length.out = nrow(theDf)), size = nrow(theDf), replace = F)
table(sameSizefolds)

# train data set
theDf[folds != 3, ]

# test data set 
theDf[folds == 3, ]



## More careful bootstrapping --  Consider this when using sample()
## programmatically (i.e., in your function or simulation)!

# sample()'s surprise -- example
# x <- 1:10
#     sample(x[x >  8]) # length 2
#     sample(x[x >  9]) # oops -- length 10: If x has length 1, is numeric and x >= 1, sampling via sample takes place from 1:x
#     sample(x[x > 10]) # length 0
# 
# ## safer version:
# x[sample.int(length(x)) >  8] # length 2
# x[sample.int(length(x)) >  9] # length 1
# x[sample.int(length(x)) >  10] # length 0
# 
# ## R 3.x.y only
# sample.int(1e10, 12, replace = TRUE)
# sample.int(1e10, 12) # not that there is much chance of duplicates

```


```{r deplyr sample}
library(tidyverse)
# Sample fixed number per group
auto.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/Auto.csv", header=T, stringsAsFactors = T, na.strings = "?")

(auto.df = as_tibble(auto.df))
(by_cyl <- auto.df %>% 
  group_by(year))

sample_n(auto.df, 10)
sample_n(auto.df, 50, replace = TRUE)
sample_n(auto.df, 10, weight = as.integer(mpg))

sample_n(by_cyl, 3)
sample_n(by_cyl, 10, replace = TRUE)
sample_n(by_cyl, 3, weight = mpg / mean(mpg))

# Sample fixed fraction per group
# Default is to sample all data = randomly resample rows
sample_frac(auto.df)

sample_frac(auto.df, 0.1)
sample_frac(auto.df, 1.5, replace = TRUE)
sample_frac(auto.df, 0.1, weight = 1 / mpg)

sample_frac(by_cyl, 0.2)
sample_frac(by_cyl, 1, replace = TRUE)

```



```{r Validation set approach}
library(tidyverse)

auto.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/Auto.csv", header=T, stringsAsFactors = T, na.strings = "?")
(auto.df = as_tibble(auto.df))

# Check a particular column that has NA and include the record
# dfSubsetWithNaInOneCol <- auto.df[is.na(auto.df$Directions), ]
# head(dfSubsetWithNaInOneCol)

# get a subset of records in dataframe with no NA in any column:
auto.df <- auto.df[rowSums(is.na(auto.df)) == 0, ]

# Now find a subset of records that have at least one NA 
auto.df[rowSums(is.na(auto.df)) > 0,]

set.seed(1)

train <- sample(dim(auto.df)[1],196) # choose a sample of size 196 from row indices of dataframe auto.df

lm.fit <- lm(mpg ~ horsepower, data = auto.df, subset = train)

# test error MSE
mean((auto.df$mpg - predict(lm.fit, auto.df))[-train]^2, na.rm = T) 

# claculate test.mse for polynimial regression

lm.fit2 <- lm(mpg~poly(horsepower, 2), data=auto.df, subset = train)

# test error MSE
mean((auto.df$mpg - predict(lm.fit2, auto.df))[-train]^2, na.rm = T) 

# claculate test.mse for  cubic regression

lm.fit3 <- lm(mpg~poly(horsepower, 3), data=auto.df, subset = train)

# test error MSE
mean((auto.df$mpg - predict(lm.fit3, auto.df))[-train]^2, na.rm = T) 

# if we sample again and create another traing sample MSE values will be different
train <- sample(dim(auto.df)[1],196) # choose a sample of size 196 from row indices of dataframe auto.df

lm.fit <- lm(mpg ~ horsepower, data = auto.df, subset = train)

# test error MSE
mean((auto.df$mpg - predict(lm.fit, auto.df))[-train]^2, na.rm = T) 

# claculate test.mse for polynimial regression

lm.fit2 <- lm(mpg~poly(horsepower, 2), data=auto.df, subset = train)

# test error MSE
mean((auto.df$mpg - predict(lm.fit2, auto.df))[-train]^2, na.rm = T) 

# claculate test.mse for  cubic regression

lm.fit3 <- lm(mpg~poly(horsepower, 3), data=auto.df, subset = train)

# test error MSE
mean((auto.df$mpg - predict(lm.fit3, auto.df))[-train]^2, na.rm = T) 


```

```{r LOOCV }
library(tidyverse)
library(boot)

auto.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/Auto.csv", header=T, stringsAsFactors = T, na.strings = "?")
auto.df = as_tibble(auto.df)

# remove NAs
# is.na(auto.df)
is.matrix(is.na(auto.df))
# rowSums(is.na(auto.df))

auto.df <- auto.df[rowSums(is.na(auto.df)) == 0,]

# now fit the data on the whole data
glm.fit <- glm(mpg ~ horsepower, data = auto.df)

cv.err <- cv.glm(auto.df, glm.fit)
str(cv.err)
cv.err$delta


# redo this for polynomials and save the results in a vector
cv.error <- NULL
for (i in 1:5){
  glm.fit <- glm(mpg ~ poly(horsepower,i), data = auto.df)
  cv.error <- rbind(cv.error, cv.glm(auto.df, glm.fit)$delta)
}
cv.error
```

```{r K-fold cross validation for classification }
library(tidyverse)
library(class)
library(boot)

set.seed(17)
weekly.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Weekly.csv", 
                      header=T, stringsAsFactors = T, na.strings = "?")
weekly.df = tibble(weekly.df)

# create k-fold 
k <- 10
threshold <- 0.5

folds <- sample(1:k, size = nrow(weekly.df), replace = T) #10 fold CV
table(folds)

# folds with same size
# sameSizefolds <- sample(rep(1:k, length.out = nrow(weekly.df)), size = nrow(weekly.df), replace = F)
# table(sameSizefolds)

# Run the model by making the model on 9 folds and predicting on the hold out:

results <- lapply(1:k, function(x){  # x is the index of test portion, the rest are for training
  glm.fit <- glm(Direction ~ Lag2, data = weekly.df[folds != x,], family = binomial)
  glm.probs <- predict(glm.fit, weekly.df[folds == x,], type =  "response")
  # since contrasts(weekly.df$Direction) shows dummy variable 1 asigned to 'Up'
  # and since P(y=1|x) is glm.probs what we get is prosterior of probability of 'Up' case
  glm.pred <- ifelse(glm.probs > threshold, "Up", "Down")
  return(data.frame(probs = glm.probs, predicted = glm.pred, real = weekly.df[folds == x, ]$Direction ))
})

# calculate confusion table and other measeres

missclassificationRate = NULL
nullClassificationRate = NULL
FP_rates = NULL
TP_rates = NULL
precisions = NULL
specificities = NULL
confusionTables = NULL
aucs = NULL

library(pROC)

for( df in results){
  confusion_table <- table(df$predicted, df$real)
  
  nullClassifier <- max(
    (confusion_table[1,1] + confusion_table[2,1])/(confusion_table[1,1] + confusion_table[2,1]+ confusion_table[1,2] + confusion_table[2,2] ), 
    (confusion_table[1,2] + confusion_table[2,2])/(confusion_table[1,1] + confusion_table[2,1]+ confusion_table[1,2] + confusion_table[2,2] ))
  
  nullClassificationRate <- c(nullClassificationRate, nullClassifier)
  
  roc_obj <- roc(df$real, df$probs)
  aucs <- c(aucs, auc(roc_obj))

  confusionTables  <- cbind(confusionTables, confusion_table)
  missclassificationRate <- c(missclassificationRate, mean(df$predicted != df$real))
  FP_rates <- c(FP_rates, confusion_table[2,1]/(confusion_table[2,1]+ confusion_table[1,1]))
  TP_rates <- c(TP_rates, confusion_table[2,2]/(confusion_table[2,2]+ confusion_table[1,2]))
  precisions <- c(precisions, confusion_table[2,2] / (confusion_table[2,2] + confusion_table[2,1]))
  specificities <- c(specificities , 1 - confusion_table[2,1]/(confusion_table[2,1]+ confusion_table[1,1]) )

  # overall fraction of wrong predictions:
  # print(confusion_table)
}

# average missclassification error rate
sprintf("Logistic Regression : Missclassification error rate : %s", mean(missclassificationRate))

sprintf("Logistic regression : Null Classifier: %s", mean(nullClassificationRate))

sprintf("Logistic Regression AUC: %s", mean (aucs))
# FP rate:
sprintf("Logistic Regression : FP rate (TypeI error, 1 - specificity) : %s", mean(FP_rates))

# TP rate:
sprintf("Logistic Regression : TP rate (1-TypeII error, power, sensetivity, recall) : %s", mean(TP_rates))

# precision:
sprintf("Logistic Regression : precision: %s", mean(precisions))

# specificity 1-FP/N:
sprintf("Logistic Regression : specificity 1-FP/N: %s", mean(specificities))

```


```{r Use Bootstrap to find variability of LDAs coefficient estimates }
library(tidyverse)
library(class)
library(boot)


weekly.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Weekly.csv", 
                      header=T, stringsAsFactors = T, na.strings = "?")
weekly.df = tibble(weekly.df)


# train <- (weekly.df$Year >= 1990 & weekly.df$Year <= 2008)
# test.Y <- weekly.df[!train,]$Direction
# test.X <- weekly.df[!train,]
# train.Y <- weekly.df[train, ]$Direction
# 
# train.X <- weekly.df[train,]

boot.fn <- function (df, index){
  lda.fit <- MASS::lda(Direction ~ Lag2, data = df,family = binomial, subset = index)
  lda.fit$scaling
} 

# First estimate the coefficients on the full set 
boot.fn(weekly.df,1:nrow(weekly.df))

# we can use the function to create bootstrap estimate for LDA coefficient
# by randomly sampling from among the observations with replacement 
set.seed(17)
boot.fn(weekly.df, sample(nrow(weekly.df),nrow(weekly.df), replace=T))

boot.fn(weekly.df, sample(nrow(weekly.df),nrow(weekly.df), replace=T))

boot.fn(weekly.df, sample(nrow(weekly.df),nrow(weekly.df), replace=T))

boot.fn(weekly.df, sample(nrow(weekly.df),nrow(weekly.df), replace=T))

# next we plugin the function into 'boot()' to compute SE[] of 1000 bootstrap estimates for the LDA coefficient
boot(data=weekly.df, statistic = boot.fn, R = 1000)
```

```{r exercise 5}

library(tidyverse)
library(class)
library(boot)
set.seed(1)

default.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/Default.csv", 
                      header=T, stringsAsFactors = T, na.strings = "?")
default.df = tibble(default.df)
colnames(default.df)

str(default.df)

# a)

glm.fit <- glm(default ~ balance + income, data = default.df, family = binomial)
sprintf("summary of logistic regression: ")
summary(glm.fit)

# b) 
# get a random sample 
train <- sample(nrow(default.df), nrow(default.df)/2) 

# let's create a function that get the full data and a subset of indices as training set
# and return the miss classification error rate on the validation set
classify <- function(df , train.indices){
  
  # train on random set
  glm.fit <- glm(default ~ balance + income, data = default.df, family = binomial, subset=train.indices)
  
  # obtain the prediction of default in validation set
  contrasts(default.df$default)
  
  # since contrasts(default.df$default) shows dummy variable 1 asigned to "Yes"
  # since P(y=1|x) is actually glm.probs what we get is prosterior of probability of defauly = Yes
  
  glm.probs <- predict(glm.fit, default.df[-train.indices, ], type =  "response")
  
  # convert posterior probabilities into "Yes" and "No"
  glm.pred <- ifelse(glm.probs > 0.5, "Yes", "No")
  stopifnot(length(glm.pred) == length(default.df[-train.indices, ]$default))
  
  (confusion_matrix <- table(glm.pred, default.df[-train.indices, ]$default))
  
  # validation set missclassification error rate
  mean(glm.pred != default.df[-train.indices, ]$default)

}

classify(default.df, train)
# c) 

train <- sample(nrow(default.df), nrow(default.df)/3) 
classify(default.df, train)

train <- sample(nrow(default.df), 2*nrow(default.df)/3) 
classify(default.df, train)

train <- sample(nrow(default.df), 4*nrow(default.df)/5) 
classify(default.df, train)

train <- sample(nrow(default.df), nrow(default.df)/5) 
classify(default.df, train)

# in general all error rates are around 0.025

# d) 
classify1 <- function(df , train.indices){
  
  # train on random set
  glm.fit <- glm(default ~ balance + income + student, 
                 data = default.df, family = binomial, subset=train.indices)
  
  # obtain the prediction of default in validation set
  contrasts(default.df$default)
  
  # since contrasts(default.df$default) shows dummy variable 1 asigned to "Yes"
  # since P(y=1|x) is actually glm.probs what we get is prosterior of probability of defauly = Yes
  
  glm.probs <- predict(glm.fit, default.df[-train.indices, ], type =  "response")
  
  # convert posterior probabilities into "Yes" and "No"
  glm.pred <- ifelse(glm.probs > 0.5, "Yes", "No")
  stopifnot(length(glm.pred) == length(default.df[-train.indices, ]$default))
  
  (confusion_matrix <- table(glm.pred, default.df[-train.indices, ]$default))
  
  # validation set missclassification error rate
  mean(glm.pred != default.df[-train.indices, ]$default)

}

train <- sample(nrow(default.df), nrow(default.df)/2) 
classify(default.df, train)
# adding student actually increased the error rate
```

```{r exercise 6}

library(tidyverse)
library(class)
library(boot)
set.seed(1)

default.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/Default.csv", 
                      header=T, stringsAsFactors = T, na.strings = "?")
default.df = tibble(default.df)
colnames(default.df)

glm.fit <- glm(default ~ balance + income ,data = default.df, family = binomial)

# a)
summary(glm.fit)

# b)
boot.fn <- function(df, index) {
  glm.fit <- glm(default ~ balance + income ,data = default.df, family = binomial, subset = index)
  coefficients(glm.fit)
}

# c)
# next we plugin the boot.fn function into 'boot()' to compute SE[] of 1000
# bootstrap estimates for the logistic regression coefficients
(result <- boot(data=weekly.df, statistic = boot.fn, R = 100))

print("--------------- Here are the values by capturing output:-------------------- ")
library(stringr)

(x <- capture.output(result)) # store the output as text
(x <- str_extract(x ,"^t1.*$")) # grab the line that starts with t1
(x <- x[!is.na(x)]) # remove all the lines we don't need

# (se <- as.numeric(unlist(str_extract_all(x, '[0-9.]+$')))) # extract the final value (se)


# d) 

# For balance SE is shrink from 0.005647 to 0.000709 
# For income  SE is changed from 2.081e-05 to 1.443805e-05 not much change

```



```{r Exercise 7}
library(tidyverse)
library(class)
weekly.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Weekly.csv", header=T, stringsAsFactors = T, na.strings = "?")
weekly.df = tibble(weekly.df)


# a) Logistic regression using full data set:
glm.fit1 <- glm(Direction ~ Lag1 + Lag2, data = weekly.df, family = binomial)

# b) Logistic regression using full data set but the first observation:

glm.fit2 <- glm(Direction ~ Lag1 + Lag2, data = weekly.df, family = binomial, subset=(1:nrow(weekly.df))[-1])

# c) Predict first observation using model b 

# first lets see the contrasts of Direction to know what is assigned to 1 and which is 2
contrasts(weekly.df$Direction)

# Contrasts shows Down is 0 and Up is 1
# Since Posterior is P(Y=1|X) Tuse if posterior > 0.5 it should be "Up"

glm.probs <- predict(glm.fit2, weekly.df[1,], type =  "response")
(glm.predict <- ifelse(glm.probs > 0.5 , "Up", "Down"))
weekly.df[1,]$Direction

# d)
# First observation is not correctly classified
# errors
errorList = NULL

for(i in 1:nrow(weekly.df)){
  glm.fit <- glm(Direction ~ Lag1 + Lag2, data = weekly.df, family = binomial, subset = (1:nrow(weekly.df))[-i])
  # Predict ith observation using model that is trained on all records but the ith one
  glm.probs <- predict(glm.fit, weekly.df[i,], type =  "response")
  (glm.predict <- ifelse(glm.probs > 0.5 , "Up", "Down"))
  
  # accumulate errors
  errorList <-c(errorList , glm.predict != weekly.df[i,]$Direction)
}

# e)
sprintf("average LOOC error rate: %s", mean(errorList) )


```
```{r Exercise 8: cross validation on a simulated data set}
library(tidyverse)
library(class)
library(boot)

# a ) generate simulated data set:
set.seed(1)
y <- rnorm(100)
x <- rnorm(100)
y <- x - 2*x^2+rnorm(100)

df <- tibble(y = y, x = x)
head(df)

# b)
df %>%
  ggplot(mapping = aes(x=x, y=y))+
  geom_point(alpha=1/3)

# c)
set.seed(1)
errors <- NULL
for (i in 1:4){ 
  glm.fit <- glm(y ~ poly(x, i), data=df)
  errors <- rbind(errors, cv.glm(df, glm.fit)$delta)  
}
errors

# d) repeat c with another random seed
set.seed(17)
errors <- NULL
for (i in 1:4){ 
  glm.fit <- glm(y ~ poly(x, i), data=df)
  summary(glm.fit)
  errors <- rbind(errors, cv.glm(df, glm.fit)$delta)  
}
errors

# The result is the same for different seed vlues becuse there is no random 
# componenet in LOOC procedure

# f)

for (i in 1:4){ 
  glm.fit <- glm(y ~ poly(x, i), data=df)
  print("---------- Ploy Model of oder -------------- ")
  print(i)
  
  print(summary(glm.fit))
}

# Clearly in all the models only beta 1 and beta 2 are statistically signifocant which matches 
# The result from LOOCV that shows ploynomial of order 2 has smallest error

```

```{r Exercise 8}
library(tidyverse)
library(class)
library(boot)
boston.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/Boston.csv", 
                      header=T, stringsAsFactors = T, na.strings = "?")
boston.df = tibble(boston.df)
str(boston.df)

# a) 
(muHat <- mean (boston.df$medv))

# b)
(SE_of_muHat <- sd(boston.df$medv) / sqrt(nrow(boston.df)))

#c)
boot.fn <- function(df, index) mean (df[index, ]$medv)

(result <- boot(data=boston.df, statistic = boot.fn, R = 1000))

# comparing with part b) , bootstrap error is a bit larger

# d) 
# lets calculate 95% confedence interval for the muHat estimator:
# [muHat - 2*SE[muHat], muHat + 2*SE[muHat]]
(leftBound <- 22.53281 - 2 * 0.4251931)
(rightBound <- 22.53281 + 2 * 0.4251931)

# e) provide an estimate for median value of the population based on the data set
(muHatMed <- median(boston.df$medv))

# f) calculate SE of muhatMed using bootstrap

boot.fn <- function(df, index) median(df[index, ]$medv)
(result <- boot(data=boston.df, statistic = boot.fn, R = 1000))

# g) 10th percentile
percentiles <- quantile(boston.df$medv,  probs = c(10, 25, 50, 75, 100)/100)
print("-------- 10th percentile mdev: ------------- ")
(muHat01 <- percentiles[1])

# h) use bootstrap to find SE error for t0th percentile
boot.fn <- function(df, index) quantile(df[index,]$medv,  probs = c(10, 25, 50, 75, 100)/100)[1]
print("-------- bootstrap 10th percentile mdev: ------------- ")
(result <- boot(data=boston.df, statistic = boot.fn, R = 10000))
```

```{r Quantile}
set.seed(10)
x <- rnorm(10000)

tibble(x1 = quantile(x,  probs = seq(0,1,0.01), type=1), y = seq(0,1,0.01)) %>%
  ggplot(mapping = aes(x=x1, y=y))+
  geom_point(alpha=1/2)

quantile(x) # Extremes & Quartiles by default
quantile(x,  probs = c(0, 0.1, 0.5, 1, 2, 5, 10, 25, 50, 75, 100)/100)

### Compare different types
quantAll <- function(x, prob, ...)
  t(vapply(1:9, function(typ) quantile(x, prob=prob, type = typ, ...), quantile(x, prob, type=1)))
p <- c(0.1, 0.5, 1, 2, 5, 10, 50)/100
signif(quantAll(x, p), 4)
## for complex numbers:
z <- complex(re=x, im = -10*x)
signif(quantAll(z, p), 4)

```

