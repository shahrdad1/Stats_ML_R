---
title: "ISLR CH7 Exercises"
output:
  pdf_document: default
  html_notebook: default
---

```{r apply lapply and sapply}

# apply wors like reduce() (sum over columns (or rows) of a matrix or a tibble)
(m1 <- matrix(C<-(1:10),nrow=5, ncol=6))
(a_m1 <- apply(m1, 2, sum)) 

# apply on tibble (reduce on columns)
(a_tibble <- as_tibble(m1))
(a_m2 <- apply(a_tibble, 2, sum)) 

# lApply (apply a function over a list (or tibble as a list of columns) and return a new list)
movies <- c("SPYDERMAN","BATMAN","VERTIGO","CHINATOWN")
movies_lower <-lapply(movies, tolower)
str(movies_lower)

# lapply on tible returns a list and then 
(a_list <- lapply(a_tibble, function(x) {x*2}))

# sapply() function takes list, vector or data frame as input and gives output 
# in vector or matrix
(matrix <-sapply(a_tibble, function(x) {x*2}))

# We can use lapply() or sapply() interchangeable to slice a data frame
below_ave <- function(x) {  
    ave <- mean(x) 
    return(x[x > ave])
}

(dt_s<- as_tibble(sapply(a_tibble, below_ave)))
(dt_l<- as_tibble(lapply(a_tibble, below_ave)))
identical(dt_s, dt_l)

# tapply() computes a measure (mean, median, min, max, etc..) or a function for 
# each factor variable in a vector. It is a very useful function that lets you 
# create a subset of a vector and then apply some functions to each of the subset.

# -X: An object, usually a vector
# -INDEX: A list containing factor
# -FUN: Function applied to each element of x

#As a prior work, we can compute the median of the length for each species. 
# tapply() is a quick way to perform this computation.

data(iris)
tapply(iris$Sepal.Width, iris$Species, median)


# remove any row that has at least one empty string value 
(df1 <- tibble(x=c("" , " ", "  abc  " ,"", "  de","f  "), 
              y=c("12", "  54", "  ", "  c12  ", "  ", "  No  "))
)


(df2 <- as_tibble(sapply(df1, function(the_col) gsub("\\s+", "", the_col))))

trim.f <- function(x) trimws(x, which = c("both"))
space.f <- function(x) gsub("\\s+", NA, x)
empty.f <- function(x) gsub("^$", NA, x)
library(tidyverse)
df1 %>%
  mutate_each(funs(trim.f)) %>%
  mutate_each(funs(space.f)) %>%
  mutate_each(funs(empty.f))  %>%
  na.omit

  
  

```

```{r fit polynomial (with cv to find the best degree) and step function}
library(tidyverse)
set.seed(1)
wage.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Wage.csv", 
                      header=T, stringsAsFactors = F, na.strings = "?")
(wage.df.original = tibble(wage.df))
(wage.df = tibble(wage.df))

# first clean up NA and particularly character columns
# 1) remove leading, trainling and empty characters from character columns

trim.f <- function(x) trimws(x, which = c("both")) # leading and trailing spaces
empty.f <- function(x) gsub("^$", NA, x) # empty strings

wage.df[sapply(wage.df, is.character)] <- 
  wage.df %>%
    select(which(sapply(.,is.character))) %>%
    mutate_each(funs(trim.f)) %>%
    mutate_each(funs(empty.f))  %>%
    na.omit


sprintf("Any NA or empty string in character columns recognized: %s",
        !identical(wage.df,wage.df.original) )

# 2) It is safe now to convert all character fields into factors
wage.df[sapply(wage.df, is.character)] <-
  wage.df %>%
  select(which(sapply(., is.character))) %>%
  mutate_each(funs(factor))

  
# Now fit a 4 degree polynomial
fit.poly <- lm(wage ~ poly(age, 4), data = wage.df)
coef(summary(fit.poly))

# draw standard error 
# first get the range of the values of age
(age.limits  <- range(wage.df$age))

# Now create an interaval from these range values
(age.grid <- seq(from=age.limits[1], to=age.limits[2]))

# predict value for this interval using fitted model
predicts <- predict(fit.poly, newdata = tibble(age=age.grid), se=T)
names(predicts)

se.bands <- cbind(predicts$fit + 2*predicts$se.fit, 
                  predicts$fit - 2*predicts$se.fit)

# now plot the predicts
#par(mfrow=c(1,2), mar=c(4.5,4.5,1,1), oma=c(0,0,4,0))
plot(wage.df$age, wage.df$wage, xlim=age.limits, cex=0.5, col="darkgrey")
title("Degree 4 poly", outer = T)
lines(age.grid, predicts$fit, lwd=2, col = "blue")
matlines(age.grid, se.bands, lwd=2, col="red", lty=3)

# use anova to find the best model between multiple nested models
fit.1 <- lm(wage ~ age, data = wage.df)
fit.2 <- lm(wage ~ poly(age, 2), data = wage.df)
fit.3 <- lm(wage ~ poly(age, 3), data = wage.df)
fit.4 <- lm(wage ~ poly(age, 4), data = wage.df)
fit.5 <- lm(wage ~ poly(age, 5), data = wage.df)

anova(fit.1, fit.2, fit.3, fit.4, fit.5)
# --------------------------------------------------------------------------
# However the best approach is using CV to choose best degree for polynomial
#---------------------------------------------------------------------------
# create k-fold 
k <- 10
degrees <- 1:10
set.seed(1)

# create k folds
folds <- sample(1:k, nrow(wage.df), replace = T)

# For folds with same size do:
# sameSizefolds <- sample(rep(1:k, length.out = nrow(weekly.df)), 
#                                        size = nrow(weekly.df), replace = F)
# table(sameSizefolds)

# perform a cross validation on a for loop
mse.per.fold <- tibble(fold.id = NULL, degree = NULL, mse=NULL)
for (dgr in degrees){
  for (j in 1:k){
    
    # fit the polynomial model for given degree on training fold
    
    fit.poly <- lm(wage ~ poly(age, dgr), data = wage.df[folds != j, ])
    # now predict on test fold
    predicts <- predict(fit.poly, 
                        newdata = list(age=wage.df[folds == j, ]$age), se=T)

    # calculte the MSE 
    mse.per.fold <- 
      rbind(mse.per.fold, 
            tibble(fold.id = j, degree = dgr, 
                   mse=mean((predicts$fit - wage.df[folds == j, ]$wage)^2)))
  }
}

# We have to find average of mse rate for each degree cross all test folds 


(summary <- mse.per.fold %>%
  group_by(degree) %>%
  summarise(mse.mean = mean(mse)) 
)

# and then find the degree that has minimum mean.mse
summary %>%
  slice(which.min(mse.mean))

# Now let's predict if an individual earns more than 20k per year

fit <- glm(I(wage > 250) ~ poly(age,4), data = wage.df, family = binomial)

# do the prediction using the model
preds <- predict(fit, newdata=list(age=age.grid), se = T)

# Note that predict function for glm model gets the prediction for logit i.e: 
# log(Pr(Y=1 | X) / (1 - Pr(Y=1 | X))) = Xβ and also the standard error is 
# of the same form, clealry to get prediction for Pr(Y=1|X) we need 
# to calculate exp(Xβ)/(1+exp(Xβ))

prob.predict <- exp(preds$fit)/(1+exp(preds$fit))
se.bound.logit <- cbind(preds$fit + 2*preds$se, preds$fit - 2*preds$se)
se.bound <- apply(se.bound.logit, 2, function(x) exp(x)/(1+exp(x)))

# now plot the predicts
#par(mfrow=c(1,2), mar=c(4.5,4.5,1,1), oma=c(0,0,4,0))
plot(wage.df$age, I(wage.df$wage > 250), 
     xlim=age.limits, cex=0.5, col="darkgrey")
# we use jitter so that observation with the same value do not cover each other
points(jitter(wage.df$age), I(wage.df$age>250)/5, 
       cex=0.5, pch="|", col="darkgrey")
title("Degree 4 poly", outer = T)
lines(age.grid, prob.predict, lwd=2, col = "blue")
matlines(age.grid, se.bound, lwd=2, col="red", lty=3)

print("----------- let's fit a step function --------------")
# we use cut() function to create knots
table(cut(wage.df$age, 4))
print("fit a step function using cut() or giving our cut points using break(): ")
# let's fit a step function
fit.step <- lm(wage ~ cut(age, 4), data=wage.df)
coef(summary(fit))

(age.limits  <- range(wage.df$age))

# Now create an interaval from these range values
(age.grid <- seq(from=age.limits[1], to=age.limits[2]))

# predict value for this interval using fitted model
predicts <- predict(fit.step, newdata = tibble(age=age.grid), se=T)

se.bands <- cbind(predicts$fit + 2*predicts$se.fit, 
                  predicts$fit - 2*predicts$se.fit)

# now plot the predicts
#par(mfrow=c(1,2), mar=c(4.5,4.5,1,1), oma=c(0,0,4,0))
plot(wage.df$age, wage.df$wage, xlim=age.limits, cex=0.5, col="darkgrey")
title("Degree 4 poly", outer = T)
lines(age.grid, predicts$fit, lwd=2, col = "blue")
matlines(age.grid, se.bands, lwd=2, col="red", lty=3)

```


```{r regression splines, natural spline, smoothing spline with CV and loess}
library(tidyverse)
library(splines)
wage.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Wage.csv", 
                      header=T, stringsAsFactors = F, na.strings = "?")
(wage.df.original = tibble(wage.df))
(wage.df = tibble(wage.df))

# first clean up NA and particularly character columns
# 1) remove leading, trainling and empty characters from character columns

trim.f <- function(x) trimws(x, which = c("both")) # leading and trailing spaces
empty.f <- function(x) gsub("^$", NA, x) # empty strings

wage.df[sapply(wage.df, is.character)] <- 
  wage.df %>%
    select(which(sapply(.,is.character))) %>%
    mutate_each(funs(trim.f)) %>%
    mutate_each(funs(empty.f))  %>%
    na.omit


sprintf("Any NA or empty string in character columns recognized: %s",
        !identical(wage.df,wage.df.original) )

# 2) It is safe now to convert all character fields into factors
wage.df[sapply(wage.df, is.character)] <-
  wage.df %>%
  select(which(sapply(., is.character))) %>%
  mutate_each(funs(factor))

# use df() to generate spline with knots at uniform quantiles
dim(bs(wage.df$age, knots=c(25,40,60)))

dim(bs(wage.df$age, df=6))

# Fit wage to age using regression splines
fit <- lm(wage~bs(age, knots = c(25,40,60)) ,data=wage.df)

# first get the range of the values of age
(age.limits  <- range(wage.df$age))

# Now create an interaval from these range values
(age.grid <- seq(from=age.limits[1], to=age.limits[2]))

# predict the age values 
pred <- predict(fit, newdata = list(age=age.grid), se=T)
plot(wage.df$age, wage.df$wage, col="gray")
lines(age.grid, pred$fit, lwd=2)
se_bonds <- cbind(pred$fit+2*pred$se, pred$fit-2*pred$se)
matlines(age.grid, se_bonds, lwd=2, col="red", lty=3)

# use ns() to create a basis function for natural spline
dim(ns(wage.df$age, knots=c(25,40,60))) # 3 + 3 - 2

dim(ns(wage.df$age, df = 4))

# Fit wage to age using natural spline
fit2 <- lm(wage~ns(age, df = 4) ,data=wage.df)
pred2 <- predict (fit2, newdata = list(age = age.grid), se=T)
plot(wage.df$age, wage.df$wage, col="gray")
lines(age.grid, pred2$fit, col="black", lwd=2)
se_bonds <- cbind(pred2$fit+2*pred2$se, pred2$fit-2*pred2$se)
matlines(age.grid, se_bonds, lwd=2, col="red", lty=3)


# fit smoothing spline
plot(wage.df$age, wage.df$wage, xlim=age.limits, cex=0.5, col="darkgrey")
title("Smoothing spline")
# first no cv, just hard code effective degree of freedom as 16
fit.smooth=smooth.spline(wage.df$age, wage.df$wage, df=16)
pred.smooth <- predict(fit.smooth, newdata = list(age=age.grid), se=T)
title("Smoothing Spline with hard coded degree of freedom")
plot(wage.df$age, wage.df$wage, col="gray")
lines(age.grid, pred.smooth$fit, col="black", lwd=2)
lines(fit.smooth, col="red", lwd=2)

# second use cv to get the best effective degree of freedom
fit.smooth.cv=smooth.spline(wage.df$age, wage.df$wage, cv=T)
pred.smooth.cv <- predict(fit.smooth.cv, newdata = list(age=age.grid), se=T)

sprintf("effective degree of freedom found by cv is %s",fit.smooth.cv$df)
title("Smoothing Spline with CV")
plot(wage.df$age, wage.df$wage, col="gray")
lines(age.grid, pred.smooth.cv$fit.cv, col="black", lwd=2)
lines(fit.smooth.cv, col="blue", lwd=2)

# Fit local regression model
plot(wage.df$age, wage.df$wage, col="gray")
title("Local regression span = 0.2")
fit.loess.1 <- loess(wage~age, span = 0.2, data = wage.df)
fit.loess.2 <- loess(wage~age, span = 0.5, data = wage.df)
pred.loess.1 <- predict(fit.loess.1, newdata = tibble(age=age.grid), se=T)
pred.loess.2 <- predict(fit.loess.2, newdata = tibble(age=age.grid), se=T)

lines(age.grid, pred.loess.1$fit, col="blue", lwd=2)
se_bonds.loess.1 <- cbind(pred.loess.1$fit+2*pred.loess.1$se, pred.loess.1$fit-2*pred.loess.1$se)
matlines(age.grid, se_bonds.loess.1, lwd=2, col="red", lty=3)

plot(wage.df$age, wage.df$wage, col="gray")
title("Local regression span = 0.4")
lines(age.grid, pred.loess.2$fit, col="blue", lwd=2)
se_bonds.loess.2 <- cbind(pred.loess.2$fit+2*pred.loess.2$se, pred.loess.2$fit-2*pred.loess.2$se)
matlines(age.grid, se_bonds.loess.2, lwd=2, col="red", lty=3)

```


```{r GAM with CV}
wage.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Wage.csv", 
                      header=T, stringsAsFactors = F, na.strings = "?")
(wage.df.original = tibble(wage.df))
(wage.df = tibble(wage.df))

# first clean up NA and particularly character columns
# 1) remove leading, trainling and empty characters from character columns

trim.f <- function(x) trimws(x, which = c("both")) # leading and trailing spaces
empty.f <- function(x) gsub("^$", NA, x) # empty strings

wage.df[sapply(wage.df, is.character)] <- 
  wage.df %>%
    select(which(sapply(.,is.character))) %>%
    mutate_each(funs(trim.f)) %>%
    mutate_each(funs(empty.f))  %>%
    na.omit


sprintf("Any NA or empty string in character columns recognized: %s",
        !identical(wage.df,wage.df.original) )

# 2) It is safe now to convert all character fields into factors
wage.df[sapply(wage.df, is.character)] <-
  wage.df %>%
  select(which(sapply(., is.character))) %>%
  mutate_each(funs(factor))

print("---------------- GAM with smoothing spline and CV :---------------- ")
library(mgcv)

cv.gam <- gam(logwage~s(age, bs="cr") + s(year, bs="cr", k=5) + education, 
              method="GACV.Cp", scale=-1, data=wage.df,family=Gamma(link=log), 
              gamma=1.4)
print("----------------- cv.gam model ---------------------")
print(cv.gam)
print("------ anova for approximate signifiocance of terms of cv.gam ---------")
anova(cv.gam)
print("----------------- summary cv.gam1 ---------------------")
summary(cv.gam)


plot(cv.gam, too.far=0.15)

#residuals(cv.gam)

#print("----------------- check ---------------------")
#gam.check(cv.gam)

# plot (cv.gam, residuals=T, select=1)
# title("age vs s(age)")
# 
# plot (cv.gam, residuals=T, select=2)
# title("year vs s(year)")

cv.gam1 <- gam(logwage~s(age, bs="cr") + s(year, bs="cr", k=5) +te(age, year, k=5) + education, 
              method="GACV.Cp", scale=-1, data=wage.df,family=Gamma(link=log), 
              gamma=1.4)

print("----------------- cv.gam1 model ---------------------")
print(cv.gam1)
print("------ anova for approximate signifiocance of terms of cv.gam1 ---------")
anova(cv.gam1)
print("----------------- summary cv.gam1 ---------------------")
summary(cv.gam1)

plot(cv.gam1, too.far=0.15)

# predict
pred <- predict(cv.gam, newdata = wage.df, se=TRUE,type="terms")


```

```{r GAM with GCV for stock data}
library(tidyverse)
library(mgcv)
library(purrr)

smarket.df = 
  read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/Smarket.csv", 
           header=T, stringsAsFactors = F, na.strings = "?")

smarket.df = as_tibble(smarket.df)
smarket.df.original = as_tibble(smarket.df)

# first clean up NA and particularly character columns
# 1) remove leading, trainling and empty characters from character columns

trim.f <- function(x) trimws(x, which = c("both")) # leading and trailing spaces
empty.f <- function(x) gsub("^$", NA, x) # empty strings

smarket.df[sapply(smarket.df, is.character)] <- 
  smarket.df %>%
    select(which(sapply(.,is.character))) %>%
    mutate_each(funs(trim.f)) %>%
    mutate_each(funs(empty.f))  %>%
    na.omit


sprintf("Any NA or empty string in character columns recognized: %s",
        !identical(smarket.df,smarket.df.original) )

# 2) It is safe now to convert all character fields into factors
smarket.df[sapply(smarket.df, is.character)] <-
  smarket.df %>%
  select(which(sapply(., is.character))) %>%
  mutate_each(funs(factor))

# split to test train
train <- smarket.df$Year < 2005
test <- !train


# Use Gam
# it returns logit{E(yi)}
cv.gam1 <- gam(Direction~ s(Year, bs="cr", k=4)+
                 s(Volume, k=20)+s(Lag1, k=20),
              method="GACV.Cp", scale=-1, data=smarket.df,subset = train,
              family=binomial(link=logit))
str(smarket.df)
print("----------------- cv.gam model ---------------------")
print(cv.gam1)
print("------ anova for approximate signifiocance of terms of cv.gam ---------")
anova(cv.gam1)
print("----------------- summary cv.gam1 ---------------------")
summary(cv.gam1)
print("----------------- check cv.gam1 ---------------------")
gam.check(cv.gam1)

# plot(cv.gam1, too.far=0.15)


# predict the test model
pred <- predict(cv.gam1, newdata = smarket.df[test,], se=TRUE)
preds <- pred$fit
# Note pred$fit is logit, we need to convert it to probability

logit.inverse <- function(logit) {
  odds <- exp(logit)
  odds/(1+odds)
}

gam.probs <- map(preds, logit.inverse)

# let's draw probabilities vs logit to see if it makes sense
library(ggplot2)
ggplot(tibble(x=preds), aes(x = x)) +
        stat_function(fun = logit.inverse) + 
  scale_x_continuous(name = "logit") +
        scale_y_continuous(name = "Probability")

# calculate test.mse
gam.pred <- ifelse(gam.probs > threshold, "Up", "Down")

missclassificationRate = NULL
nullClassificationRate = NULL
FP_rates = NULL
TP_rates = NULL
precisions = NULL
specificities = NULL
confusionTables = NULL
aucs = NULL

library(pROC)
df <- smarket.df[test,]

confusion_table <- table(gam.pred, smarket.df[test,]$Direction)
  
nullClassifier <- max(
    (confusion_table[1,1] + confusion_table[2,1])/(confusion_table[1,1] + confusion_table[2,1]+ confusion_table[1,2] + confusion_table[2,2] ), 
    (confusion_table[1,2] + confusion_table[2,2])/(confusion_table[1,1] + confusion_table[2,1]+ confusion_table[1,2] + confusion_table[2,2] ))
  
nullClassificationRate <- c(nullClassificationRate, nullClassifier)
roc_obj <- roc(df$Direction, purrr::flatten_dbl(gam.probs))

# Let's draw some AUC plots
plot(roc_obj, legacy.axes = TRUE)

aucs <- c(aucs, auc(roc_obj))

missclassificationRate <- c(missclassificationRate, mean(gam.pred != smarket.df[test,]$Direction))
FP_rates <- c(FP_rates, confusion_table[2,1]/(confusion_table[2,1]+ confusion_table[1,1]))
TP_rates <- c(TP_rates, confusion_table[2,2]/(confusion_table[2,2]+ confusion_table[1,2]))
precisions <- c(precisions, confusion_table[2,2] / (confusion_table[2,2] + confusion_table[2,1]))
specificities <- c(specificities , 1 - confusion_table[2,1]/(confusion_table[2,1]+ confusion_table[1,1]) )

# overall fraction of wrong predictions:
# print(confusion_table)

# average missclassification error rate
sprintf("GAM classifier : Missclassification error rate : %s", mean(missclassificationRate))

sprintf("GAM classifier : Null Classifier: %s", mean(nullClassificationRate))

sprintf("GAM classifier AUC: %s", mean (aucs))
# FP rate:
sprintf("GAM classifier : FP rate (TypeI error, 1 - specificity) : %s", mean(FP_rates))

# TP rate:
sprintf("GAM classifier : TP rate (1-TypeII error, power, sensetivity, recall) : %s", mean(TP_rates))

# precision:
sprintf("GAM classifier : precision: %s", mean(precisions))

# specificity 1-FP/N:
sprintf("GAM classifier : specificity 1-FP/N: %s", mean(specificities))


```


```{r loess with CV}
locv1 <- function(x1, y1, nd, span, ntrial)
{
  locvgcv <- function(sp, x1, y1)
  {
    nd <- length(x1)

    assign("data1", data.frame(xx1 = x1, yy1 = y1))
    fit.lo <- loess(yy1 ~ xx1, data = data1, span = sp, family = "gaussian", degree = 2, surface = "direct")
    res <- residuals(fit.lo)

    dhat2 <- function(x1, sp)
    {
        nd2 <- length(x1)
        diag1 <- diag(nd2)
        dhat <- rep(0, length = nd2)

        for(jj in 1:nd2){
            y2 <- diag1[, jj]
            assign("data1", data.frame(xx1 = x1, yy1 = y2))
            fit.lo <- loess(yy1 ~ xx1, data = data1, span = sp, family = "gaussian", degree = 2, surface = "direct")
            ey <- fitted.values(fit.lo)
            dhat[jj] <- ey[jj]
            }
            return(dhat)
        }

        dhat <- dhat2(x1, sp)
        trhat <- sum(dhat)
        sse <- sum(res^2)

        cv <- sum((res/(1 - dhat))^2)/nd
        gcv <- sse/(nd * (1 - (trhat/nd))^2)

        return(gcv)
    }

    gcv <- lapply(as.list(span1), locvgcv, x1 = x1, y1 = y1)
    #cvgcv <- unlist(cvgcv)
    #cv <- cvgcv[attr(cvgcv, "names") == "cv"]
    #gcv <- cvgcv[attr(cvgcv, "names") == "gcv"]

    return(gcv)
}
```


