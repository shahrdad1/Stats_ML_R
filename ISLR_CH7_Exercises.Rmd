---
title: "ISLR CH7 Exercises"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

```{r apply lapply and sapply}
library(tidyverse)
# apply wors like reduce() (sum over columns (or rows) of a matrix or a tibble)
(m1 <- matrix(C<-(1:10),nrow=5, ncol=6))
(a_m1 <- apply(m1, 2, sum)) 

# apply on tibble (reduce on columns)
(a_tibble <- as_tibble(m1))
(a_m2 <- apply(a_tibble, 2, sum)) 

# lApply (apply a function over a list (or tibble as a list of columns) and return a new list)
movies <- c("SPYDERMAN","BATMAN","VERTIGO","CHINATOWN")
movies_lower <-lapply(movies, tolower)
str(movies_lower)

# lapply on tible returns a list and then 
(a_list <- lapply(a_tibble, function(x) {x*2}))

# sapply() function takes list, vector or data frame as input and gives output 
# in vector or matrix
(matrix <-sapply(a_tibble, function(x) {x*2}))

# We can use lapply() or sapply() interchangeable to slice a data frame
below_ave <- function(x) {  
    ave <- mean(x) 
    return(x[x > ave])
}

(dt_s<- as_tibble(sapply(a_tibble, below_ave)))
(dt_l<- as_tibble(lapply(a_tibble, below_ave)))
identical(dt_s, dt_l)

# tapply() computes a measure (mean, median, min, max, etc..) or a function for 
# each factor variable in a vector. It is a very useful function that lets you 
# create a subset of a vector and then apply some functions to each of the subset.

# -X: An object, usually a vector
# -INDEX: A list containing factor
# -FUN: Function applied to each element of x

#As a prior work, we can compute the median of the length for each species. 
# tapply() is a quick way to perform this computation.

tapply(matrix(1:6, c(6,1)), c(1,1,1,2,2,1),sum)

data(iris)
tapply(iris$Sepal.Width, iris$Species, median)

str(iris$Sepal.Width)

# remove any row that has at least one empty string value 
(df1 <- tibble(x=c("" , " ", "  abc  " ,"", "  de","f  "), 
              y=c("12", "  54", "  ", "  c12  ", "  ", "  No  "))
)


(df2 <- as_tibble(sapply(df1, function(the_col) gsub("\\s+", "", the_col))))

trim.f <- function(x) trimws(x, which = c("both"))
space.f <- function(x) gsub("\\s+", NA, x)
empty.f <- function(x) gsub("^$", NA, x)
library(tidyverse)
df1 %>%
  mutate_each(funs(trim.f)) %>%
  mutate_each(funs(space.f)) %>%
  mutate_each(funs(empty.f))  %>%
  na.omit

  
  

```

```{r fit polynomial (with cv to find the best degree) and step function}
library(tidyverse)
set.seed(1)
wage.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Wage.csv", 
                      header=T, stringsAsFactors = F, na.strings = "?")
(wage.df.original = tibble(wage.df))
(wage.df = tibble(wage.df))

# first clean up NA and particularly character columns
# 1) remove leading, trainling and empty characters from character columns

trim.f <- function(x) trimws(x, which = c("both")) # leading and trailing spaces
empty.f <- function(x) gsub("^$", NA, x) # empty strings

wage.df[sapply(wage.df, is.character)] <- 
  wage.df %>%
    select(which(sapply(.,is.character))) %>%
    mutate_each(funs(trim.f)) %>%
    mutate_each(funs(empty.f))  %>%
    na.omit


sprintf("Any NA or empty string in character columns recognized: %s",
        !identical(wage.df,wage.df.original) )

# 2) It is safe now to convert all character fields into factors
wage.df[sapply(wage.df, is.character)] <-
  wage.df %>%
  select(which(sapply(., is.character))) %>%
  mutate_each(funs(factor))

  
# Now fit a 4 degree polynomial
fit.poly <- lm(wage ~ poly(age, 4), data = wage.df)
coef(summary(fit.poly))

# draw standard error 
# first get the range of the values of age
(age.limits  <- range(wage.df$age))

# Now create an interaval from these range values
(age.grid <- seq(from=age.limits[1], to=age.limits[2]))

# predict value for this interval using fitted model
predicts <- predict(fit.poly, newdata = tibble(age=age.grid), se=T)
names(predicts)

se.bands <- cbind(predicts$fit + 2*predicts$se.fit, 
                  predicts$fit - 2*predicts$se.fit)

# now plot the predicts
#par(mfrow=c(1,2), mar=c(4.5,4.5,1,1), oma=c(0,0,4,0))
plot(wage.df$age, wage.df$wage, xlim=age.limits, cex=0.5, col="darkgrey")
title("Degree 4 poly", outer = T)
lines(age.grid, predicts$fit, lwd=2, col = "blue")
matlines(age.grid, se.bands, lwd=2, col="red", lty=3)

# use anova to find the best model between multiple nested models
fit.1 <- lm(wage ~ age, data = wage.df)
fit.2 <- lm(wage ~ poly(age, 2), data = wage.df)
fit.3 <- lm(wage ~ poly(age, 3), data = wage.df)
fit.4 <- lm(wage ~ poly(age, 4), data = wage.df)
fit.5 <- lm(wage ~ poly(age, 5), data = wage.df)

anova(fit.1, fit.2, fit.3, fit.4, fit.5)
# --------------------------------------------------------------------------
# However the best approach is using CV to choose best degree for polynomial
#---------------------------------------------------------------------------
# create k-fold 
k <- 10
degrees <- 1:10
set.seed(1)

# create k folds
folds <- sample(1:k, nrow(wage.df), replace = T)

# For folds with same size do:
# sameSizefolds <- sample(rep(1:k, length.out = nrow(weekly.df)), 
#                                        size = nrow(weekly.df), replace = F)
# table(sameSizefolds)

# perform a cross validation on a for loop
mse.per.fold <- tibble(fold.id = NULL, degree = NULL, mse=NULL)
for (dgr in degrees){
  for (j in 1:k){
    
    # fit the polynomial model for given degree on training fold
    
    fit.poly <- lm(wage ~ poly(age, dgr), data = wage.df[folds != j, ])
    # now predict on test fold
    predicts <- predict(fit.poly, 
                        newdata = list(age=wage.df[folds == j, ]$age), se=T)

    # calculte the MSE 
    mse.per.fold <- 
      rbind(mse.per.fold, 
            tibble(fold.id = j, degree = dgr, 
                   mse=mean((predicts$fit - wage.df[folds == j, ]$wage)^2)))
  }
}

# We have to find average of mse rate for each degree cross all test folds 


(summary <- mse.per.fold %>%
  group_by(degree) %>%
  summarise(mse.mean = mean(mse)) 
)

# and then find the degree that has minimum mean.mse
summary %>%
  slice(which.min(mse.mean))

# Now let's predict if an individual earns more than 20k per year

fit <- glm(I(wage > 250) ~ poly(age,4), data = wage.df, family = binomial)

# do the prediction using the model
preds <- predict(fit, newdata=list(age=age.grid), se = T)

# Note that predict function for glm model gets the prediction for logit i.e: 
# log(Pr(Y=1 | X) / (1 - Pr(Y=1 | X))) = X*beta and also the standard error is 
# of the same form, clealry to get prediction for Pr(Y=1|X) we need 
# to calculate exp(X*beta)/(1+exp(X*beta))

prob.predict <- exp(preds$fit)/(1+exp(preds$fit))
se.bound.logit <- cbind(preds$fit + 2*preds$se, preds$fit - 2*preds$se)
se.bound <- apply(se.bound.logit, 2, function(x) exp(x)/(1+exp(x)))

# now plot the predicts
#par(mfrow=c(1,2), mar=c(4.5,4.5,1,1), oma=c(0,0,4,0))
plot(wage.df$age, I(wage.df$wage > 250), 
     xlim=age.limits, cex=0.5, col="darkgrey")
# we use jitter so that observation with the same value do not cover each other
points(jitter(wage.df$age), I(wage.df$age>250)/5, 
       cex=0.5, pch="|", col="darkgrey")
title("Degree 4 poly", outer = T)
lines(age.grid, prob.predict, lwd=2, col = "blue")
matlines(age.grid, se.bound, lwd=2, col="red", lty=3)

print("----------- let's fit a step function --------------")
# we use cut() function to create knots
table(cut(wage.df$age, 4))
print("fit a step function using cut() or giving our cut points using break(): ")
# let's fit a step function
fit.step <- lm(wage ~ cut(age, 4), data=wage.df)
coef(summary(fit))

(age.limits  <- range(wage.df$age))

# Now create an interaval from these range values
(age.grid <- seq(from=age.limits[1], to=age.limits[2]))

# predict value for this interval using fitted model
predicts <- predict(fit.step, newdata = tibble(age=age.grid), se=T)

se.bands <- cbind(predicts$fit + 2*predicts$se.fit, 
                  predicts$fit - 2*predicts$se.fit)

# now plot the predicts
#par(mfrow=c(1,2), mar=c(4.5,4.5,1,1), oma=c(0,0,4,0))
plot(wage.df$age, wage.df$wage, xlim=age.limits, cex=0.5, col="darkgrey")
title("Degree 4 poly", outer = T)
lines(age.grid, predicts$fit, lwd=2, col = "blue")
matlines(age.grid, se.bands, lwd=2, col="red", lty=3)

```


```{r regression splines, natural spline, smoothing spline with CV and loess}
library(tidyverse)
library(splines)
wage.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Wage.csv", 
                      header=T, stringsAsFactors = F, na.strings = "?")
(wage.df.original = tibble(wage.df))
(wage.df = tibble(wage.df))

# first clean up NA and particularly character columns
# 1) remove leading, trainling and empty characters from character columns

trim.f <- function(x) trimws(x, which = c("both")) # leading and trailing spaces
empty.f <- function(x) gsub("^$", NA, x) # empty strings

wage.df[sapply(wage.df, is.character)] <- 
  wage.df %>%
    select(which(sapply(.,is.character))) %>%
    mutate_each(funs(trim.f)) %>%
    mutate_each(funs(empty.f))  %>%
    na.omit


sprintf("Any NA or empty string in character columns recognized: %s",
        !identical(wage.df,wage.df.original) )

# 2) It is safe now to convert all character fields into factors
wage.df[sapply(wage.df, is.character)] <-
  wage.df %>%
  select(which(sapply(., is.character))) %>%
  mutate_each(funs(factor))

# use df() to generate spline with knots at uniform quantiles
dim(bs(wage.df$age, knots=c(25,40,60)))

dim(bs(wage.df$age, df=6))

# Fit wage to age using regression splines
fit <- lm(wage~bs(age, knots = c(25,40,60)) ,data=wage.df)

# first get the range of the values of age
(age.limits  <- range(wage.df$age))

# Now create an interaval from these range values
(age.grid <- seq(from=age.limits[1], to=age.limits[2]))

# predict the age values 
pred <- predict(fit, newdata = list(age=age.grid), se=T)
plot(wage.df$age, wage.df$wage, col="gray")
lines(age.grid, pred$fit, lwd=2)
se_bonds <- cbind(pred$fit+2*pred$se, pred$fit-2*pred$se)
matlines(age.grid, se_bonds, lwd=2, col="red", lty=3)

# use ns() to create a basis function for natural spline
dim(ns(wage.df$age, knots=c(25,40,60))) # 3 + 3 - 2

dim(ns(wage.df$age, df = 4))

# Fit wage to age using natural spline
fit2 <- lm(wage~ns(age, df = 4) ,data=wage.df)
pred2 <- predict (fit2, newdata = list(age = age.grid), se=T)
plot(wage.df$age, wage.df$wage, col="gray")
lines(age.grid, pred2$fit, col="black", lwd=2)
se_bonds <- cbind(pred2$fit+2*pred2$se, pred2$fit-2*pred2$se)
matlines(age.grid, se_bonds, lwd=2, col="red", lty=3)


# fit smoothing spline
plot(wage.df$age, wage.df$wage, xlim=age.limits, cex=0.5, col="darkgrey")
title("Smoothing spline")
# first no cv, just hard code effective degree of freedom as 16
fit.smooth=smooth.spline(wage.df$age, wage.df$wage, df=16)
pred.smooth <- predict(fit.smooth, newdata = list(age=age.grid), se=T)
title("Smoothing Spline with hard coded degree of freedom")
plot(wage.df$age, wage.df$wage, col="gray")
lines(age.grid, pred.smooth$fit, col="black", lwd=2)
lines(fit.smooth, col="red", lwd=2)

# second use cv to get the best effective degree of freedom
fit.smooth.cv=smooth.spline(wage.df$age, wage.df$wage, cv=T)
pred.smooth.cv <- predict(fit.smooth.cv, newdata = list(age=age.grid), se=T)

sprintf("effective degree of freedom found by cv is %s",fit.smooth.cv$df)
title("Smoothing Spline with CV")
plot(wage.df$age, wage.df$wage, col="gray")
lines(age.grid, pred.smooth.cv$fit.cv, col="black", lwd=2)
lines(fit.smooth.cv, col="blue", lwd=2)

# Fit local regression model
plot(wage.df$age, wage.df$wage, col="gray")
title("Local regression span = 0.2")
fit.loess.1 <- loess(wage~age, span = 0.2, data = wage.df)
fit.loess.2 <- loess(wage~age, span = 0.5, data = wage.df)
pred.loess.1 <- predict(fit.loess.1, newdata = tibble(age=age.grid), se=T)
pred.loess.2 <- predict(fit.loess.2, newdata = tibble(age=age.grid), se=T)

lines(age.grid, pred.loess.1$fit, col="blue", lwd=2)
se_bonds.loess.1 <- cbind(pred.loess.1$fit+2*pred.loess.1$se, pred.loess.1$fit-2*pred.loess.1$se)
matlines(age.grid, se_bonds.loess.1, lwd=2, col="red", lty=3)

plot(wage.df$age, wage.df$wage, col="gray")
title("Local regression span = 0.4")
lines(age.grid, pred.loess.2$fit, col="blue", lwd=2)
se_bonds.loess.2 <- cbind(pred.loess.2$fit+2*pred.loess.2$se, pred.loess.2$fit-2*pred.loess.2$se)
matlines(age.grid, se_bonds.loess.2, lwd=2, col="red", lty=3)

```


```{r GAM with CV}
wage.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Wage.csv", 
                      header=T, stringsAsFactors = F, na.strings = "?")
(wage.df.original = tibble(wage.df))
(wage.df = tibble(wage.df))

# first clean up NA and particularly character columns
# 1) remove leading, trainling and empty characters from character columns

trim.f <- function(x) trimws(x, which = c("both")) # leading and trailing spaces
empty.f <- function(x) gsub("^$", NA, x) # empty strings

wage.df[sapply(wage.df, is.character)] <- 
  wage.df %>%
    select(which(sapply(.,is.character))) %>%
    mutate_each(funs(trim.f)) %>%
    mutate_each(funs(empty.f))  %>%
    na.omit


sprintf("Any NA or empty string in character columns recognized: %s",
        !identical(wage.df,wage.df.original) )

# 2) It is safe now to convert all character fields into factors
wage.df[sapply(wage.df, is.character)] <-
  wage.df %>%
  select(which(sapply(., is.character))) %>%
  mutate_each(funs(factor))

print("---------------- GAM with smoothing spline and CV :---------------- ")
library(mgcv)

cv.gam <- gam(logwage~s(age, bs="cr") + s(year, bs="cr", k=5) + education, 
              method="GACV.Cp", scale=-1, data=wage.df,family=Gamma(link=log), 
              gamma=1.4)
print("----------------- cv.gam model ---------------------")
print(cv.gam)
print("------ anova for approximate significance of terms of cv.gam ---------")
anova(cv.gam)
print("----------------- summary cv.gam1 ---------------------")
summary(cv.gam)


plot(cv.gam, too.far=0.15)

#residuals(cv.gam)

#print("----------------- check ---------------------")
#gam.check(cv.gam)

# plot (cv.gam, residuals=T, select=1)
# title("age vs s(age)")
# 
# plot (cv.gam, residuals=T, select=2)
# title("year vs s(year)")

cv.gam1 <- gam(logwage~s(age, bs="cr") + s(year, bs="cr", k=5) +te(age, year, k=5) + education, 
              method="GACV.Cp", scale=-1, data=wage.df,family=Gamma(link=log), 
              gamma=1.4)

print("----------------- cv.gam1 model ---------------------")
print(cv.gam1)
print("------ anova for approximate signifiocance of terms of cv.gam1 ---------")
anova(cv.gam1)
print("----------------- summary cv.gam1 ---------------------")
summary(cv.gam1)

plot(cv.gam1, too.far=0.15)

# predict
pred <- predict(cv.gam, newdata = wage.df, se=TRUE,type="terms")


```

```{r GAM with GCV for stock data}
library(tidyverse)
library(mgcv)
library(purrr)

smarket.df = 
  read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/Smarket.csv", 
           header=T, stringsAsFactors = F, na.strings = "?")

smarket.df = as_tibble(smarket.df)
smarket.df.original = as_tibble(smarket.df)

# first clean up NA and particularly character columns
# 1) remove leading, trainling and empty characters from character columns

trim.f <- function(x) trimws(x, which = c("both")) # leading and trailing spaces
empty.f <- function(x) gsub("^$", NA, x) # empty strings

smarket.df[sapply(smarket.df, is.character)] <- 
  smarket.df %>%
    select(which(sapply(.,is.character))) %>%
    mutate_each(funs(trim.f)) %>%
    mutate_each(funs(empty.f))  %>%
    na.omit


sprintf("Any NA or empty string in character columns recognized: %s",
        !identical(smarket.df,smarket.df.original) )

# 2) It is safe now to convert all character fields into factors
smarket.df[sapply(smarket.df, is.character)] <-
  smarket.df %>%
  select(which(sapply(., is.character))) %>%
  mutate_each(funs(factor))

# split to test train
train <- smarket.df$Year < 2005
test <- !train


# Use Gam
# it returns logit{E(yi)}
cv.gam1 <- gam(Direction~ s(Year, bs="cr", k=4)+
                 s(Volume, k=20)+s(Lag1, k=20),
              method="GACV.Cp", scale=-1, data=smarket.df,subset = train,
              family=binomial(link=logit))
str(smarket.df)
print("----------------- cv.gam model ---------------------")
print(cv.gam1)
print("------ anova for approximate signifiocance of terms of cv.gam ---------")
anova(cv.gam1)
print("----------------- summary cv.gam1 ---------------------")
summary(cv.gam1)
print("----------------- check cv.gam1 ---------------------")
gam.check(cv.gam1)

# plot(cv.gam1, too.far=0.15)


# predict the test model
pred <- predict(cv.gam1, newdata = smarket.df[test,], se=TRUE)
preds <- pred$fit
# Note pred$fit is logit, we need to convert it to probability

logit.inverse <- function(logit) {
  odds <- exp(logit)
  odds/(1+odds)
}

gam.probs <- map(preds, logit.inverse)

# let's draw probabilities vs logit to see if it makes sense
library(ggplot2)
ggplot(tibble(x=preds), aes(x = x)) +
        stat_function(fun = logit.inverse) + 
  scale_x_continuous(name = "logit") +
        scale_y_continuous(name = "Probability")

# calculate test.mse
threshold = 0.5
gam.pred <- ifelse(gam.probs > threshold, "Up", "Down")
gam.obs <- smarket.df[test,]$Direction

library(pROC)

confusion_table <- table(gam.pred, gam.obs)
  
nullClassifier <- max(
    (confusion_table[1,1] + confusion_table[2,1])/(confusion_table[1,1] + confusion_table[2,1]+ confusion_table[1,2] + confusion_table[2,2] ), 
    (confusion_table[1,2] + confusion_table[2,2])/(confusion_table[1,1] + confusion_table[2,1]+ confusion_table[1,2] + confusion_table[2,2] ))

roc_obj <- roc(gam.obs, purrr::flatten_dbl(gam.probs))

# Let's draw some AUC plots
plot(roc_obj, legacy.axes = TRUE)

missclassificationRate <- mean(gam.pred != gam.obs)
FP_rates <- confusion_table[2,1]/(confusion_table[2,1]+ confusion_table[1,1])
TP_rates <- confusion_table[2,2]/(confusion_table[2,2]+ confusion_table[1,2])
precisions <- confusion_table[2,2] / (confusion_table[2,2] + confusion_table[2,1])
specificities <- 1 - confusion_table[2,1]/(confusion_table[2,1]+ confusion_table[1,1])

# overall fraction of wrong predictions:
# print(confusion_table)

# average missclassification error rate
sprintf("GAM classifier : Missclassification test error rate : %s", missclassificationRate)

sprintf("GAM classifier : Null Classifier: %s", nullClassifier)

sprintf("GAM classifier AUC: %s", auc(roc_obj))
# FP rate:
sprintf("GAM classifier : FP rate (TypeI error, 1 - specificity) : %s", FP_rates)

# TP rate:
sprintf("GAM classifier : TP rate (1-TypeII error, power, sensetivity, recall) : %s", TP_rates)

# precision:
sprintf("GAM classifier : precision: %s", precisions)

# specificity 1-FP/N:
sprintf("GAM classifier : specificity 1-FP/N: %s", specificities)


```


```{r loess with CV}
library(tidyverse)
library(mgcv)
library(purrr)

locv1 <- function(x1, y1, nd, span, ntrial)
{
  locvgcv <- function(sp, x1, y1)
  {
    nd <- length(x1)

    assign("data1", data.frame(xx1 = x1, yy1 = y1))
    fit.lo <- loess(yy1 ~ xx1, data = data1, span = sp, family = "gaussian", degree = 2, surface = "direct")
    res <- residuals(fit.lo)

    dhat2 <- function(x1, sp)
    {
        nd2 <- length(x1)
        diag1 <- diag(nd2)
        dhat <- rep(0, length = nd2)

        for(jj in 1:nd2){
            y2 <- diag1[, jj]
            assign("data1", data.frame(xx1 = x1, yy1 = y2))
            fit.lo <- loess(yy1 ~ xx1, data = data1, span = sp, family = "gaussian", degree = 2, surface = "direct")
            ey <- fitted.values(fit.lo)
            dhat[jj] <- ey[jj]
            }
            return(dhat)
        }

        dhat <- dhat2(x1, sp)
        trhat <- sum(dhat)
        sse <- sum(res^2)

        cv <- sum((res/(1 - dhat))^2)/nd
        gcv <- sse/(nd * (1 - (trhat/nd))^2)

        return(gcv)
    }

    gcv <- lapply(as.list(span1), locvgcv, x1 = x1, y1 = y1)
    #cvgcv <- unlist(cvgcv)
    #cv <- cvgcv[attr(cvgcv, "names") == "cv"]
    #gcv <- cvgcv[attr(cvgcv, "names") == "gcv"]

    return(gcv)
}
```

```{r Exercise 6 part a}
library(tidyverse)
library(mgcv)


wage.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Wage.csv", header=T, stringsAsFactors = F, na.strings = "?")
wage.df.original = tibble(wage.df)
wage.df = tibble(wage.df)

# remove empty characters and NA helper
remove.empty.characters <- function(df) 
  df %>%
    select_all %>%
    filter_if(is.character, all_vars(!is.na(.) & trimws(.) != ""))

# Next remove leading and trailing spaces from all elements in character columns
trim.f <- function(col, na.rm = F) { 
  isNA <- !reduce(col, ~ (is.na(.x) & is.na(.y)))
  if (na.rm && isNA) 
    unlist(map(col, ~ (if (is.na(.x)) "" else .x) ),use.names = F)
  else trimws(col, which = c("both")) # leading and trailing spaces
}

trim.spaces <- function(df)
  df %>%
    mutate_if(is.character, trim.f, na.rm = T)

# Finally convert character columns to factor

char.to.fctor <- function(df)
  df %>%
    mutate_if(is.character, ~ factor(.x, levels = (.x %>% table() %>% names())))


wage.df <- 
  wage.df %>%
  na.omit() %>%
  trim.spaces() %>%
  remove.empty.characters() %>%
  char.to.fctor()
  

# Polynomial regression to predict wage using age and find the degree with cv
set.seed(1)
k <- 10
degrees <- 1:5

# create k folds
folds <- sample(1:k, nrow(wage.df), replace = T)

cv.result <- tibble(degree=NULL, cv.mse = NULL)

for (degree in degrees){
  test.mses <- double(length(folds))
  for (fold in folds){
    fit.poly <- lm(wage ~ poly(age, degree), data = wage.df[folds != fold, ])
    predicts <- predict(fit.poly, 
                        newdata = list(age=wage.df[folds == fold, ]$age), se=T)
    testMSE <- mean((predicts$fit - wage.df[folds == fold, ]$wage)^2)
    test.mses <- c(test.mses, testMSE)
  }
  cv.result<- rbind(cv.result, tibble(degree = degree, cv.mse=mean(test.mses)))

}
cv.result

# Among ploynimials of degrees 1 to 5 , CV shows ploynomial with degree 
# 4 has smallest MSE.
# ----------- Expect to get the same result when using ANOVA ------------------ 
# perform an analysis of variance (ANOVA, using an F-test) in order to test the 
# null hypothesis that a model M1 is sufficient to explain the data against the 
# alternative hypothesis that a more complex model M2 is required.
# M1 and M2 must be nested models. Model M1 ploynomial with smaller degree is nested 
# in model M2 which has polynomial with larger degree.

# Note that ANOVA is all  based on training data only
nested.models <- 1:5 %>% map(~lm(wage ~ poly(age, .), data = wage.df))
do.call("anova", nested.models)

# ploynomial with degree 1 comparing with ploynomial with degree 2 
# has almost zero p-value (2.2e-16) showing model is not enough.
# similarly ploynimial degree 2 comparing with degree 3 shows smaller 
# p-value. Comparing Plynomial degree 3 to degree 4 show still smaller p-value 
# but ploynomial with degree 4 has the best p-value(0.051046)
# Ploynomial with degree 5 is unnecessarily too complex (p-va;ue > 0.05).
# So models with plynomial degree 3 or 4 are best fit, which is align
# with whqt we got from CV.

# --------------  Use orthogonal polynomial to find p-values -----------------

# We can use the fact that poly() creates orthogonal polynomials.'
# so we can obtain p-values for each model using summary() function:
# note that square of t-statistics here is equal to -statistics from anova()

coef(summary(nested.models[[5]]))


# ANOVA method works whether or not we used orthogonal polynomials
# particularly when we have other terms in the model


# lets draw the prediction
age.range <- range(wage.df$age)
age.grid <- seq (age.range[[1]], age.range[[2]])
preds <- predict(lm(wage ~ poly(age, 4), data = wage.df), newdata = tibble(age = age.grid), se=T)
se.bands <- cbind(preds$fit + 2 * preds$se.fit , preds$fit - 2 * preds$se.fit )
plot(wage.df$age, wage.df$wage, xlim = age.range, cex=0.5, col="darkgray")
title("Degree 4 polynomial ", outer = T)
lines(age.grid, preds$fit, lwd=2, col="blue")
matlines(age.grid, se.bands, lwd=1, col="red", lty=3)
```


```{r Exercise 6 part b}
library(tidyverse)
library(mgcv)


wage.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Wage.csv", header=T, stringsAsFactors = F, na.strings = "?")
wage.df.original = tibble(wage.df)
wage.df = tibble(wage.df)

# remove empty characters and NA helper
remove.empty.characters <- function(df) 
  df %>%
    select_all %>%
    filter_if(is.character, all_vars(!is.na(.) & trimws(.) != ""))

# Next remove leading and trailing spaces from all elements in character columns
trim.f <- function(col, na.rm = F) { 
  isNA <- !reduce(col, ~ (is.na(.x) & is.na(.y)))
  if (na.rm && isNA) 
    unlist(map(col, ~ (if (is.na(.x)) "" else .x) ),use.names = F)
  else trimws(col, which = c("both")) # leading and trailing spaces
}

trim.spaces <- function(df)
  df %>%
    mutate_if(is.character, trim.f, na.rm = T)

# Finally convert character columns to factor

char.to.fctor <- function(df)
  df %>%
    mutate_if(is.character, ~ factor(.x, levels = (.x %>% table() %>% names())))


wage.df <- 
  wage.df %>%
  na.omit() %>%
  trim.spaces() %>%
  remove.empty.characters() %>%
  char.to.fctor()

# Fit a step function to predict wage using age
# use cv to find optimal number of cuts

# we use cut function to unformly cut the quantiles
table(cut(wage.df$age, 4))


# use cv to find number of cuts
set.seed(12)
cuts <- 2:10
k = 10
row.indices <- sample(1:k, nrow(wage.df), replace = T)

cv.result <- tibble(cuts = NULL, cv.mse = NULL)
for (k in cuts){
  # cut needs to have its own column in dataframe

  wage.df$age_cut <- cut(wage.df$age, k)
  cv.mses <- double(length(cuts))
  for (row.index.test in row.indices){
    #  If we do below line we get : "factor cut(age, k) has new levels "
    # problem is cut(age, k) existed only as an inline creation within your lm()   
    #  step.model <- lm(wage~cut(age, k), data=wage.df[row.indices != row.index.test, ])
    step.model <- lm(wage~age_cut, data=wage.df[row.indices != row.index.test, ])
    preds <- predict(step.model, newdata=wage.df[row.indices == row.index.test, ], se=T)
    cv.mses = c(cv.mses, mean((preds$fit - wage.df[row.indices == row.index.test, ]$wage)^2))
  }
  cv.result <- rbind(cv.result , tibble(cuts = k, cv.mse = mean(cv.mses))) 
}
cv.result

plot(cv.result,xlab='Number of cuts',ylab='Cross Validation Error',type='l')
x=which.min(cv.result$cv.mse)
points(x,cv.result[x, ]$cv.mse,pch=20,cex=2,col='red')

# cv shows 8 cuts is the best, lets make a model with 8 cuts:
wage.df$age_cut <- cut(wage.df$age, 8)
step.model <- lm(wage~age_cut, data=wage.df[row.indices != row.index.test, ])
preds <- predict(step.model, newdata=wage.df[row.indices == row.index.test, ], se=T)

# lets draw the prediction
age.range <- range(wage.df$age)
age.grid <- seq (age.range[[1]], age.range[[2]])
age.grid.cut = cut(age.grid, 8)
preds <- predict(step.model, newdata = tibble(age_cut = age.grid.cut), se=T)
se.bands <- cbind(preds$fit + 2 * preds$se.fit , preds$fit - 2 * preds$se.fit )
plot(wage.df$age, wage.df$wage, xlim = age.range, cex=0.5, col="darkgray")
title("step function with 8 cuts ", outer = T)
lines(age.grid, preds$fit, lwd=2, col="blue")
matlines(age.grid, se.bands, lwd=1, col="red", lty=3)

```

```{r Exercise 7}

# Explore the relationships between some of these other predictors and wage, 
# and use non-linear fitting techniques in order to fit flexible models to the data. 
# Create plots of the results obtained, and write a summary of your findings.

library(boot)
library(tidyverse)
library(mgcv)


wage.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Wage.csv", header=T, stringsAsFactors = F, na.strings = "?")
wage.df.original = tibble(wage.df)
wage.df = tibble(wage.df)

# remove empty characters and NA helper
remove.empty.characters <- function(df) 
  df %>%
    select_all %>%
    filter_if(is.character, all_vars(!is.na(.) & trimws(.) != ""))

# Next remove leading and trailing spaces from all elements in character columns
trim.f <- function(col, na.rm = F) { 
  isNA <- !reduce(col, ~ (is.na(.x) & is.na(.y)))
  if (na.rm && isNA) 
    unlist(map(col, ~ (if (is.na(.x)) "" else .x) ),use.names = F)
  else trimws(col, which = c("both")) # leading and trailing spaces
}

trim.spaces <- function(df)
  df %>%
    mutate_if(is.character, trim.f, na.rm = T)

# Finally convert character columns to factor

char.to.fctor <- function(df)
  df %>%
    mutate_if(is.character, ~ factor(.x, levels = (.x %>% table() %>% names())))


wage.df <- 
  wage.df %>%
  na.omit() %>%
  trim.spaces() %>%
  remove.empty.characters() %>%
  char.to.fctor()

# "sex" and "region" are factors with single levels are constants and to be rmoved
wage.df <- wage.df %>%
  select(-c(sex, region))

# wage (DV) is factor/continous (and hoefully normal). 
# To see its relation with 1 or more interval IVs and/or 1 or more categorical IVs
# we use multiple regression
#(see: https://stats.idre.ucla.edu/other/mult-pkg/whatstat/)

summary(glm(wage ~ ., data = wage.df))

# summary shows only "maritl", "education", "health_ins"
# are sinnificantly important, in other word  "wage"
# has relationship with these 4 DV

# First note that all the IVs that has relationship with DV are factors so we 
# use step function to model them. Let's model each one separately using step function:
library(grid)
library(gridExtra)
fit.step.model <- function(df, DV, IVs, seed=1113){
  frm <- as.formula(glue::glue(DV,"~", str_c(IVs, collapse = "+")))
  model.fit <- glm(frm, data = df)
  new.sample <- sample( levels(df [[IVs]]), 1000, replace=T)
  new.df = tibble(factor(new.sample))
  names(new.df) <- IVs
  preds <- predict(model.fit, newdata=new.df, se=T)
  
  se.bands <- cbind(preds$fit + 2 * preds$se.fit , preds$fit - 2 * preds$se.fit )
  
  data.to.draw <- tibble(x = factor(new.sample), y = preds$fit, 
                         y.se.1 = preds$fit + 2 * preds$se.fit, 
                         y.se.2 = preds$fit - 2 * preds$se.fit)
  
  # draw box plots for quality columns
  g1 <- ggplot(df, aes(factor(.data[[IVs]]), .data[[DV]], color = factor(.data[[IVs]]))) +
    geom_bar(aes(factor(.data[[IVs]]), .data[[DV]]),  stat = "summary", fun.y = mean) +
    stat_summary(fun.data = mean_sdl, width=0.05, geom = "errorbar",fun.y = mean) 

  
  #the sample we predict on
  g2 <- ggplot(data.to.draw, aes(x=x)) +
    geom_boxplot(aes(y = y, colour = DV)) +
    geom_boxplot(aes(y = y.se.1, colour = "standard error")) +
    geom_boxplot(aes(y = y.se.2, colour = "standard error"))

  grid.arrange(g1, g2, nrow = 1,
             top = textGrob("Step function fitted to a qualitative data"))
}

fit.step.model(wage.df, "wage", "maritl")
fit.step.model(wage.df, "wage", "education")
fit.step.model(wage.df, "wage", "health_ins")



```

```{r Exercise 8}
library(boot)
library(tidyverse)
library(dataPreparation)
library(mgcv)

auto.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Auto.csv", header=T, stringsAsFactors = F, na.strings = "?")
auto.df.original = tibble(auto.df)
auto.df = tibble(auto.df)

# remove empty characters and NA helper
remove.empty.characters <- function(df) 
  df %>%
    select_all %>%
    filter_if(is.character, all_vars(!is.na(.) & trimws(.) != ""))

# Next remove leading and trailing spaces from all elements in character columns
trim.f <- function(col, na.rm = F) { 
  isNA <- !reduce(col, ~ (is.na(.x) & is.na(.y)))
  if (na.rm && isNA) 
    unlist(map(col, ~ (if (is.na(.x)) "" else .x) ),use.names = F)
  else trimws(col, which = c("both")) # leading and trailing spaces
}

trim.spaces <- function(df)
  df %>%
    mutate_if(is.character, trim.f, na.rm = T)

# Finally convert character columns to factor

char.to.fctor <- function(df)
  df %>%
    mutate_if(is.character, ~ factor(.x, levels = (.x %>% table() %>% names())))


auto.df <- 
  auto.df %>%
  na.omit() %>%
  trim.spaces() %>%
  remove.empty.characters() %>%
  char.to.fctor()

# remove constant variables
(constant_cols <- whichAreConstant(auto.df))

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(auto.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(auto.df))

# we fit smoothing spine and step function using GAM'
# bs="cr" cubic regression spline
# Gamma is usefull because DV (i.e mpg) is strictly positive real valued
# also defaukt link is used in some waiting time applications, log link is most often used
# Use GCV.CP to estimate the parameter
# k-1 is upper limit for degree of freedom using GCV
# gamma = 1.2 since GCV tends to slightly overfit
# bs="cr" is cubic regression , default is thin plate spline which is computatinally very heavy

model.fit <- gam(mpg ~ s(displacement, bs="cr", k= 13) + 
             s(horsepower, bs="cr", k= 13) + s(weight, bs="cr", k= 13) +
             s(acceleration, bs="cr", k= 13) + name + cylinders + year + origin, 
             method="GACV.Cp", scale=-1,family = Gamma(link=log), data=auto.df, gamma = 1.2)

model.fit

plot(model.fit,residuals=TRUE,pch=19)

gam.check(model.fit)
plot(fitted(model.fit),residuals(model.fit))
plot(auto.df$displacement,residuals(model.fit))
plot(auto.df$horsepower,residuals(model.fit))
plot(auto.df$weight,residuals(model.fit))
# vis.gam(model.fit,theta=-45,ticktype="detailed",se=2)
# vis.gam(model.fit,theta=30,ticktype="detailed")
# vis.gam(model.fit,plot.type="contour")

# Model shows there is a non linear relationship between 
# mpg on one hand and  horsepower and acceleration on the other hand
# however mpgg has linear relation with displacement and 


# to check the robostness model with different criterion 
# (i.e. "ML" or "REML" versus "GCV.Cp" or "GACV.Cp")
# because likelihood based methods tend to be more robust

model.fit1 <- gam(mpg ~ s(displacement, bs="cr", k= 13) + 
             s(horsepower, bs="cr", k= 13) + s(weight, bs="cr", k= 13) +
             s(acceleration, bs="cr", k= 13) + name + cylinders + year + origin, 
             method="ML", scale=-1,family = Gamma(link=log), data=auto.df, gamma = 1)

model.fit1

plot(model.fit1,residuals=TRUE,pch=19)

gam.check(model.fit1)
plot(auto.df$displacement,residuals(model.fit1))
plot(auto.df$horsepower,residuals(model.fit1))
plot(auto.df$weight,residuals(model.fit1))

# Clearly ML has a tendency to overfit, therefore we see all models have high edf 
# vis.gam(model.fit1,theta=-45,ticktype="detailed",se=2)
# vis.gam(model.fit1,theta=30,ticktype="detailed")
# vis.gam(model.fit1,plot.type="contour")

```
```{r Exercise 9}
library(boot)
library(tidyverse)
library(dataPreparation)
library(splines)

boston.df = read.csv(
  "/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/BostonHousing.csv", 
  header=T, stringsAsFactors = F, na.strings = "?")

boston.df <- tibble(boston.df)

# see if there is any NA in any records
nrow(df[which(is.na(boston.df)),])

# No NA anywhere , let's do usual clean up:

# remove constant variables
constant_cols <- whichAreConstant(boston.df)

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(boston.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(boston.df))

# a) use ploynomial regression to predict nox using dis

# Now fit a 4 degree polynomial
fit.poly <- glm(nox ~ poly(dis, 3), data = boston.df)

# Here is the regression output
# polynomial coefficients are all statistically significamt
summary(fit.poly)

# draw standard error 
# first get the range of the values of dis
(dis.limits  <- range(boston.df$dis))

# Now create an interaval from these range values
(dis.grid <- seq(from=dis.limits[1], to=dis.limits[2]))

# predict value for this interval using fitted model
predicts <- predict(fit.poly, newdata = tibble(dis=dis.grid), se=T)
names(predicts)

se.bands <- cbind(predicts$fit + 2*predicts$se.fit, 
                  predicts$fit - 2*predicts$se.fit)

# now plot the predicts
plot(boston.df$dis, boston.df$nox, xlim=dis.limits, cex=0.5, col="darkgrey")
title("Degree 3 polynomial", outer = F)
lines(dis.grid, predicts$fit, lwd=2, col = "blue")
matlines(dis.grid, se.bands, lwd=2, col="red", lty=3)



res <- residuals(fit.poly, type = "response")
# ensure that x- and y-axis have the same range
pred <- fit.poly$fitted.values

observed <- boston.df$nox
# determine maximal range
val.range <- range(pred, observed)
plot(observed, pred, 
    xlim = val.range, ylim = val.range,  
    xlab = "Observed nax", 
    ylab = "Predicted nox",
    main = "Residuals of the linear model for the training data")
# show ideal prediction as a diagonal
abline(0,1, col = "red")
# add residuals to the plot
segments(observed, pred, observed, pred + res)

# calculate RSS
rss <- res %>% reduce( ~.x + .y^2 , .init = 0) %>% unlist


# ----------------------------------------------------------------
# b) plot plynomial fit for a range of degrees and report each rss
# ----------------------------------------------------------------
fit.ploys.fun <- function (df, IV, DV, i) {
    
  # Now fit a polynomial with degree i
  frm <- as.formula(glue::glue(DV," ~ poly(",IV,",",i,")"))
  fit.poly <- glm(frm, data = df)

  # Here is the regression output
  # polynomial coefficients are all statistically significamt
  summary(fit.poly)
  
  # draw standard error 
  # first get the range of the values of dis
  IV.limits  <- range(df[[IV]])
  
  # Now create an interaval from these range values
  IV.grid <- seq(from=IV.limits[1], to=IV.limits[2])
  
  # predict value for this interval using fitted model
  new.data <- tibble(IV.grid)
  names(new.data)  <- IV
  predicts <- predict(fit.poly, newdata = new.data, se=T)
  
  se.bands <- cbind(predicts$fit + 2*predicts$se.fit, 
                    predicts$fit - 2*predicts$se.fit)
  
  # now plot the predicts
  plot(df[[IV]], df[[DV]], xlim=IV.limits, cex=0.5, col="darkgrey")
  title(glue::glue("Degree ",i, " polynomial"), outer = F)
  lines(IV.grid, predicts$fit, lwd=2, col = "blue")
  matlines(IV.grid, se.bands, lwd=2, col="red", lty=3)
  
  res <- residuals(fit.poly, type = "response")
  
  # calculate RSS
  res %>% reduce( ~.x + .y^2 , .init = 0) %>% unlist

}

# RSS for ploynomilas degree 1 to 10 
1:10 %>% reduce (~append(.x,fit.ploys.fun(boston.df, "dis", "nox", .y)), 
                 .init = NULL)

# c) perform cross validation to find optimal degree for cross validation

set.seed(1113)
k <- 10
degrees <- 2:10

# bootstrap resampling
row.indices <- sample(1:k, nrow(boston.df), replace = T)

results <- tibble(degrees=NULL, cv.mse=NULL)

poly.cv <- function(df, DV, IV, row.indices, degree){
  test.mses <- double(k)
  for(test.index in 1:k){
    frm <- formula(glue::glue(DV , "~" , "poly(",IV,",",degree,")"))
    train.model <- glm(frm, data = df[row.indices != test.index,])
    
    # now predict it on test rows
    new.data <- tibble(x = df[row.indices == test.index,][[IV]])
    names(new.data) <- IV
    predicts <- predict(train.model, newdata = new.data, se=T)
    test.mses <- c(test.mses, 
                   mean((df[row.indices == test.index,][[DV]] - predicts$fit)^2))
  }
  tibble(degrees=degree, cv.mse=mean(test.mses))
}

results <- 
  degrees %>% 
  reduce(~rbind(.x,poly.cv(boston.df, "nox", "dis",row.indices, .y)), 
                   .init = results)

# plot the min degree
plot(results, xlab = "polynomial degrees", ylab="CVMSE",type = "l")

index <- which.min(results$cv.mse)
points(index, results$cv.mse[index], col="red", cex=2, pch=20)

# so degree 2 is most optimized degree for the polynomial based on CV


#d) use bs() to fit regression spline to predict nox using dis using 4 degrees of freedom
k <- 4

fit <- glm(nox~bs(dis, knots = cut(dis, k)) ,data=boston.df)

# first get the range of the values of dis
(dis.limits  <- range(boston.df$dis))

# Now create an interaval from these range values
dis.grid <- seq(from=dis.limits[1], to=dis.limits[2], length.out = 100)

# predict the nox values 
pred <- predict(fit, newdata = list(dis=dis.grid), se=T)
plot(boston.df$dis, boston.df$nox, col="gray")
lines(dis.grid, pred$fit, lwd=2)
se_bonds <- cbind(pred$fit+2*pred$se, pred$fit-2*pred$se)
matlines(dis.grid, se_bonds, lwd=2, col="red", lty=3)

# knots are chosen uniformly
# Since Boston DF is too small , only 3 degree os freedom is chosen by R

# e) fit a regression splines with range of degrees of freedom and reort resulting RSS
regression.spline <- function(k){
  fit <- glm(nox~bs(dis, df = k) ,data=boston.df)
  
  # first get the range of the values of dis
  (dis.limits  <- range(boston.df$dis))
  
  # Now create an interaval from these range values
  (dis.grid <- seq(from=dis.limits[1], to=dis.limits[2]))
  
  # predict the nox values 
  pred <- predict(fit, newdata = list(dis=dis.grid), se=T)
  plot(boston.df$dis, boston.df$nox, col="gray")
  title(glue::glue("Degree of freedom: ", k), outer = F)
  lines(dis.grid, pred$fit, lwd=2)
  se_bonds <- cbind(pred$fit+2*pred$se, pred$fit-2*pred$se)
  matlines(dis.grid, se_bonds, lwd=2, col="red", lty=3)
  
  rse <- residuals(fit, type = "response") 
  
  # calculate RSS
  res %>% reduce( ~.x + .y^2 , .init = 0) %>% unlist
    
}
regression.spline(2)
regression.spline(3)
regression.spline(4)
results <- tibble(degree.of.freedom = NULL, rse=NULL)
results <- 2:10 %>%
  reduce(~rbind(.x, tibble(degree.of.freedom = .y , 
                           rse = regression.spline(.y))), 
         .init=results)
  
results

# Since Boston DF is too small , only 3 degree os freedom is chosen by R

# f) Perform cross validation to select the best fegree of freedom
 # since number of data is not enough , matrix of the model will have
 # rank defficiency do cross validation would give the same result.



```

```{r Exercise 10}

library(leaps)
library(tidyverse)
library(dataPreparation)
library(mgcv)

# first define some helpers 
# remove empty characters and NA helper
remove.empty.characters <- function(df) 
  df %>%
    select_all %>%
    filter_if(is.character, all_vars(!is.na(.) & trimws(.) != ""))

# Next remove leading and trailing spaces from all elements in character columns
trim.f <- function(col, na.rm = F) { 
  isNA <- !reduce(col, ~ (is.na(.x) & is.na(.y)))
  if (na.rm && isNA) 
    unlist(map(col, ~ (if (is.na(.x)) "" else .x) ),use.names = F)
  else trimws(col, which = c("both")) # leading and trailing spaces
}

trim.spaces <- function(df)
  df %>%
    mutate_if(is.character, trim.f, na.rm = T)

# Finally convert character columns to factor

char.to.fctor <- function(df)
  df %>%
    mutate_if(is.character, ~ factor(.x, levels = (.x %>% table() %>% names())))

# ------------  read the data ----------------

college.df = read.csv(
  "/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/College.csv", 
  header=T, stringsAsFactors = F, na.strings = "?")

college.df <- tibble(college.df)

# a) First and foremost lets split data to 80% train and the rest test 

set.seed(1113)
train.idx <- sample(1:nrow(college.df), 0.8*nrow(college.df))
test.idx <- setdiff(1:nrow(college.df), train.idx)

train.df <- college.df[train.idx, ]
test.df <- college.df[test.idx, ]

# --------------- prepare train data ----------------------
# see if there is any NA in any records
nrow(df[which(is.na(train.df)),])

# No NA anywhere , let's do usual clean up:

# remove constant variables
constant_cols <- whichAreConstant(train.df)

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(train.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(train.df))

# It is safe now to convert all character fields into factors

train.df <- 
  train.df %>%
  na.omit() %>%
  trim.spaces() %>%
  remove.empty.characters() %>%
  char.to.fctor()

# --------------- prepare test data ----------------------

# see if there is any NA in any records
nrow(df[which(is.na(test.df)),])

# No NA anywhere , let's do usual clean up:

# remove constant variables
constant_cols <- whichAreConstant(test.df)

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(test.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(test.df))

# It is safe now to convert all character fields into factors

test.df <- 
  test.df %>%
  na.omit() %>%
  trim.spaces() %>%
  remove.empty.characters() %>%
  char.to.fctor()


# ---------- stepwise forward feature selectin with CV -----------#

k.fold <- 10

set.seed(1113)

# create k folds
folds <- sample(1:k.fold, nrow(train.df), replace = T)

# number of features
noOfFeatures <- ncol(train.df) -1


cv.errors <- matrix(NA, k.fold, noOfFeatures, 
                    dimnames = list(NULL, paste(1:noOfFeatures)))


# perform a cross validation on a for loop
for (j in 1:k.fold){
  # step# 2 of algorithm 6.2 page 207 is evaluated on all folds except one of 
  # them each time it chooses best models with number of features 
  # 1,2,..., noOfFeatures on k-1 training folds.
  best.fit <- regsubsets(Outstate ~ ., data = train.df[folds != j, ], 
                         nvmax = noOfFeatures, method = "forward")

  # now compute CV test error for each of  models that have best number of 
  # predictors on test fold # j 
  for(i in 1:noOfFeatures){
    # extract coefficients for model # i
    coefi <- coef(best.fit, id = i)
    
    # For GAM we are interested in name 
    # of features for model # i (not their coefficients)
    # except intercept
    col.names <- names(coefi)[-1]
    
    # first separate factors , then remove "Yes" from the end of factor name and 
    #finally drop "(Intercept)"
    
    factor.post.script <- "Yes"
    intercept.name <- "(Intercept)"

    regular.features <- col.names %>% 
      keep(!str_detect(., factor.post.script)) %>%
      discard(str_detect(., intercept.name))
    
    factor.features <- col.names %>% 
        keep(str_detect(., factor.post.script)) %>%
        str_replace(factor.post.script, "")
        
    # now dynamically build the formula
    
    k = 10
    bs = "\"cr\""
    
    regular.part <- regular.features %>%
      map(~glue::glue("s(",.,", bs=",bs,","," k=",k,")")) %>%
      str_c(., collapse = " + ")
    
    factor.part <- factor.features %>% str_c(., collapse = " + ")
    
    formula <- 
      case_when(
        factor.part != "" & regular.part != "" ~ glue::glue("Outstate ~ " , 
                                                            regular.part , 
                                                            " + ", 
                                                            factor.part),
        
        factor.part != "" & regular.part == "" ~ glue::glue("Outstate ~ " , 
                                                            factor.part),
  
        factor.part == "" & regular.part != "" ~ glue::glue("Outstate ~ " , 
                                                    regular.part)
      )
      

    model.fit <- gam(as.formula(formula),method="GACV.Cp", scale=-1,
                                family = Gamma(link=log), 
                                data=train.df[folds != j, ], gamma = 1)
    
#    apply model on test fold and accumulate the test errors
    preds <- predict(model.fit, newdata = train.df[folds == j, ], se=TRUE)
    cv.errors[j,i] <- mean ((preds$fit - train.df[folds == j, ]$Outstate)^2)

  }
}

# finally calculate mean of CV MSE error for each model
cv.error.means <- rep(NA, ncol(cv.errors))
for (l in 1:ncol(cv.errors)){
  cv.error.means[l] <- mean(cv.errors[ ,l])
}


print("most optimal No. of selected covariates")
(covariates.selected <- which.min(cv.error.means))

gam.best.fit <- regsubsets(Outstate ~ ., data = train.df, 
                         nvmax = ncol(train.df), method = "forward")

plot(1/nrow(train.df)*summary(gam.best.fit)$rss,type='l',xlab='Number of Predictors',ylab='Train MSE Score',xaxt='n')
axis(side=1,at=seq(1,17,2),labels = seq(1,17,2))



print("Seems like 7 is the good choice selected covariates:")
which(summary(gam.best.fit)$which[7,-1])

(covariates.selected.names <- names(coef(gam.best.fit, id = 7)))

# "Private", "Room.Board", "Personal", "Terminal", "perc.alumni", "Expend", "Grad.Rate"  


# b) ---------------- Apply GAM ---------------------

# first separate factors , then remove "Yes" ate the end of factor name and finally drop intercept
factor.post.script <- "Yes"
intercept.name <- "(Intercept)"

regular.features <- covariates.selected.names %>% 
  keep(!str_detect(., factor.post.script)) %>%
  discard(str_detect(., intercept.name))

factor.features <- covariates.selected.names %>% 
    keep(str_detect(., factor.post.script)) %>%
    str_replace(factor.post.script, "")
    
# now dynamically build the formula

k = 20
bs = "\"cr\""

# regular.part <- regular.features %>%
#   map(~glue::glue("s(",.,")")) %>%
#   str_c(., collapse = " + ")


regular.part <- regular.features %>%
  map(~glue::glue("s(",.,", bs=",bs,","," k=",k,")")) %>%
  str_c(., collapse = " + ")

factor.part <- factor.features %>% str_c(., collapse = " + ")
  
formula <- 
  case_when(
    factor.part != "" & regular.part != "" ~ glue::glue("Outstate ~ " , 
                                                        regular.part , 
                                                        " + ", 
                                                        factor.part),
    
    factor.part != "" & regular.part == "" ~ glue::glue("Outstate ~ " , 
                                                        factor.part),

    factor.part == "" & regular.part != "" ~ glue::glue("Outstate ~ " , 
                                                regular.part)
  )

# "Private", "Room.Board", "Personal", "Terminal", "perc.alumni", "Expend", "Grad.Rate"  


print ("-------------- GAM -----------------")
best.model.fit <- gam(as.formula(formula),method="GACV.Cp", scale=-1,
                            family = Gamma(link=log), 
                            data=train.df, gamma = 1.5)

print.gam(best.model.fit)

print ("-------------- model checking -----------------")
gam.check(best.model.fit)

# finally draw the plots
plot.gam(best.model.fit)

rsd <- residuals(best.model.fit,type="deviance")
print(" ------ Check value of K is large enough for Room.Board ------------")
gam(rsd~s(Room.Board,k=20)-1,data=train.df,select=TRUE)  # -1 supresses the intercept

print(" ------ Check value of K is large enough for Personal ------------")
gam(rsd~s(Personal,k=20)-1,data=train.df,select=TRUE)  # -1 supresses the intercept

print(" ------ Check value of K is large enough for Terminal ------------")
gam(rsd~s(Terminal,k=20)-1,data=train.df,select=TRUE)  # -1 supresses the intercept

print(" ------ Check value of K is large enough for perc.alumni ------------")
gam(rsd~s(perc.alumni,k=20)-1,data=train.df,select=TRUE)  # -1 supresses the intercept

print(" ------ Check value of K is large enough for Expend ------------")
gam(rsd~s(Expend,k=20)-1,data=train.df,select=TRUE)  # -1 supresses the intercept

print(" ------ Check value of K is large enough for Grad.Rate ------------")
gam(rsd~s(Grad.Rate,k=20)-1,data=train.df,select=TRUE)  # -1 supresses the intercept


print("We can extract the lambdai estimates, and, for RE/ML smoothness
selection, the covariance matrix of the log lambdai estimates.")
best.model.fit$sp

# Alternativly
#gam.vcomp(best.model.fit)

print(" Seems like the only statistically significant covariate is Room.Board")
anova(best.model.fit)

best.preds <- predict(best.model.fit, newdata = test.df, se=TRUE)

# print(" matrix mapping the estimated coefficients to the linear predictor:")
# Xp <- predict(best.model.fit,newdata=test.df,type="lpmatrix")
# Xp%*%coef(best.model.fit) ## result same as predict(ct1,pd)

# let's see how much variance explained by model
tss <- mean((test.df$Outstate - mean(test.df$Outstate))^2)
rss <- mean ((test.df$Outstate - best.preds$fit)^2)
(r.squared <- 1 - rss/tss)


# Clearly  many of the predictors are NOT statistically significant and thus
# the model does not explain any variation in response.

plot(best.model.fit, too.far=0.15)


print("-------------- use gam library to get a better result -----------------") 
library(gam)

regular.part <- regular.features %>%
  map(~glue::glue("s(",.,,")")) %>%
  str_c(., collapse = " + ")

factor.part <- factor.features %>% str_c(., collapse = " + ")
  
formula <- 
  case_when(
    factor.part != "" & regular.part != "" ~ glue::glue("Outstate ~ " , 
                                                        regular.part , 
                                                        " + ", 
                                                        factor.part),
    
    factor.part != "" & regular.part == "" ~ glue::glue("Outstate ~ " , 
                                                        factor.part),

    factor.part == "" & regular.part != "" ~ glue::glue("Outstate ~ " , 
                                                regular.part)
  )


best.model.fit = gam(as.formula(formula), data = train.df)

best.preds <- predict(best.model.fit, newdata = test.df)

# let's see how much variance explained by model
tss <- mean((test.df$Outstate - mean(test.df$Outstate))^2)
rss <- mean ((test.df$Outstate - best.preds)^2)

print("seems like with 'gam' librry we get more sensible result: ")
(r.squared <- 1 - rss/tss)

print("in this model all chosen covariates are statistically significant")
summary(best.model.fit)


plot(best.model.fit, se = T, col = "blue")
print("Plot shows all covariates except 'private' are non linear")

```
```{r Exercise 11}
library(tidyverse)
# a) Generate response Y and predictor X1 and X2 using n = 100

set.seed(1)
x1 <- rnorm(100)
x2 <- rnorm(100)
y <- 1003.24 + 0.002*x1 - 0.0089*x2+rnorm(100)

# b) initialize beta1.hat
beta1.hat <- 0.2

# c) keep beta1.hat fixed and fit the model 
a <- y - beta1.hat * x1
beta2.hat <- lm(a ~ x2)$coef[[2]]

# d) keep beta2.hat fixed and fit the model 
a <- y - beta2.hat * x2
beta1.hat <- lm(a ~ x1)$coef[[2]]
beta1.hat

# e) repeat c and d above 100 times and plot the values

beta1.hat <- -13.3
result <- tibble(b0=0, b1=beta1.hat, b2=0)

find.betas <- function(acc){
  beta1.hat <- acc[nrow(acc), ]$b1
  a1 <- y - beta1.hat * x1
  model1 <- lm(a1 ~ x2)
  beta2.hat <- model1$coef[[2]]

  a2 <- y - beta2.hat * x2
  model2 <- lm(a2 ~ x1)
  beta1.hat <- model2$coef[[2]]
  beta0.hat <- model2$coef[[1]]
  
  rbind(acc ,tibble(b0 = beta0.hat, b1 = beta1.hat, b2 = beta2.hat))
}

results <- 1:1000 %>%
  reduce(~find.betas (.x), .init=result)

dim(results)

plot(1:1001, results$b0, type = "l", xlab = "iteration", ylab = "betas", ylim = c(-2.2, 
    1.6), col = "green")
lines(1:1001, results$b1, col = "red")
lines(1:1001, results$b2, col = "blue")
legend("center", c("beta0", "beta1", "beta2"), lty = 1, col = c("green", "red", 
    "blue"))

# f) perform a linear regression and compare thr coefficients
model3 <- lm(y ~ x1 + x2)
model3$coefficients

# Results are very close
#   (Intercept)            x1            x2 
# 1003.26535343    0.02311017   -0.06236682 
# 1003.265	       0.02311017	  -0.06236682

plot(1:1001, results$b0, type = "l", xlab = "iteration", ylab = "betas", ylim = c(-2.2, 
    1.6), col = "green")
lines(1:1001, results$b1, col = "red")
lines(1:1001, results$b2, col = "blue")
legend("center", c("beta0", "beta1", "beta2"), lty = 1, col = c("green", "red", 
    "blue"))
abline(h = model3$coef[1], lty = "dashed", lwd = 3, col = rgb(0, 0, 0, alpha = 0.4))
abline(h = model3$coef[2], lty = "dashed", lwd = 3, col = rgb(0, 0, 0, alpha = 0.4))
abline(h = model3$coef[3], lty = "dashed", lwd = 3, col = rgb(0, 0, 0, alpha = 0.4))
legend("center", c("beta0", "beta1", "beta2", "multiple regression"), lty = c(1, 
    1, 1, 2), col = c("green", "red", "blue", "black"))

# g) Only 2 back fitting iteration was required 


```



