---
title: "ISLR CH8 Exercises"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---

\usepackage{amsthm}

```{r classification tree}
library(tidyverse)
library(tree)
options(warn = 1)
set.seed(1)
carseats.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Carseats.csv", 
                      header=T, stringsAsFactors = F, na.strings = "?")

carseats.df <- tibble(carseats.df)


# find NAs

carseats.df %>% 
   filter_all(all_vars(is.na(.)))

# convert all character columns to factor
char.to.fctor <- function(df)
  df %>%
    mutate_if(is.character, ~ factor(.x, levels = (.x %>% table() %>% names())))

carseats.df <- char.to.fctor(carseats.df)

# Add new column High as label for all records
carseats.df$High <- ifelse(carseats.df$Sales <= 8, "No", "Yes")

# convert High column into factor
carseats.df$High <- ordered(carseats.df$High, levels = c("Yes","No"))
str(carseats.df)

# construct a classification tree on data 
tree.carseats <- tree(High ~ .-Sales, carseats.df)

summary(tree.carseats)

plot(tree.carseats)
text(tree.carseats, pretty=0)

tree.carseats

```

Tree Deviance is defined as :$-2\sum_{m=1}^\overline{T_{0}} \sum_{k=1}^K n_{m,k} \ln \widehat{p}_{m,k}$

Residual mean devince is:
$\frac{-2\sum_{m=1}^\overline{T_{0}} \sum_{k=1}^K n_{m,k} \ln \widehat{p}_{m,k}}{N - |T_{0}|}$

Where N is Number of observations.

Note that values for each branch are as follows:

ShelveLoc: Good $(split\,criterion)$ 85 $(No\,of\,observation\,in\,the\,branch)$ 90.330 $(deviance)$ Yes $(overall\,prediction\,for\,the\,branch)$ (0.77647 0.22353 )  $(fractions\,of\,observation\, classifications\,for\,the\,branch)$
```{r Calculate test error for above classification tree}

library(tidyverse)
library(tree)
options(warn = 1)
set.seed(1)
carseats.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Carseats.csv", 
                      header=T, stringsAsFactors = F, na.strings = "?")

carseats.df <- tibble(carseats.df)


# find NAs

carseats.df %>% 
   filter_all(all_vars(is.na(.))) 

# convert all character columns to factor
char.to.fctor <- function(df)
  df %>%
    mutate_if(is.character, ~ factor(.x, levels = (.x %>% table() %>% names())))

carseats.df <- char.to.fctor(carseats.df)

# Add new column High as label for all records
carseats.df$High <- ifelse(carseats.df$Sales <= 8, "No", "Yes")

# convert High column into factor
carseats.df$High <- factor(carseats.df$High, levels = c("Yes","No"))

set.seed(1113)

no.of.train <- ceiling(0.7 * nrow(carseats.df))
no.of.test <- nrow(carseats.df) - no.of.train

train.idx <- sample(seq_len(nrow(carseats.df)), no.of.train)
test.idx <- setdiff(seq_len(nrow(carseats.df)), train.idx)

# build classification tree on train data
tree.carseats <- tree(High ~ .-Sales, carseats.df[train.idx,])

plot(tree.carseats)
text(tree.carseats, pretty = 0)
# do the predoiction on test data (type="class" makes R return class predictions):
pred.probs <- predict(tree.carseats,carseats.df[test.idx,])
tree.pred <- predict(tree.carseats, carseats.df[test.idx,], type = "class")

# see the confusion matrix
library(pROC)
(confusion_table <- table(tree.pred, carseats.df[test.idx,]$High))

nullClassifier <- max(
    (confusion_table[1,1] + confusion_table[2,1])/(confusion_table[1,1] + confusion_table[2,1]+ confusion_table[1,2] + confusion_table[2,2] ), 
    (confusion_table[1,2] + confusion_table[2,2])/(confusion_table[1,1] + confusion_table[2,1]+ confusion_table[1,2] + confusion_table[2,2] ))
  
roc_obj <- roc(unlist(carseats.df[test.idx,]$High), pred.probs[, 1])

# Let's draw some AUC plots
plot(roc_obj, legacy.axes = TRUE)


missclassificationRate <- mean(tree.pred != carseats.df[test.idx,]$High)
FP_rates <- confusion_table[2,1]/(confusion_table[2,1]+ confusion_table[1,1])
TP_rates <- confusion_table[2,2]/(confusion_table[2,2]+ confusion_table[1,2])
precisions <- confusion_table[2,2] / (confusion_table[2,2] + confusion_table[2,1])
specificities <-  1 - confusion_table[2,1]/(confusion_table[2,1]+ confusion_table[1,1]) 
classification.rate <- (confusion_table[2,2] + confusion_table[1,1]) / 
  (confusion_table[1,1]+ confusion_table[1,2] + confusion_table[2,1]+ confusion_table[2,2])
# overall fraction of wrong predictions:
# print(confusion_table)

sprintf("Null Classifier error rate : %s", 1 - nullClassifier)

# average missclassification error rate
sprintf("tree classifier : Missclassification error rate : %s", missclassificationRate)

# FP rate:
sprintf("tree classifier : FP rate (TypeI error, 1 - specificity) : %s", FP_rates)

# tree classification 
print(glue::glue("Tree classification rate: ", classification.rate))

# Null classifier rate
sprintf("Null Classifier rate: %s", nullClassifier)

# TP rate:
sprintf("tree classifier : TP rate (1-TypeII error, power, sensetivity, recall) : %s", TP_rates)

# precision:
sprintf("tree classifier : precision: %s", precisions)

# specificity 1-FP/N:
sprintf("tree classifier : specificity 1-FP/N: %s", specificities)

# -----------------------------------------------------------------------------
# ---------- Lets prune the tree to see if we get better results ---------------
cv.carseats <- cv.tree(object = tree.carseats, FUN = prune.misclass, K = 10)

# Number of terminal nodes of each tree that CV considered
cv.carseats$size

# Error rate corresponding to each tree that CV considered
cv.carseats$dev

# value of cost complexity parameter alpha that corresponds to each tree considered by CV
cv.carseats$k

# plot the missclassification error rate as the function of size and k 
plot(cv.carseats$size, cv.carseats$dev, type = 'b')
plot(cv.carseats$k, cv.carseats$dev, type = 'b')

# seems like 9 is the best size of the tree, let's prune it to get to tree with size 9
prune.carseats <- prune.misclass(tree.carseats, best = 9)

#original data
str(carseats.df)

#pruned tree
summary(prune.carseats)

plot(prune.carseats)
text(prune.carseats, pretty = 0)
# let's run it on test set to see the ,iss classification rate again:


# do the predoiction on test data (type="class" makes R return class predictions):
pred.probs <- predict(prune.carseats,carseats.df[test.idx,])
tree.pred <- predict(prune.carseats, carseats.df[test.idx,], type = "class")

# see the confusion matrix
library(pROC)
(confusion_table <- table(tree.pred, carseats.df[test.idx,]$High))

nullClassifier <- max(
    (confusion_table[1,1] + confusion_table[2,1])/(confusion_table[1,1] + confusion_table[2,1]+ confusion_table[1,2] + confusion_table[2,2] ), 
    (confusion_table[1,2] + confusion_table[2,2])/(confusion_table[1,1] + confusion_table[2,1]+ confusion_table[1,2] + confusion_table[2,2] ))
  
roc_obj <- roc(unlist(carseats.df[test.idx,]$High), pred.probs[, 1])

# Let's draw some AUC plots
plot(roc_obj, legacy.axes = TRUE)


missclassificationRate <- mean(tree.pred != carseats.df[test.idx,]$High)
FP_rates <- confusion_table[2,1]/(confusion_table[2,1]+ confusion_table[1,1])
TP_rates <- confusion_table[2,2]/(confusion_table[2,2]+ confusion_table[1,2])
precisions <- confusion_table[2,2] / (confusion_table[2,2] + confusion_table[2,1])
specificities <-  1 - confusion_table[2,1]/(confusion_table[2,1]+ confusion_table[1,1]) 
classification.rate <- (confusion_table[2,2] + confusion_table[1,1]) / 
  (confusion_table[1,1]+ confusion_table[1,2] + confusion_table[2,1]+ confusion_table[2,2])
# overall fraction of wrong predictions:
# print(confusion_table)


sprintf("Null Classifier error rate : %s", 1 - nullClassifier)

# average missclassification error rate
sprintf("tree classifier : Missclassification error rate : %s", missclassificationRate)

# FP rate:
sprintf("tree classifier : FP rate (TypeI error, 1 - specificity) : %s", FP_rates)

# tree classification 
print(glue::glue("Tree classification rate: ", classification.rate))

# Null classifier rate
sprintf("Null Classifier: %s", nullClassifier)

# TP rate:
sprintf("tree classifier : TP rate (1-TypeII error, power, sensetivity, recall) : %s", TP_rates)

# precision:
sprintf("tree classifier : precision: %s", precisions)

# specificity 1-FP/N:
sprintf("tree classifier : specificity 1-FP/N: %s", specificities)

```


Remember to each given value of $\alpha$  (k in R) we assign a subtree $T_{\alpha}$ as below:  
Using \space Gini \space index:  
$T_{\alpha} = \underset{T \subset T_{0}}{\operatorname{argMin}} \{\sum\limits_{i=1}^{|T|} \sum\limits_{k=1}^{K} \hat{p_{m,k}} \cdot (1 -  \hat{p_{m,k})} + \alpha \cdot |T| \space\space | T \subset T_{0} \}$

or \space using  \space Cross  \space Entropy:  

$T_{\alpha} = \underset{T \subset T_{0}}{\operatorname{argMin}} \{\sum\limits_{i=1}^{|T|} \sum\limits_{k=1}^{K} \hat{-p_{m,k}} \cdot \log  (\hat{p_{m,k}}) + \alpha \cdot |T| \space\space | T \subset T_{0} \}$

And then we use CV to find few good values of $\alpha$ and their corresponding $T_{\alpha}$ based on 
miss classification error rate (FUN = prune.misclass).

```{r Fit Regression tree to Boston data set}
library(tidyverse)
library(dataPreparation)
library(tree)
boston.df = read.csv(
  "/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/BostonHousing.csv", 
  header=T, stringsAsFactors = F, na.strings = "?")

boston.df <- tibble(boston.df)

# see if there is any NA in any records
na.idx <- which(is.na(boston.df))

# No NA anywhere , let's do usual clean up:

# remove constant variables
constant_cols <- whichAreConstant(boston.df)

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(boston.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(boston.df))
boston.df <- if (!is.null(bijections_cols) & length(bijections_cols) > 0 ) boston.df[- bijections_cols] else boston.df

str(boston.df)

set.seed(1113)
no.of.train <- ceiling(0.7 * nrow(boston.df))
no.of.test <- nrow(boston.df) - no.of.train

train.idx <- sample(seq_len(nrow(boston.df)), no.of.train)
test.idx <- setdiff(seq_len(nrow(boston.df)), train.idx)

tree.boston <- tree(medv ~ ., boston.df, subset = train.idx)

plot(tree.boston)
text(tree.boston, pretty = 0)

tree.boston

# let's draw the residuals pdf, very close to standard normal
plot(density(summary(tree.boston)$residuals))

# deviance is sum of squared error for the tree

# do the predoiction on test data 
pred.values <- predict(tree.boston, newdata = boston.df[test.idx,])
obs.values <- boston.df[test.idx,]$medv
# find the R^2 for test data
(R.squared <- round((cor(pred.values, obs.values) ^ 2), 3))


# now plot the observed vs predicted on test data
plot.data <- data.frame("Pred.values" = pred.values, "obs.values" = obs.values, 
                                "DataSet" = "test")
residuals <- obs.values - pred.values

plot.residuals <- data.frame("x" = obs.values, "y" = pred.values, 
                "x1" = obs.values, "y2" = pred.values + residuals,
                "DataSet" = "test")

ggplot() + 
        # plot test samples
        geom_point(data = plot.data, 
            aes(x = obs.values, y = pred.values, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.residuals, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "blue", slope = 1)

# plot the residuals
plot(density(residuals))

plot(pred.values, obs.values)
abline(0,1)

# tetMSE
mean((obs.values - pred.values)^2)

#---------------------------------------------------------
# Now let's prune the tree to see if it performs better
# --------------------------------------------------------
cv.boston <- cv.tree(tree.boston, K = 10)
plot(cv.boston$size, cv.boston$dev, type='b')

# The deviance is lowest at 7, so let's construct a pruned tree with 7 nodes
prune.boston <- prune.tree(tree = tree.boston, best=7)

# summary of pruned tree
summary(prune.boston)

plot(prune.boston)
text(prune.boston, pretty = 0)

# let's draw the residuals pdf for pruned tree
plot(density(summary(prune.boston)$residuals))

# do the predoiction on test data 
pred.values <- predict(prune.boston, newdata = boston.df[test.idx,])
obs.values <- boston.df[test.idx,]$medv
# find the R^2 for test data
(R.squared <- round((cor(pred.values, obs.values) ^ 2), 3))


# now plot the observed vs predicted on test data
plot.data <- data.frame("Pred.values" = pred.values, "obs.values" = obs.values, 
                                "DataSet" = "test")
residuals <- obs.values - pred.values

plot.residuals <- data.frame("x" = obs.values, "y" = pred.values, 
                "x1" = obs.values, "y2" = pred.values + residuals,
                "DataSet" = "test")

ggplot() + 
        # plot test samples
        geom_point(data = plot.data, 
            aes(x = obs.values, y = pred.values, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.residuals, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "blue", slope = 1)

# plot the residuals
plot(density(residuals))

plot(pred.values, obs.values)
abline(0,1)

# tetMSE
mean((obs.values - pred.values)^2)

# seemed like pruning downgraded the performance of the model 
# So pruning the tree does not add to performance

# From another perspective since testMSE ~ 25 then we can infere that 
# real house price is 5k around home value in suberb 
```

```{r bagging and random forest}
library(tidyverse)
library(dataPreparation)
library(randomForest)
boston.df = read.csv(
  "/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/BostonHousing.csv", 
  header=T, stringsAsFactors = F, na.strings = "?")

boston.df <- tibble(boston.df)

# see if there is any NA in any records
na.idx <- which(is.na(boston.df))

# No NA anywhere , let's do usual clean up:

# remove constant variables
constant_cols <- whichAreConstant(boston.df)

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(boston.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(boston.df))
boston.df <- if (!is.null(bijections_cols) & length(bijections_cols) > 0 ) boston.df[- bijections_cols] else boston.df

set.seed(1113)
no.of.train <- ceiling(0.7 * nrow(boston.df))
no.of.test <- nrow(boston.df) - no.of.train

train.idx <- sample(seq_len(nrow(boston.df)), no.of.train)
test.idx <- setdiff(seq_len(nrow(boston.df)), train.idx)

# ---------------------------- Bagging  ---------------------------

# Bagging (mtry is number of predictors should be considered for each split )
bag.boston <- randomForest(medv ~ ., data = boston.df,subset = train.idx,  
                           mtry=ncol(boston.df)-1, importance=T)

# importance of each predictor
# % IncMSE : Increase in out of bag MSE When given predictor excluded from model 
# IncNodePurity : Decrease on training RSS (regression) or deviance (classification) 
#                 in each split
importance(bag.boston)

# plot importance predictors
varImpPlot(bag.boston)

# do the predoiction on test data 
pred.values <- predict(bag.boston, newdata = boston.df[test.idx,])
obs.values <- boston.df[test.idx,]$medv
# find the R^2 for test data (Bagging)
(R.squared <- round((cor(pred.values, obs.values) ^ 2), 3))


# now plot the observed vs predicted on test data
plot.data <- data.frame("pred.values" = pred.values, "obs.values" = obs.values, 
                                "DataSet" = "test")
residuals <- obs.values - pred.values

plot.residuals <- data.frame("x" = obs.values, "y" = pred.values, 
                "x1" = obs.values, "y2" = pred.values + residuals,
                "DataSet" = "test")

ggplot() + 
        # plot test samples
        geom_point(data = plot.data, 
            aes(x = obs.values, y = pred.values, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.residuals, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "blue", slope = 1)

# plot the residuals
plot(density(residuals))

plot(pred.values, obs.values)
abline(0,1)

# tetMSE (Bagging)
mean((obs.values - pred.values)^2)

# ---------------------------- Bagging with 25 trees ---------------------------
# change number of trees in bagging or random forest using ntree parameter

# Bagging (mtry is number of predictors should be considered for each split )
bag.boston.25 <- randomForest(medv ~ ., data = boston.df,subset = train.idx,  
                           mtry=ncol(boston.df)-1, importance=T, ntree = 25)


bag.boston.25

# importance of each predictor
# % IncMSE : Increase in out of bag MSE When given predictor excluded from model 
# IncNodePurity : Decrease on training RSS (regression) or deviance (classification) 
#                 in each split

importance(bag.boston.25)

# plot importance predictors
varImpPlot(bag.boston.25)

# do the predoiction on test data 
pred.values <- predict(bag.boston.25, newdata = boston.df[test.idx,])
obs.values <- boston.df[test.idx,]$medv
# find the R^2 for test data (Bagging with 25 trees)
(R.squared <- round((cor(pred.values, obs.values) ^ 2), 3))


# now plot the observed vs predicted on test data
plot.data <- data.frame("pred.values" = pred.values, "obs.values" = obs.values, 
                                "DataSet" = "test")
residuals <- obs.values - pred.values

plot.residuals <- data.frame("x" = obs.values, "y" = pred.values, 
                "x1" = obs.values, "y2" = pred.values + residuals,
                "DataSet" = "test")

ggplot() + 
        # plot test samples
        geom_point(data = plot.data, 
            aes(x = obs.values, y = pred.values, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.residuals, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "blue", slope = 1)

# plot the residuals
plot(density(residuals))

plot(pred.values, obs.values)
abline(0,1)

# tetMSE (Bagging with 25 trees)
mean((obs.values - pred.values)^2)



# ------------------------ Random forest ---------------------------------
# By Default :
#   mtry = sqrt(Number of features) For classification
#   mtry = one third of number of features For Regression
rf.boston <- randomForest(medv ~ ., data = boston.df,subset = train.idx,  
                           mtry=6, importance=T)


rf.boston

# importance of each predictor
# % IncMSE : Increase in out of bag MSE When given predictor excluded from model 
# IncNodePurity : Decrease on training RSS (regression) or deviance (classification) 
#                 in each split
importance(rf.boston)

# plot importance predictors
# across all of the rees considered in the random forest rm and lstat are by far 
# most important features
varImpPlot(rf.boston)

# do the predoiction on test data 
pred.values <- predict(rf.boston, newdata = boston.df[test.idx,])
obs.values <- boston.df[test.idx,]$medv
# find the R^2 for test data (Random Forest with 6 predictors)
(R.squared <- round((cor(pred.values, obs.values) ^ 2), 3))


# now plot the observed vs predicted on test data
plot.data <- data.frame("pred.values" = pred.values, "obs.values" = obs.values, 
                                "DataSet" = "test")
residuals <- obs.values - pred.values

plot.residuals <- data.frame("x" = obs.values, "y" = pred.values, 
                "x1" = obs.values, "y2" = pred.values + residuals,
                "DataSet" = "test")

ggplot() + 
        # plot test samples
        geom_point(data = plot.data, 
            aes(x = obs.values, y = pred.values, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.residuals, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "blue", slope = 1)

# plot the residuals
plot(density(residuals))

plot(pred.values, obs.values)
abline(0,1)

# tetMSE (Random Forest with 6 predictors)
mean((obs.values - pred.values)^2)


```
```{r Boosting}
library(tidyverse)
library(dataPreparation)
library(gbm)
boston.df = read.csv(
  "/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/BostonHousing.csv", 
  header=T, stringsAsFactors = F, na.strings = "?")

boston.df <- tibble(boston.df)

# see if there is any NA in any records
na.idx <- which(is.na(boston.df))

# No NA anywhere , let's do usual clean up:

# remove constant variables
constant_cols <- whichAreConstant(boston.df)

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(boston.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(boston.df))
caravan.df <- if (!is.null(bijections_cols) & length(bijections_cols) > 0 ) boston.df[- bijections_cols] else boston.df

set.seed(1113)
no.of.train <- ceiling(0.7 * nrow(boston.df))
no.of.test <- nrow(boston.df) - no.of.train

train.idx <- sample(seq_len(nrow(boston.df)), no.of.train)
test.idx <- setdiff(seq_len(nrow(boston.df)), train.idx)

# 5000 trees each with depth of 4, default value for shrinkage parameter is 0.001
# lambda (Algorithm 8.2 page 322)
boost.boston <- gbm(medv ~ ., data = boston.df[train.idx, ],
                    distribution = "gaussian", n.trees = 5000, 
                    interaction.depth = 4) 

# rm and lstat are most important features
summary(boost.boston)

# partial dependence plots:
# marginal affect of selected feature on response after interating out other features
plot(boost.boston, i="rm")

# median house price is decreasing with lstat
plot(boost.boston, i="lstat")

# do the predoiction on test data 
pred.values <- predict(boost.boston, newdata = boston.df[test.idx,], n.trees = 5000)
obs.values <- boston.df[test.idx,]$medv
# find the R^2 for test data (Random Forest with 6 predictors)
(R.squared <- round((cor(pred.values, obs.values) ^ 2), 3))


# now plot the observed vs predicted on test data
plot.data <- data.frame("pred.values" = pred.values, "obs.values" = obs.values, 
                                "DataSet" = "test")
residuals <- obs.values - pred.values

plot.residuals <- data.frame("x" = obs.values, "y" = pred.values, 
                "x1" = obs.values, "y2" = pred.values + residuals,
                "DataSet" = "test")

ggplot() + 
        # plot test samples
        geom_point(data = plot.data, 
            aes(x = obs.values, y = pred.values, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.residuals, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "blue", slope = 1)

# plot the residuals
plot(density(residuals))

plot(pred.values, obs.values)
abline(0,1)

# tetMSE (Random Forest with 6 predictors)
mean((obs.values - pred.values)^2)



# ------------------------------------------------------------------ 
# boosting with specefic shrinkage parameter lambda
# ------------------------------------------------------------------

# 5000 trees each with depth of 4, default value for shrinkage parameter is 0.2
# lambda (Algorithm 8.2 page 322)
boost.boston <- gbm(medv ~ ., data = boston.df[train.idx, ],
                    distribution = "gaussian", n.trees = 5000, 
                    interaction.depth = 4, shrinkage = 0.2) 

# rm and lstat are most important features
summary(boost.boston)

# partial dependence plots:
# marginal affect of selected feature on response after interating out other features
plot(boost.boston, i="rm")

# median house price is decreasing with lstat
plot(boost.boston, i="lstat")

# do the predoiction on test data 
pred.values <- predict(boost.boston, newdata = boston.df[test.idx,], n.trees = 5000)
obs.values <- boston.df[test.idx,]$medv
# find the R^2 for test data (Random Forest with 6 predictors)
(R.squared <- round((cor(pred.values, obs.values) ^ 2), 3))


# now plot the observed vs predicted on test data
plot.data <- data.frame("pred.values" = pred.values, "obs.values" = obs.values, 
                                "DataSet" = "test")
residuals <- obs.values - pred.values

plot.residuals <- data.frame("x" = obs.values, "y" = pred.values, 
                "x1" = obs.values, "y2" = pred.values + residuals,
                "DataSet" = "test")

ggplot() + 
        # plot test samples
        geom_point(data = plot.data, 
            aes(x = obs.values, y = pred.values, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.residuals, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "blue", slope = 1)

# plot the residuals
plot(density(residuals))

plot(pred.values, obs.values)
abline(0,1)

# tetMSE (Random Forest with 6 predictors)
mean((obs.values - pred.values)^2)


```
```{r Exercise 7}
library(tidyverse)
library(dataPreparation)
library(randomForest)
library(tree)
boston.df = read.csv(
  "/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/BostonHousing.csv", 
  header=T, stringsAsFactors = F, na.strings = "?")

boston.df <- tibble(boston.df)

# see if there is any NA in any records
na.idx <- which(is.na(boston.df))

# No NA anywhere , let's do usual clean up:

# remove constant variables
constant_cols <- whichAreConstant(boston.df)

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(boston.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(boston.df))
boston.df <- if (!is.null(bijections_cols) & length(bijections_cols) > 0 ) boston.df[- bijections_cols] else boston.df

set.seed(1113)
no.of.train <- ceiling(0.7 * nrow(boston.df))
no.of.test <- nrow(boston.df) - no.of.train

train.idx <- sample(seq_len(nrow(boston.df)), no.of.train)
test.idx <- setdiff(seq_len(nrow(boston.df)), train.idx)

# ------------------------ Random forest ---------------------------------
# By Default :
#   mtry = sqrt(Number of features) For classification
#   mtry = one third of number of features For Regression
plot.data.trees <- tibble("No.Of.Trees" = NULL, "test.MSE" = NULL, "R^2" = NULL)

for (no.of.trees in 10:100){
  rf.boston <- randomForest(medv ~ ., data = boston.df,subset = train.idx,  
                            n.trees = no.of.trees, importance=F)

  pred.values <- predict(rf.boston, newdata = boston.df[test.idx,])
  obs.values <- boston.df[test.idx,]$medv
  R.squared <- round((cor(pred.values, obs.values) ^ 2), 3)
  test.MSE <- mean((obs.values - pred.values)^2)

  plot.data.trees <- rbind(plot.data.trees, 
                           tibble("No.Of.Trees" = no.of.trees, 
                                  "test.MSE" = test.MSE, 
                                  "R^2" = R.squared))
}


ggplot() + 
        geom_line(data = plot.data.trees, 
            aes(x = No.Of.Trees, y = test.MSE)) + 
  labs(title="No of trees and test MSE")

# Number of trees that makes test.MSE minimim in range of 100 trees
plot.data.trees[which.min(plot.data.trees$test.MSE), ]

# --------------------------- mtry ------------------------------- #
plot.data.mtry <- tibble("mtry" = NULL, "test.MSE" = NULL, "R^2" = NULL)

for (m.try in 2:ncol(boston.df) - 1){
  rf.boston <- randomForest(medv ~ ., data = boston.df,subset = train.idx,  
                            mtry = m.try, importance=F)

  pred.values <- predict(rf.boston, newdata = boston.df[test.idx,])
  obs.values <- boston.df[test.idx,]$medv
  R.squared <- round((cor(pred.values, obs.values) ^ 2), 3)
  test.MSE <- mean((obs.values - pred.values)^2)

  plot.data.mtry <- rbind(plot.data.mtry, 
                           tibble("mtry" = m.try, 
                                  "test.MSE" = test.MSE, 
                                  "R^2" = R.squared))

}

ggplot() + 
        geom_line(data = plot.data.mtry, 
            aes(x = mtry, y = test.MSE)) + 
  labs(title="mtry and test MSE")

# value of mtry  that makes test.MSE minimim in range of 100 trees
plot.data.mtry[which.min(plot.data.mtry$test.MSE), ]

# over all the best result obtained for number of predictors = 4 and No of trees = 18:
rf.boston <- randomForest(medv ~ ., data = boston.df,subset = train.idx,  
                          mtry = 4, n.trees = 18, importance=F)

pred.values <- predict(rf.boston, newdata = boston.df[test.idx,])
obs.values <- boston.df[test.idx,]$medv
# R sruared:
(R.squared <- round((cor(pred.values, obs.values) ^ 2), 3))
# testMSE:
(test.MSE <- mean((obs.values - pred.values)^2))

# now plot the observed vs predicted on test data
plot.data <- data.frame("pred.values" = pred.values, "obs.values" = obs.values, 
                                "DataSet" = "test")
residuals <- obs.values - pred.values

plot.residuals <- data.frame("x" = obs.values, "y" = pred.values, 
                "x1" = obs.values, "y2" = pred.values + residuals,
                "DataSet" = "test")

ggplot() + 
        # plot test samples
        geom_point(data = plot.data, 
            aes(x = obs.values, y = pred.values, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.residuals, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "blue", slope = 1)

```
```{r r Exercise 8}
library(tidyverse)
library(tree)
library(randomForest)
library(dataPreparation)
options(warn = 1)
set.seed(1)
carseats.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Carseats.csv", 
                      header=T, stringsAsFactors = F, na.strings = "?")

carseats.df <- tibble(carseats.df)


# find NAs

carseats.df %>% 
   filter_all(all_vars(is.na(.))) 

# convert all character columns to factor
char.to.fctor <- function(df)
  df %>%
    mutate_if(is.character, ~ factor(.x, levels = (.x %>% table() %>% names())))

carseats.df <- char.to.fctor(carseats.df)

# remove constant variables
constant_cols <- whichAreConstant(carseats.df)

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(carseats.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(carseats.df))
carseats.df <- if (!is.null(bijections_cols) & length(bijections_cols) > 0 ) carseats.df[- bijections_cols] else carseats.df

#----------------------------------------
# a) split the data into train and test
set.seed(1113)
no.of.train <- ceiling(0.7 * nrow(carseats.df))
no.of.test <- nrow(carseats.df) - no.of.train

train.idx <- sample(seq_len(nrow(carseats.df)), no.of.train)
test.idx <- setdiff(seq_len(nrow(carseats.df)), train.idx)

# b) Fit a regression tree , plot the three and interpret the results, testMSE
tree.carseats <- tree(Sales ~ ., carseats.df, subset = train.idx)

plot(tree.carseats)
text(tree.carseats, pretty = 0)

# summary
summary(tree.carseats)

# the tree
tree.carseats

# let's draw the residuals pdf, very close to standard normal
plot(density(summary(tree.carseats)$residuals))

# deviance is sum of squared error for the tree

# do the predoiction on test data 
pred.values <- predict(tree.carseats, newdata = carseats.df[test.idx,])
obs.values <- carseats.df[test.idx,]$Sales

# R sruared:
(R.squared <- round((cor(pred.values, obs.values) ^ 2), 3))
# testMSE:
(test.MSE <- mean((obs.values - pred.values)^2))


# now plot the observed vs predicted on test data
plot.data <- data.frame("Pred.values" = pred.values, "obs.values" = obs.values, 
                                "DataSet" = "test")
residuals <- obs.values - pred.values

plot.residuals <- data.frame("x" = obs.values, "y" = pred.values, 
                "x1" = obs.values, "y2" = pred.values + residuals,
                "DataSet" = "test")

ggplot() + 
        # plot test samples
        geom_point(data = plot.data, 
            aes(x = obs.values, y = pred.values, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.residuals, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "blue", slope = 1)

# ----------------------------------
# c) Let's use CV to prune th tree
cv.carseats <- cv.tree(object = tree.carseats, FUN = prune.tree, K = 10)

# Number of terminal nodes of each tree that CV considered
cv.carseats$size

# error rate corresponding to each tree that CV considered
cv.carseats$dev

# value of cost complexity parameer alpha that corresponds to each tree considered by CV
cv.carseats$k

# creata a tibble to have these values in one place
(cv.result <- tibble(error.rate = cv.carseats$dev, tree.size = cv.carseats$size, 
                    aplpha = cv.carseats$k)
)

cv.result[which.min(cv.result$error.rate), ]
# plot error rate against size and k 
plot(cv.carseats$size, cv.carseats$dev, type = 'b')
plot(cv.carseats$k, cv.carseats$dev, type = 'b')

# seems like tree with 14 terminals results in minimum cv error rate
prune.carseats <- prune.tree(tree.carseats, best = 14)

#original data
str(carseats.df)

#pruned tree
summary(prune.carseats)

plot(prune.carseats)
text(prune.carseats, pretty = 0)

# do the predoiction on test data 
pred.values <- predict(prune.carseats, newdata = carseats.df[test.idx,])
obs.values <- carseats.df[test.idx,]$Sales

# R sruared (pruned tree):
(R.squared <- round((cor(pred.values, obs.values) ^ 2), 3))
# testMSE (pruned tree) :
(test.MSE <- mean((obs.values - pred.values)^2))


# now plot the observed vs predicted on test data
plot.data <- data.frame("Pred.values" = pred.values, "obs.values" = obs.values, 
                                "DataSet" = "test")
residuals <- obs.values - pred.values

plot.residuals <- data.frame("x" = obs.values, "y" = pred.values, 
                "x1" = obs.values, "y2" = pred.values + residuals,
                "DataSet" = "test")

ggplot() + 
        # plot test samples
        geom_point(data = plot.data, 
            aes(x = obs.values, y = pred.values, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.residuals, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "blue", slope = 1)

# pruning shows slight improvement on R squared and Test.MSE

# ---------------------------------------------
# d) Use bagging approach


bag.carseats <- randomForest(Sales ~ ., data = carseats.df,subset = train.idx,  
                           mtry=ncol(carseats.df)-1, importance=T)

# importance of each predictor
# % IncMSE : Increase in out of bag MSE When given predictor excluded from model 
# IncNodePurity : Decrease on training RSS (regression) or deviance (classification) 
#                 in each split
importance(bag.carseats)

# plot importance predictors
varImpPlot(bag.carseats)

# do the predoiction on test data 
pred.values <- predict(bag.carseats, newdata = carseats.df[test.idx,])
obs.values <- carseats.df[test.idx,]$Sales

# R sruared (pruned tree):
(R.squared <- round((cor(pred.values, obs.values) ^ 2), 3))
# testMSE (pruned tree) :
(test.MSE <- mean((obs.values - pred.values)^2))

# now plot the observed vs predicted on test data
plot.data <- data.frame("pred.values" = pred.values, "obs.values" = obs.values, 
                                "DataSet" = "test")
residuals <- obs.values - pred.values

plot.residuals <- data.frame("x" = obs.values, "y" = pred.values, 
                "x1" = obs.values, "y2" = pred.values + residuals,
                "DataSet" = "test")

ggplot() + 
        # plot test samples
        geom_point(data = plot.data, 
            aes(x = obs.values, y = pred.values, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.residuals, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "blue", slope = 1)

# plot the residuals
plot(density(residuals))

# ---------------------------------------------
# e) Use Random Forest


# ------------------------ Random forest ---------------------------------
# By Default :
#   mtry = sqrt(Number of features) For classification
#   mtry = one third of number of features For Regression

# first let's find optimal number of trees 
plot.data.trees <- tibble("No.Of.Trees" = NULL, "test.MSE" = NULL, "R^2" = NULL)

for (no.of.trees in 10:100){
  rf.carseats <- randomForest(Sales ~ ., data = carseats.df,subset = train.idx,  
                            n.trees = no.of.trees, importance=F)

  pred.values <- predict(rf.carseats, newdata = carseats.df[test.idx,])
  obs.values <- carseats.df[test.idx,]$Sales
  R.squared <- round((cor(pred.values, obs.values) ^ 2), 3)
  test.MSE <- mean((obs.values - pred.values)^2)

  plot.data.trees <- rbind(plot.data.trees, 
                           tibble("No.Of.Trees" = no.of.trees, 
                                  "test.MSE" = test.MSE, 
                                  "R^2" = R.squared))
}


ggplot() + 
        geom_line(data = plot.data.trees, 
            aes(x = No.Of.Trees, y = test.MSE)) + 
  labs(title="No of trees and test MSE")

# Number of trees that makes test.MSE minimim in range of 100 trees
plot.data.trees[which.min(plot.data.trees$test.MSE), ]


# Let's find number of predictors to build the tree 
# --------------------------- mtry ------------------------------- #
plot.data.mtry <- tibble("mtry" = NULL, "test.MSE" = NULL, "R^2" = NULL)

for (m.try in 2:ncol(carseats.df) - 1){
  rf.carseats <- randomForest(Sales ~ ., data = carseats.df,subset = train.idx,  
                            mtry = m.try, importance=F)

  pred.values <- predict(rf.carseats, newdata = carseats.df[test.idx,])
  obs.values <- carseats.df[test.idx,]$Sales
  R.squared <- round((cor(pred.values, obs.values) ^ 2), 3)
  test.MSE <- mean((obs.values - pred.values)^2)

  plot.data.mtry <- rbind(plot.data.mtry, 
                           tibble("mtry" = m.try, 
                                  "test.MSE" = test.MSE, 
                                  "R^2" = R.squared))

}

ggplot() + 
        geom_line(data = plot.data.mtry, 
            aes(x = mtry, y = test.MSE)) + 
  labs(title="mtry and test MSE")

# value of mtry  that makes test.MSE minimim in range of 100 trees
plot.data.mtry[which.min(plot.data.mtry$test.MSE), ]


# over all the best result obtained for number of predictors = 8 and No of trees = 33:
rf.carseats <- randomForest(Sales ~ ., data = carseats.df,subset = train.idx,  
                          mtry = 8, n.trees = 33, importance=F)

pred.values <- predict(rf.carseats, newdata = carseats.df[test.idx,])
obs.values <- carseats.df[test.idx,]$Sales
# R sruared:
(R.squared <- round((cor(pred.values, obs.values) ^ 2), 3))
# testMSE:
(test.MSE <- mean((obs.values - pred.values)^2))

# now plot the observed vs predicted on test data
plot.data <- data.frame("pred.values" = pred.values, "obs.values" = obs.values, 
                                "DataSet" = "test")
residuals <- obs.values - pred.values

plot.residuals <- data.frame("x" = obs.values, "y" = pred.values, 
                "x1" = obs.values, "y2" = pred.values + residuals,
                "DataSet" = "test")

ggplot() + 
        # plot test samples
        geom_point(data = plot.data, 
            aes(x = obs.values, y = pred.values, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.residuals, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "blue", slope = 1)


# importance: how much training RSS is decreaded (node purity increased) on each split  
importance(rf.carseats)
varImpPlot(rf.carseats)


```
```{r Exercise 9}

library(tidyverse)
library(tree)
library(randomForest)
library(dataPreparation)
options(warn = 1)
oj.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/orange_juice_withmissing.csv", 
                      header=T, stringsAsFactors = F, na.strings = "?")

oj.df <- tibble(oj.df)

#----------------------------------------
# a) split the data into train and test
# lets first split data to 70% train and the rest test 
set.seed(1113)
train.idx <- sample(1:nrow(oj.df), 0.7*nrow(oj.df))
test.idx <- setdiff(1:nrow(oj.df), train.idx)

train.df <- oj.df[train.idx, ]
test.df <- oj.df[test.idx, ]

# remove empty characters and NA helper
remove.NA.chars <- function(df)
  df %>%
  select_all %>%
  filter_if(is.character, all_vars(trimws(.) != "" & trimws(.) != "NA"))
  
# Clean up train data set
train.df <- 
  train.df %>%
  na.omit() %>%
  remove.NA.chars()

# remove constant variables
(constant_cols <- whichAreConstant(train.df))

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(train.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(train.df))
train.df <- if (!is.null(bijections_cols) & length(bijections_cols) > 0 ) train.df[- bijections_cols] else train.df

# Convert characters to numeric
train.df$PriceCH <- as.numeric(as.character(train.df$PriceCH))
train.df$PriceMM <- as.numeric(as.character(train.df$PriceMM))
train.df$DiscCH <- as.numeric(as.character(train.df$DiscCH))
train.df$DiscMM <- as.numeric(as.character(train.df$DiscMM))
train.df$SpecialCH <- as.numeric(as.character(train.df$SpecialCH))
train.df$SpecialMM <- as.numeric(as.character(train.df$SpecialMM))
train.df$LoyalCH <- as.numeric(as.character(train.df$LoyalCH))
train.df$SalePriceCH <- as.numeric(as.character(train.df$SalePriceCH))
train.df$SalePriceMM <- as.numeric(as.character(train.df$SalePriceMM))
train.df$PriceDiff <- as.numeric(as.character(train.df$PriceDiff))
train.df$PctDiscMM <- as.numeric(as.character(train.df$PctDiscMM))
train.df$PctDiscCH <- as.numeric(as.character(train.df$PctDiscCH))
train.df$StoreID <- as.numeric(as.character(train.df$StoreID))
train.df$STORE <- as.numeric(as.character(train.df$STORE))

# remove constant variables
constant_cols <- whichAreConstant(train.df)

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(train.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(train.df))
train.df <- if (!is.null(bijections_cols) & length(bijections_cols) > 0 ) train.df[- bijections_cols] else train.df

# It looks Purchase has only two values "CH" and "MM"
table(train.df$Purchase)

# convert all character columns to factor
char.to.fctor <- function(df)
  df %>%
    mutate_if(is.character, ~ factor(.x, levels = (.x %>% table() %>% names())))


train.df <- char.to.fctor(train.df)
# Purchase is encoded by factor as CH -> 1, MM -> 2

head(train.df$Purchase)
as.integer(head(train.df$Purchase))

# --------- do similar clean up for test data ----------------

# remove empty characters and NA helper
remove.NA.chars <- function(df)
  df %>%
  select_all %>%
  filter_if(is.character, all_vars(trimws(.) != "" & trimws(.) != "NA"))
  
# Finally convert character columns to factor

# Clean up train data set
test.df <- 
  test.df %>%
  na.omit() %>%
  remove.NA.chars()


# remove constant variables
(constant_cols <- whichAreConstant(test.df))

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(test.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(test.df))
test.df <- if (!is.null(bijections_cols) & length(bijections_cols) > 0 ) test.df[- bijections_cols] else test.df

# Convert characters to numeric
test.df$PriceCH <- as.numeric(as.character(test.df$PriceCH))
test.df$PriceMM <- as.numeric(as.character(test.df$PriceMM))
test.df$DiscCH <- as.numeric(as.character(test.df$DiscCH))
test.df$DiscMM <- as.numeric(as.character(test.df$DiscMM))
test.df$SpecialCH <- as.numeric(as.character(test.df$SpecialCH))
test.df$SpecialMM <- as.numeric(as.character(test.df$SpecialMM))
test.df$LoyalCH <- as.numeric(as.character(test.df$LoyalCH))
test.df$SalePriceCH <- as.numeric(as.character(test.df$SalePriceCH))
test.df$SalePriceMM <- as.numeric(as.character(test.df$SalePriceMM))
test.df$PriceDiff <- as.numeric(as.character(test.df$PriceDiff))
test.df$PctDiscMM <- as.numeric(as.character(test.df$PctDiscMM))
test.df$PctDiscCH <- as.numeric(as.character(test.df$PctDiscCH))
test.df$StoreID <- as.numeric(as.character(test.df$StoreID))
test.df$STORE <- as.numeric(as.character(test.df$STORE))

# remove constant variables
constant_cols <- whichAreConstant(test.df)

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(test.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(test.df))
test.df <- if (!is.null(bijections_cols) & length(bijections_cols) > 0 ) test.df[- bijections_cols] else test.df

# convert all character columns to factor
char.to.fctor <- function(df)
  df %>%
    mutate_if(is.character, ~ factor(.x, levels = (.x %>% table() %>% names())))

test.df <- char.to.fctor(test.df)
# Purchase is encoded by factor as CH -> 1, MM -> 2

head(test.df$Purchase)
as.integer(head(test.df$Purchase))

# --------------------------------------------------------------------------------
# b) Fit a classification tree , plot the three and interpret the results, testMSE
# Note since "Purchase" is a factor, tree() API construct a classification tree
tree.oj <- tree(Purchase ~ ., train.df)

# Number of terminal nodes:  7 
# Residual mean deviance:  0.7995
# Misclassification error rate: 0.177
summary(tree.oj)

# ----------------------------------
# c) the tree
tree.oj
# feature split criterion: LoyalCH < 0.0356415 
# Number of training data observations in the branch: 44   
# deviance : 0.00 
# prediction for the branch : MM 
# Fractions of observations classification for thr branch ( 0.00000 1.00000 )

#d) plot the tree
# looks the decision is mostly based on differen ranges on LoyalCH and PriceDiff
# not much other predictors matter in decision making
plot(tree.oj)
text(tree.oj, pretty = 0)

# --------------------------
# e) predoiction on test data 
# --------------------------


# pred.probs generates below data:
#            CH         MM
# 1   0.5750000 0.42500000
# 2   0.2388060 0.76119403
# 3   0.9588015 0.04119850

pred.probs <- predict(tree.oj, newdata = test.df)

pred.values <- ifelse(pred.probs[, 1] > pred.probs[, 2], "CH", "MM")

obs.values <- test.df$Purchase

(confusion_table <- table(pred.values, obs.values))

# TN: observes as 0 and predicted as 0 
true.negative <- confusion_table[1,1]

# TP: observes as 1 and predicted as 1 
true.positive <- confusion_table[2,2]

# FP: observes as 0 and predicted as 1 
false.positive <- confusion_table[2,1]

# FN: observed as 1 and predicted as 0 
false.negative <- confusion_table[1,2]

observations.total <- true.negative + false.negative + true.positive + false.positive
observed.as.0 <- true.negative + false.positive
observed.as.1 <- false.negative + true.positive

# Random guessing Rate
(nullClassifier <- max(observed.as.0 / observations.total, observed.as.1 / observations.total))

# classification Rate:
(classification.rate <- mean(pred.values == obs.values))

# Test error Rate:
(missclassificationRate <- 1 - classification.rate)

# False Positive Rate (or: Type I error , 1 - specificty):
(FP_rates <- false.positive / observed.as.0)

# True Positive Rate (or: 1 - Type II error , power , sesetivity , recall):
(TP_rates <- true.positive / observed.as.1)

# Precision (Accuracy Rate, 1 - false discovery proportion):
# Model predicted that Two people make a purchase but in reality only one did (50%)
(precisions <- true.positive / (true.positive + false.positive))

# Specificity
(specificities <-  1 - false.positive/(false.positive + true.negative))

library(pROC)
pred.probs.roc <- ifelse(pred.probs[, 1] > pred.probs[, 2], pred.probs[, 1], pred.probs[, 2])
roc_obj <- roc(unlist(obs.values), pred.probs.roc)

# Let's draw some AUC plots
plot(roc_obj, legacy.axes = TRUE)

# ------------------------------------
# f) apply cv.tree to prune the tree
# ------------------------------------
cv.oj <- cv.tree(object = tree.oj, FUN = prune.misclass, K = 10)

# Number of terminal nodes of each tree that CV considered
cv.oj$size

# Error rate corresponding to each tree that CV considered
cv.oj$dev

# value of cost complexity parameter alpha that corresponds to each tree considered by CV
cv.oj$k

# -------------------------------------------------
# g) plot tree size vs CV classification error rate
# --------------------------------------------------
# plot the missclassification error rate as the function of size and k 
plot(cv.oj$size, cv.oj$dev, type = 'b')
plot(cv.oj$k, cv.oj$dev, type = 'b')

# ---------------------------------------------------------------------------------------
# h) seems like 3 is the best size of the tree,
# ---------------------------------------------------------------------------------------

#--------------------------------------
# i)  let's prune it to get to tree with size 3
# ---------------------------------------------
prune.oj <- prune.misclass(tree.oj, best = 3)



plot(prune.oj)
text(prune.oj, pretty = 0)
# let's run it on test set to see the ,iss classification rate again:

#---------------------------------------------------------------------
# j) compare training error rates between prune and unpruned tree
# --------------------------------------------------------------------

#pruned tree
summary(prune.oj)



#---------------------------------------------------------------------
# j) compare test error rates between prune and unpruned tree
# --------------------------------------------------------------------

# pred.probs generates below data:
#            CH         MM
# 1   0.5750000 0.42500000
# 2   0.2388060 0.76119403
# 3   0.9588015 0.04119850

pred.probs <- predict(prune.oj, newdata = test.df)

pred.values <- ifelse(pred.probs[, 1] > pred.probs[, 2], "CH", "MM")

obs.values <- test.df$Purchase

pred.probs <- predict(tree.oj, newdata = test.df)

(confusion_table <- table(pred.values, obs.values))

# TN: observes as 0 and predicted as 0 
true.negative <- confusion_table[1,1]

# TP: observes as 1 and predicted as 1 
true.positive <- confusion_table[2,2]

# FP: observes as 0 and predicted as 1 
false.positive <- confusion_table[2,1]

# FN: observed as 1 and predicted as 0 
false.negative <- confusion_table[1,2]

observations.total <- true.negative + false.negative + true.positive + false.positive
observed.as.0 <- true.negative + false.positive
observed.as.1 <- false.negative + true.positive

# Random guessing Rate
(nullClassifier <- max(observed.as.0 / observations.total, observed.as.1 / observations.total))

# classification Rate:
(classification.rate <- mean(pred.values == obs.values))

# Test error Rate:
(missclassificationRate <- 1 - classification.rate)

# False Positive Rate (or: Type I error , 1 - specificty):
(FP_rates <- false.positive / observed.as.0)

# True Positive Rate (or: 1 - Type II error , power , sesetivity , recall):
(TP_rates <- true.positive / observed.as.1)

# Precision (Accuracy Rate, 1 - false discovery proportion):
# Model predicted that Two people make a purchase but in reality only one did (50%)
(precisions <- true.positive / (true.positive + false.positive))

# Specificity
(specificities <-  1 - false.positive/(false.positive + true.negative))

library(pROC)
pred.probs.roc <- ifelse(pred.probs[, 1] > pred.probs[, 2], pred.probs[, 1], pred.probs[, 2])

roc_obj <- roc(unlist(obs.values), pred.probs.roc)

# Let's draw some AUC plots
plot(roc_obj, legacy.axes = TRUE)
#  pruned tree classifier behaves worse than the full tree:

```




```{r Exercise 10}

library(tidyverse)
library(gbm)
library(glmnet) # for Lasso
library(randomForest)
library(dataPreparation)
options(warn = 1)

set.seed(1311)

hitters.df = read.csv(
  "/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Hitters.csv", 
  header=T, stringsAsFactors = F, na.strings = "?")

# -------------------------------
# 

# note that Salary is of type string and some of them are NA
sum(hitters.df$Salary=="NA")

hitters.df <- tibble(hitters.df)

# b) first devide the data to 70% train and 30% test
n <- nrow(hitters.df)
no.of.train <- ceiling(0.7*n)
no.of.test <- n - no.of.train

train.idx <- sample(seq_len(n), no.of.train)
test.idx <- setdiff(seq_len(n), train.idx)

train.df <- hitters.df[train.idx, ]
test.df <- hitters.df[test.idx, ]

# a) Cleaning and log transform

#So let's clean train data set

na.train.idx <- 
  train.df %>%
  pmap(~c(...)) %>%
  map(~sum(is.na(.) | str_trim(.) == "NA") > 0) %>%
  unlist %>%
  which()

rest.train.idx <- setdiff(seq_len(no.of.train) , na.train.idx)

train.df <- train.df[rest.train.idx, ]

# remove constant variables
constant_cols <- whichAreConstant(train.df)

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(train.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(train.df))
train.df <- if (!is.null(bijections_cols) & length(bijections_cols) > 0 ) train.df[- bijections_cols] else train.df

train.df$Salary <- log(as.numeric(as.character(train.df$Salary)))

# convert all character columns to factor
char.to.fctor <- function(df)
  df %>%
    mutate_if(is.character, ~ factor(.x, levels = (.x %>% table() %>% names())))

train.df <- char.to.fctor(train.df)

# Now clean test data

na.test.idx <- 
  test.df %>%
  pmap(~c(...)) %>%
  map(~sum(is.na(.) | str_trim(.) == "NA") > 0) %>%
  unlist %>%
  which()

rest.test.idx <- setdiff(seq_len(no.of.test) , na.test.idx)

test.df <- test.df[rest.test.idx, ]

# remove constant variables
constant_cols <- whichAreConstant(test.df)

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(test.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(test.df))
test.df <- if (!is.null(bijections_cols) & length(bijections_cols) > 0 ) test.df[- bijections_cols] else test.df


test.df$Salary <- log(as.numeric(as.character(test.df$Salary)))

# convert all character columns to factor
char.to.fctor <- function(df)
  df %>%
    mutate_if(is.character, ~ factor(.x, levels = (.x %>% table() %>% names())))

test.df <- char.to.fctor(test.df)


# c) perform Regression boosting on training sets with 1000 trees for range of shrinkage parameters

shrinkage.MSE.train.plot <- tibble(lambdas = NULL, train.MSEs = NULL, DataSet=NULL)
shrinkage.MSE.test.plot <- tibble(lambdas = NULL, test.MSEs = NULL, r.squared = NULL, DataSet=NULL)

for (lambda in seq(0.1, 1, len = 100)){
  hitters.boost <- gbm(Salary ~ ., data = train.df, distribution="gaussian", 
                     n.trees = 1000, shrinkage=lambda, verbose=F)
 
  # claculate training MSE
  yhat.boost.train <- predict(hitters.boost, newdata = train.df, n.trees = 1000)
  stopifnot(identical (length(yhat.boost.train) , length(train.df$Salary)) )

  
  shrinkage.MSE.train.plot <- rbind(shrinkage.MSE.train.plot , 
                   tibble(lambdas = lambda,
                          train.MSEs = mean((yhat.boost.train - train.df$Salary)^2),
                          DataSet = "train"))
  
  
  
  # claculate test MSE
  yhat.boost.test <- predict(hitters.boost, newdata = test.df, n.trees = 1000)
  stopifnot(identical (length(yhat.boost.test) , length(test.df$Salary)) )


  shrinkage.MSE.test.plot <- rbind(shrinkage.MSE.test.plot , 
                   tibble(lambdas = lambda,
                          test.MSEs = mean((yhat.boost.test - test.df$Salary)^2),
                          r.squared = round((cor(yhat.boost.test, test.df$Salary) ^ 2), 3),
                          DataSet = "test"))
  

}


ggplot() + 
        # plot test samples
        geom_line(data = shrinkage.MSE.train.plot,
            aes(x = lambdas, y = train.MSEs, color = DataSet)) +
  labs(title="Shinkage factor vs. train MSE")

ggplot() + 
        # plot test samples
        geom_line(data = shrinkage.MSE.test.plot,
            aes(x = lambdas, y = test.MSEs, color = DataSet)) +
  labs(title="Shinkage factor vs. test MSE")



# e) Compare test.mse of boosing with test.mse of linear model and lasso 

# find which lambda value causes minimum test.mse
shrinkage.MSE.test.plot %>% slice(which.min(test.MSEs))

# build a linear model on training data and find the test mse
hitters.lm = lm(Salary ~ ., data = train.df)
preds <- predict(hitters.lm,  test.df, interval = "prediction")
yhat.lm.test <- preds[,"fit"]
stopifnot(identical (length(yhat.lm.test), length(test.df$Salary)) )

print ("Apply Linear regression:")

# LM test MSE
(test.MSE = mean((yhat.lm.test - test.df$Salary)^2))

# LM R squared
(r.squared = round((cor(yhat.lm.test, test.df$Salary) ^ 2), 3))

# Clearly performance of linear model cannot beat boosing model


print ("Apply Lasso  cross validation to find the best lambda and corresponding coeffs")

# First construct matrix from dataframe (and drop intercept column)

x.train <- model.matrix(Salary~., train.df)[, -1] # drop the intercept
x.test <- model.matrix(Salary~., test.df)[, -1] # drop the intercept
y.train <- train.df$Salary
y.test <- test.df$Salary

cv.out <- cv.glmnet(x.train, y.train, alpha=1) # Lasso
plot(cv.out)
best.lambda <- cv.out$lambda.min

# predict the model on test
pred.lasso <- predict(cv.out, s=best.lambda, newx=x.test)

# Lasso test mse for the best lambda we chose:
(lasso.mse <- mean((pred.lasso - y.test)^2))

# R squared for the best lambda we chose:
(lasso.R.squared <- round((cor(pred.lasso, y.test) ^ 2), 3))

# f) Most important predictors in boostnig model:

summary (hitters.boost)

# it to what predictors lasso chose:
# Coeffs for Lasso:
(coefs.lasso <- predict (cv.out, type = "coefficients", s = best.lambda))

# It is noticable that "CHits" predictor which is the most important 
# predictor in boosting model is dropped by Lasso

# g) apply bagging to training set and calculate testMSE and R squared
hitters.bag = randomForest(Salary ~ ., data = train.df, mtry=ncol(train.df) - 1, importance=T)
hitters.bag

yhat.bag <- predict(hitters.bag, newdata = test.df)

# bagging test mse:
(bag.mse <- mean((yhat.bag - test.df$Salary)^2))

# R squared for testMSE:
(bag.R.squared <- round((cor(yhat.bag, test.df$Salary) ^ 2), 3))

# surprisingly bagging produced better result than gradiant boosting

```
```{r Exercise 11}
library(tidyverse)
library(gbm)
library(glmnet) # for Lasso
library(randomForest)
library(dataPreparation)
library(class)
options(warn = 1)
set.seed(1113)
caravan.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/Caravan.csv", header=T, 
                      stringsAsFactors = F, na.strings = "?")

caravan.df = as_tibble(caravan.df)


# remove constant variables
constant_cols <- whichAreConstant(caravan.df)

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(caravan.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(caravan.df))

caravan.df <- if (!is.null(bijections_cols) & length(bijections_cols) > 0 ) caravan.df[- bijections_cols] else caravan.df

nrow(caravan.df)
caravan.df %>% 
   filter_all(all_vars(is.na(.))) 

# No NA is in data set 

# Since boosting API with 'benoulli' distribution needs 0 and 1 values
# we cannot use facotr, instead we should manually encode them
caravan.df$Purchase <- ifelse(caravan.df$Purchase == "Yes", 1, 0)
str(caravan.df)
# a) let's creata 1000 observations in training set and the rest in test set:

(n <- nrow(caravan.df))
train.idx <- sample(seq_len(n), 1000)
test.idx <- setdiff(seq_len(n), train.idx)

stopifnot(identical(length(train.idx) + length(test.idx), n))

train.df <- caravan.df[train.idx, ]
test.df <- caravan.df[test.idx, ]



# b) Fit a boosting model and find the most important variable
caravan.boost <- gbm(Purchase ~ . , data = train.df, distribution="bernoulli", 
                   n.trees = 1000, shrinkage=0.01, verbose=F)

# MSKC is the most important predictor
summary(caravan.boost)

# c) predict that a person will make a purchase if estimated probability of purchase > 0.2
prob.boost.test <- predict(caravan.boost, newdata = test.df, n.trees = 1000)

# Since we encoded Yes as 1 and No as 0 we have
yhat.boost.test <- ifelse(prob.boost.test > 0.2, 1, 0)

stopifnot(identical(length(prob.boost.test),length(test.df$Purchase)))

#Confusion Matrix:
(confusion_table <- table(yhat.boost.test, test.df$Purchase))

# TN: observes as 0 and predicted as 0 (No of people that predicted Not to make any purchase and they really did not)
true.negative <- confusion_table[1,1]

# TP: observes as 1 and predicted as 1 (No of people that predicted to make a purchase and in reality they did)
true.positive <- confusion_table[2,2]

# FP: observes as 0 and predicted as 1 (No of people that predicted to make a purchase but they did not)
false.positive <- confusion_table[2,1]

# FN: observed as 1 and predicted as 0 (No of people that predicted they did not make a purchase but they in realty did)
false.negative <- confusion_table[1,2]

observations.total <- true.negative + false.negative + true.positive + false.positive
observed.as.0 <- true.negative + false.positive
observed.as.1 <- false.negative + true.positive

# Random guessing Rate
(nullClassifier <- max(observed.as.0 / observations.total, observed.as.1 / observations.total))

# classification Rate:
(classification.rate <- mean(yhat.boost.test == test.df$Purchase))

# Test error Rate:
(missclassificationRate <- 1 - classification.rate)

# False Positive Rate (or: Type I error , 1 - specificty):
(FP_rates <- false.positive / observed.as.0)

# True Positive Rate (or: 1 - Type II error , power , sesetivity , recall):
(TP_rates <- true.positive / observed.as.1)

# Precision (Accuracy Rate, 1 - false discovery proportion):
# Model predicted that Two people make a purchase but in reality only one did (50%)
(precisions <- true.positive / (true.positive + false.positive))

# Specificity
(specificities <-  1 - false.positive/(false.positive + true.negative))

library(pROC)
roc_obj <- roc(unlist(test.df$Purchase), prob.boost.test)

# Let's draw some AUC plots
plot(roc_obj, legacy.axes = TRUE)

# -------------------------------------------------------------
# Let's use logiostic regression to estimate what percentage of 
# people who predicted to make a purchase and they really did
glm.model <- glm(Purchase ~ . , data = train.df, family = binomial)

summary(glm.model)
# do the prediction using the model
preds <- predict(glm.model, newdata=test.df, se = T)

# Note that predict function for glm model gets the prediction for logit i.e: 
# log(Pr(Y=1 | X) / (1 - Pr(Y=1 | X))) = X*beta and also the standard error is 
# of the same form, clealry to get prediction for Pr(Y=1|X) we need 
# to calculate exp(X*beta)/(1+exp(X*beta))

prob.predict <- exp(preds$fit)/(1+exp(preds$fit))
se.bound.logit <- cbind(preds$fit + 2*preds$se, preds$fit - 2*preds$se)
se.bound <- apply(se.bound.logit, 2, function(x) exp(x)/(1+exp(x)))

yhat.logReg.test <- ifelse(prob.predict > 0.2, 1, 0)

stopifnot(identical(length(prob.predict),length(test.df$Purchase)))

#Confusion Matrix:
(confusion_table <- table(yhat.logReg.test, test.df$Purchase))

# TN: observes as 0 and predicted as 0 (No of people that predicted Not to make any purchase and they really did not)
true.negative <- confusion_table[1,1]

# TP: observes as 1 and predicted as 1 (No of people that predicted to make a purchase and in reality they did)
true.positive <- confusion_table[2,2]

# FP: observes as 0 and predicted as 1 (No of people that predicted to make a purchase but they did not)
false.positive <- confusion_table[2,1]

# FN: observed as 1 and predicted as 0 (No of people that predicted they did not make a purchase but they in realty did)
false.negative <- confusion_table[1,2]

observations.total <- true.negative + false.negative + true.positive + false.positive
observed.as.0 <- true.negative + false.positive
observed.as.1 <- false.negative + true.positive

# Random guessing Rate
(nullClassifier <- max(observed.as.0 / observations.total, observed.as.1 / observations.total))

# classification Rate:
(classification.rate <- mean(yhat.logReg.test == test.df$Purchase))

# Test error Rate:
(missclassificationRate <- 1 - classification.rate)

# False Positive Rate (or: Type I error , 1 - specificty):
(FP_rates <- false.positive / observed.as.0)

# True Positive Rate (or: 1 - Type II error , power , sesetivity , recall):
(TP_rates <- true.positive / observed.as.1)

# Precision (Accuracy Rate, 1 - false discovery proportion):
# Model predicted that 311+57 people make a purchase but in reality only 57 people did (15%)
(precisions <- true.positive / (true.positive + false.positive))

# Specificity
(specificities <-  1 - false.positive/(false.positive + true.negative))

library(pROC)
roc_obj <- roc(unlist(test.df$Purchase), yhat.logReg.test)

# Let's draw some AUC plots
plot(roc_obj, legacy.axes = TRUE)

# Precision drop down from 50% in gradiant boosting to 15% in logistic regression 





```
```{r Exercise 12 part 1 Stochastic GBM}
library(tidyverse)
library(gbm)      # for original implementation of regular and stochastic GBMs
library(h2o)      # for a java-based implementation of GBM variants
library(xgboost)  # for fitting extreme gradient boosting
library(glmnet) # for Lasso
library(randomForest)
library(dataPreparation)
library(class)
library(doParallel)  # for parallel backend to foreach
library(foreach)     # for parallel processing with for loops

# Modeling packages
library(caret)       # for general model fitting
library(rpart)       # for fitting decision trees
library(ipred)   


options(warn = 1)
set.seed(1113)


smarket.df = 
  read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/Smarket.csv", 
           header=T, stringsAsFactors = F, na.strings = "?")

str(smarket.df)
# remove constant variables
constant_cols <- whichAreConstant(smarket.df)

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(smarket.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(smarket.df))

smarket.df <- if (!is.null(bijections_cols) & length(bijections_cols) > 0 ) 
  smarket.df[- bijections_cols] else smarket.df

smarket.df %>% 
   filter_all(all_vars(is.na(.))) 

# make sure no "NA is in the charachter column Direction"
(na.count <- smarket.df %>%
  filter(Direction == "NA") %>%
  nrow)

stopifnot(na.count != numeric(0))

# make sure all are Either "Up" of #Down"

ups <- smarket.df %>%
  filter(Direction == "Up") %>%
  nrow

downs <- smarket.df %>%
  filter(Direction == "Down") %>%
  nrow

stopifnot(identical(ups+downs, nrow(smarket.df)))

# Since boosting API with 'benoulli' distribution needs 0 and 1 values
# we cannot use facotr, instead we should manually encode them
smarket.df$Direction.01 <- ifelse(smarket.df$Direction == "Up", 1, 0)

smarket.df$Direction <- as.factor(smarket.df$Direction)

str(smarket.df$Direction)

# seems like "Up" -> 1 and "Down" -> 2
head(smarket.df$Direction)
head(as.integer(smarket.df$Direction))

# split to train and test
(n <- nrow(smarket.df))
(no.of.train <- round(0.7*n, 3))
train.idx <- sample(seq_len(n), no.of.train) 
test.idx <- setdiff(seq_len(n), train.idx)

stopifnot(identical(length(train.idx) + length(test.idx), n))

train.df <- smarket.df[train.idx, ]
test.df <- smarket.df[test.idx, ]


# ----------------------Gradiant boosting -----------------------
# Boosting hyperparameters: 
# -------------------------
#   
# Number of trees: The total number of trees in the sequence or ensemble.
# Learning rate (shrinkage): the smaller this value, the more accurate the model can 
# be but also will require more trees in the sequence.
# 
#
# Tree hyperparameters:
# ----------------------  
# 
# Tree depth: Controls the depth of the individual trees. Typical values range from a 
# depth of 38 but it is not uncommon to see a tree depth of 1  Note that larger n or p
# training data sets are more tolerable to deeper trees.
# 
# Minimum number of observations in terminal nodes: Controls the complexity of each tree. 
# Typical values range from 515 where higher values help prevent a model from learning 
# relationships which might be highly specific to the particular sample selected 
# for a tree (overfitting) but smaller values can help with imbalanced target classes in classification problems.


# "expand.grid" create a Data Frame from All Combinations of Factor Variables
hyper_grid <- expand.grid(
  learning_rate = c(0.3, 0.1, 0.05, 0.01, 0.005), # values between 0.050.2 should work across a wide range of problems
  RMSE = NA,
  trees = NA,
  time = NA
)

for(i in seq_len(nrow(hyper_grid))){
  set.seed(123)  
  train_time <- system.time({
    m <- gbm(
      formula = Direction.01 ~ . -Direction ,
      data = train.df,
      distribution = "bernoulli",
      n.trees = 5000, 
      shrinkage = hyper_grid$learning_rate[i], 
      interaction.depth = 3, 
      n.minobsinnode = 10,
      cv.folds = 10 
   )
  })
  
  # add SSE, trees, and training time to results
  hyper_grid$RMSE[i]  <- sqrt(min(m$cv.error))
  hyper_grid$trees[i] <- which.min(m$cv.error)
  hyper_grid$Time[i]  <- train_time[["elapsed"]]

}

# The optimal value for learning rate (i.e shrinkage parameter) 0.3
arrange(hyper_grid, RMSE)

# Next, well set our learning rate at the optimal level (0.3) and tune 
# the tree specific hyperparameters (interaction.depth and n.minobsinnode). 
# Adjusting the tree-specific parameters provides us with an additional 600 reduction in RMSE.

# search grid
hyper_grid <- expand.grid(
  n.trees = 6000,
  shrinkage = 0.3,
  interaction.depth = c(3, 5, 7),
  n.minobsinnode = c(5, 10, 15)
)

model_fit <- function(n.trees, shrinkage, interaction.depth, n.minobsinnode) {
  set.seed(123)
  m <- gbm(
    formula = Direction.01 ~ . -Direction,
    data = train.df,
    distribution = "bernoulli",
    n.trees = n.trees,
    shrinkage = shrinkage,
    interaction.depth = interaction.depth,
    n.minobsinnode = n.minobsinnode,
    cv.folds = 10
  )
  # compute RMSE
  sqrt(min(m$cv.error))
}

# perform search grid with functional programming
hyper_grid$rmse <- purrr::pmap_dbl(
  hyper_grid,
  ~ model_fit(
    n.trees = ..1,
    shrinkage = ..2,
    interaction.depth = ..3,
    n.minobsinnode = ..4
    )
)

# Top row RMSE shows it is reduced to 7.332922e-07 
arrange(hyper_grid, rmse)

# thus we choose values of n.trees -> 6000, interaction.depth -> 3, 
# and n.minobsinnode -> 5 from the top row as the parameters of 
# top model so far.
# Now to improve the model we took our top models hyperparameter settings, 

m <- gbm(
  formula = Direction.01 ~ . -Direction,
  data = train.df,
  distribution = "bernoulli",
  n.trees = 10000,
  shrinkage = 0.06,
  interaction.depth = 3,
  n.minobsinnode = 5,
  cv.folds = 10
)
# compute RMSE
sqrt(min(m$cv.error))

# Seems like no more improvenebt is possible thus the following are best set of 
# hyperparameters :

  # n.trees = 6000,
  # shrinkage = 0.3,
  # interaction.depth = 3,
  # n.minobsinnode = 5,


# Stochastic hyperparameters:
# ---------------------------
# _Subsample rows before creating each tree (available in gbm, h2o, & xgboost)
# _Subsample columns before creating each tree (h2o & xgboost)
# _Subsample columns before considering each split in each tree (h2o & xgboost)
# 
# Generally, aggressive subsampling of rows, such as selecting only 50% or less of 
# the training data, has shown to be beneficial and typical values range between 0.50.8.

# Now let's use h2o for grid search to implement a stochastic GBM 
# We use the optimal hyperparameters above and build onto this by assessing a 
# range of values for subsampling rows and columns before each tree is built, 
# and subsampling columns before each split.
# To speed up training we use early stopping for the individual GBM modeling 
# process and also add a stochastic search criteria.

# initial h2o setup
h2o.no_progress()
h2o.init(ip = "localhost", port = 54321, nthreads= -1, max_mem_size = "10g")
# prostate_path <- system.file("extdata", "prostate.csv", package = "h2o")
# prostate <- h2o.uploadFile(path = prostate_path)
h2o.removeAll()




train_h2o <- as.h2o(train.df)
test_h2o <- as.h2o(test.df)

# note that for classification gbm in h2o we have to use factor response
# quite oposite to R's gbm package that we must use numeric 0 , 1 as response variable
response <- "Direction"
predictors <- setdiff(colnames(train.df), c(response, "Direction.01"))


# refined hyperparameter grid (you must use list)
hyper_grid <- list(
  sample_rate = c(0.5, 0.75, 1),              # row subsampling
  col_sample_rate = c(0.5, 0.75, 1),          # col subsampling for each split
  col_sample_rate_per_tree = c(0.5, 0.75, 1)  # col subsampling for each tree
)

# random grid search strategy (you must use list)
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.001,  # stop if improvement is less than 0.05% in over all OOB error
  stopping_rounds = 10,       # over last 10 trees  
  max_runtime_secs = 60*5    # or stop search after 5 minutes  
)


# perform grid search 
grid <- h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_grid",
  x = predictors, 
  y = response,
  training_frame = train_h2o,
  hyper_params = hyper_grid,
  ntrees = 6000,
  learn_rate = 0.3,
  max_depth = 3,
  min_rows = 5,
  nfolds = 10,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  search_criteria = search_criteria,
  seed = 123
)

# collect the results and sort by our model performance metric of choice
grid_perf <- h2o.getGrid(
  grid_id = "gbm_grid", 
  sort_by = "mse", 
  decreasing = FALSE
)

grid_perf

# Grab the model_id for the top model, chosen by cross validation error
best_model_id <- grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)


# variable importance plot
vip::vip(best_model) 


# Now lets get performance metrics on the best model
h2o.giniCoef(best_model)
h2o.performance(model = best_model, xval = TRUE)

summary(best_model)


# pred <- h2o.predict(object=best_model, newdata=test_h2o)

# predict that a person will make a purchase if estimated probability of purchase > 0.5
pred.boost.test <- predict(best_model, newdata = test_h2o, n.trees = 6000)
pred.boost.test <- as_tibble(pred.boost.test)
# Since we encoded Yes as 1 and No as 0 we have

stopifnot(identical(length(pred.boost.test$predict),length(test.df$Direction)))

#Confusion Matrix:
(confusion_table <- table(pred.boost.test$predict, test.df$Direction))

# TN: observes as 0 and predicted as 0 (No of people that predicted Not to make any purchase and they really did not)
true.negative <- confusion_table[1,1]

# TP: observes as 1 and predicted as 1 (No of people that predicted to make a purchase and in reality they did)
true.positive <- confusion_table[2,2]

# FP: observes as 0 and predicted as 1 (No of people that predicted to make a purchase but they did not)
false.positive <- confusion_table[2,1]

# FN: observed as 1 and predicted as 0 (No of people that predicted they did not make a purchase but they in realty did)
false.negative <- confusion_table[1,2]

observations.total <- true.negative + false.negative + true.positive + false.positive
observed.as.0 <- true.negative + false.positive
observed.as.1 <- false.negative + true.positive

# Random guessing Rate
(nullClassifier <- max(observed.as.0 / observations.total, observed.as.1 / observations.total))

# classification Rate:
(classification.rate <- mean(pred.boost.test$predict == test.df$Direction))

# Test error Rate:
(missclassificationRate <- 1 - classification.rate)

# False Positive Rate (or: Type I error , 1 - specificty):
(FP_rates <- false.positive / observed.as.0)

# True Positive Rate (or: 1 - Type II error , power , sesetivity , recall):
(TP_rates <- true.positive / observed.as.1)

# Precision (Accuracy Rate, 1 - false discovery proportion):
# Model predicted that Two people make a purchase but in reality only one did (50%)
(precisions <- true.positive / (true.positive + false.positive))

# Specificity
(specificities <-  1 - false.positive/(false.positive + true.negative))

library(pROC)
roc_obj <- roc(unlist(test.df$Direction), pred.boost.test$Up)

# Let's draw some AUC plots
plot(roc_obj, legacy.axes = TRUE)


```


```{r Exercise 12 part 2 Bagging}
library(tidyverse)       # for data wrangling
library(doParallel)  # for parallel backend to foreach
library(foreach)     # for parallel processing with for loops
library(dataPreparation)
# Modeling packages
library(caret)       # for general model fitting
library(rpart)       # for fitting decision trees
library(ipred)       # for fitting bagged decision trees


options(warn = 1)
set.seed(1113)


smarket.df = 
  read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/Smarket.csv", 
           header=T, stringsAsFactors = F, na.strings = "?")

str(smarket.df)
# remove constant variables
constant_cols <- whichAreConstant(smarket.df)

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(smarket.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(smarket.df))

smarket.df <- if (!is.null(bijections_cols) & length(bijections_cols) > 0 ) 
  smarket.df[- bijections_cols] else smarket.df

smarket.df %>% 
   filter_all(all_vars(is.na(.))) 

# make sure no "NA is in the charachter column Direction"
(na.count <- smarket.df %>%
  filter(Direction == "NA") %>%
  nrow)

stopifnot(na.count != numeric(0))

# make sure all are Either "Up" of #Down"

ups <- smarket.df %>%
  filter(Direction == "Up") %>%
  nrow

downs <- smarket.df %>%
  filter(Direction == "Down") %>%
  nrow

stopifnot(identical(ups+downs, nrow(smarket.df)))

smarket.df$Direction <- as.factor(smarket.df$Direction)

str(smarket.df$Direction)

# seems like "Up" -> 1 and "Down" -> 2
head(smarket.df$Direction)
head(as.integer(smarket.df$Direction))

# split to train and test
(n <- nrow(smarket.df))
(no.of.train <- round(0.7*n, 3))
train.idx <- sample(seq_len(n), no.of.train) 
test.idx <- setdiff(seq_len(n), train.idx)

stopifnot(identical(length(train.idx) + length(test.idx), n))

train.df <- smarket.df[train.idx, ]
test.df <- smarket.df[test.idx, ]

# ------------------------------------
# apply bagging 
# we can use OOB error rate (similar to CV) to find the best number of trees asn you see below
bagging.plot.data.OOB <- tibble(OOB.err = NULL, no.of.trees = NULL, train.time = NULL)
for (no.tree in 50:100){

  train_time <- system.time({
    smarket.model <- bagging(
      formula = Direction ~ .,
      data = train.df,
      nbagg = no.tree,  # number of iterations to include in the bagged model (100 bootstrap replications)
      coob = TRUE, # use the OOB error rate
      #  uses rpart::rpart() build deep trees (no pruning) 
      # that require just two observations in a node to split
      control = rpart.control(minsplit = 2, no.of.trees = no.tree, train_time) 
                                                    
    )
  })
  
  bagging.plot.data.OOB <- rbind(bagging.plot.data.OOB , 
                             tibble(OOB.err = smarket.model$err, 
                                    no.of.trees = no.tree, 
                                    train.time = train_time))
}
bagging.plot.data.OOB
# For al the trees Out-of-bag estimate of misclassification error is zero!!
# Let's apply bagging with 10-fold CV to see how well our ensemble will generalize

smarket.model.cv <- train(
  Direction ~ .,
  data = train.df,
  method = "treebag",
  trControl = trainControl(method = "cv", number = 10),
  nbagg = 200,  
  control = rpart.control(minsplit = 2, cp = 0)
)

#----------------- Interpretation of Bagging Model -----------------------
# Bagging improves the prediction accuracy for high variance (and low bias) 
# models at the expense of interpretability and computational speed. However, 
# using various interpretability algorithms such as VIPs and PDPs, 
# we can still make inferences about how our bagged model leverages feature information

# with bagging, since we use many trees built on bootstrapped samples, we are likely 
# to see many more features used for splits. Consequently, we tend to have many 
# more features involved but with lower levels of importance.
vip::vip(smarket.model.cv, num_features = 5)


# Although the averaging effect of bagging diminishes the ability to interpret the 
# final ensemble, PDPs and other interpretability methods help us to 
# interpret any black box model. plot below highlights the unique, and sometimes 
# non-linear, non-monotonic relationships that may exist between a feature and response.

# Construct partial dependence plots
p1 <- pdp::partial(
  smarket.model.cv, 
  pred.var = "Today",
  grid.resolution = 20
  ) %>% 
  autoplot()

p2 <- pdp::partial(
  smarket.model.cv, 
  pred.var = "Lag1",
  grid.resolution = 20
  ) %>% 
  autoplot()

gridExtra::grid.arrange(p1, p2, nrow = 1)


# get the final plot and do the prediction
smarket.model <- smarket.model.cv$finalModel


# do the prediction 

pred.bag.test <- predict(smarket.model, newdata = test.df)

stopifnot(identical(length(pred.bag.test),length(test.df$Direction)))

#Confusion Matrix:
(confusion_table <- table(pred.bag.test, test.df$Direction))

# TN: observes as 0 and predicted as 0 (No of people that predicted Not to make any purchase and they really did not)
true.negative <- confusion_table[1,1]

# TP: observes as 1 and predicted as 1 (No of people that predicted to make a purchase and in reality they did)
true.positive <- confusion_table[2,2]

# FP: observes as 0 and predicted as 1 (No of people that predicted to make a purchase but they did not)
false.positive <- confusion_table[2,1]

# FN: observed as 1 and predicted as 0 (No of people that predicted they did not make a purchase but they in realty did)
false.negative <- confusion_table[1,2]

observations.total <- true.negative + false.negative + true.positive + false.positive
observed.as.0 <- true.negative + false.positive
observed.as.1 <- false.negative + true.positive

# Random guessing Rate
(nullClassifier <- max(observed.as.0 / observations.total, observed.as.1 / observations.total))

# classification Rate:
(classification.rate <- mean(pred.bag.test == test.df$Direction))

# Test error Rate:
(missclassificationRate <- 1 - classification.rate)

# False Positive Rate (or: Type I error , 1 - specificty):
(FP_rates <- false.positive / observed.as.0)

# True Positive Rate (or: 1 - Type II error , power , sesetivity , recall):
(TP_rates <- true.positive / observed.as.1)

# Precision (Accuracy Rate, 1 - false discovery proportion):
# Model predicted that Two people make a purchase but in reality only one did (50%)
(precisions <- true.positive / (true.positive + false.positive))

# Specificity
(specificities <-  1 - false.positive/(false.positive + true.negative))

# -------------------- Parallelizing Bagging -----------------------------
# Since bagging is essentially paralle algorithm we can use some parallelism here

# Create a parallel socket cluster
# cl <- makeCluster(8) # use 8 workers
# registerDoParallel(cl) # register the parallel backend
# 
# # Fit trees in parallel and compute predictions on the test set
# predictions <- foreach(
#   icount(160), 
#   .packages = "rpart", 
#   .combine = cbind
#   ) %dopar% {
#     # bootstrap copy of training data
#     index <- sample(nrow(train.df), replace = TRUE)
#     train_boot <- train.df[index, ]  
#   
#     # fit tree to bootstrap copy
#     bagged_tree <- rpart(
#       Direction ~ ., 
#       control = rpart.control(minsplit = 2, cp = 0),
#       data = train_boot
#       ) 
#     
#     predict(bagged_tree, newdata = test.df)
# }
# 
# # The result is 375 predictions (each for one row in test.df) 
# # performed by 160 trees (2*160columns , one for "Up" and one for "Down")
# predictions %>%
#   as.data.frame()
# 
# # We need to calculate RMSE
# predictions %>%
#   as.data.frame %>%
#   pmap(~sum(c(...))) %>%
#   unlist() %>%
#   tibble(Ups = . , Downs = ncol(predictions) - .) %>%
#   mutate(
#     observation = 1:n(),
#     actual = test.df$Direction)
#   
# stopCluster(cl)
```

```{r Exercise 12 part 3 RandomForest}
library(tidyverse)       # for data wrangling
library(doParallel)  # for parallel backend to foreach
library(dataPreparation)
# Modeling packages
library(ranger)   # a c++ implementation of random forest 
library(h2o)      # a java-based implementation of random forest


options(warn = 1)
options(warn = 1)
set.seed(1113)


smarket.df = 
  read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/Smarket.csv", 
           header=T, stringsAsFactors = F, na.strings = "?")

str(smarket.df)
# remove constant variables
constant_cols <- whichAreConstant(smarket.df)

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(smarket.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(smarket.df))

smarket.df <- if (!is.null(bijections_cols) & length(bijections_cols) > 0 ) 
  smarket.df[- bijections_cols] else smarket.df

smarket.df %>% 
   filter_all(all_vars(is.na(.))) 

# make sure no "NA is in the charachter column Direction"
na.count <- smarket.df %>%
  filter(Direction == "NA") %>%
  nrow

stopifnot(na.count != numeric(0))

# make sure all are Either "Up" of #Down"

ups <- smarket.df %>%
  filter(Direction == "Up") %>%
  nrow

downs <- smarket.df %>%
  filter(Direction == "Down") %>%
  nrow

stopifnot(identical(ups+downs, nrow(smarket.df)))

smarket.df$Direction <- as.factor(smarket.df$Direction)

str(smarket.df$Direction)

# seems like "Up" -> 1 and "Down" -> 2
head(smarket.df$Direction)
head(as.integer(smarket.df$Direction))

# split to train and test
n <- nrow(smarket.df)
no.of.train <- round(0.7*n, 3)
train.idx <- sample(seq_len(n), no.of.train) 
test.idx <- setdiff(seq_len(n), train.idx)

stopifnot(identical(length(train.idx) + length(test.idx), n))

train.df <- smarket.df[train.idx, ]
test.df <- smarket.df[test.idx, ]


# ----------------------------------------------
# --------------- Random forest ----------------

# By default, ranger sets the mtry parameter to  floor(sqrt(number of features)) 
# however, for regression problems the preferred mtry to start with isfloor(number of features/3). 

# We also set respect.unordered.factors = "order". This specifies how to treat 
# unordered factor variables and we recommend setting this to order. 

# number of features
n_features <- length(setdiff(names(train.df), "Direction"))

# train a default random forest model
smarket_rf1 <- ranger(
  Direction ~ ., 
  data = train.df,
  mtry = floor(sqrt(n_features)),
  respect.unordered.factors = "order",
  seed = 123
)

# get OOB RMSE
(default_rmse <- sqrt(smarket_rf1$prediction.error))

# The main hyperparameters to consider include:
# ----------------------------------------------
# _The number of trees in the forest
# _The number of features to consider at any given split:  mtry (have the largest impact on predictive accuracy)
# _The complexity of each tree (node size: one for classification and five for regression, 
#   max depth, max number of terminal nodes, or the required node size to allow additional splits)
# _The sampling scheme (by default each bootstrap copy has the same size as the original training data)
#  if you have categories that are not balanced, sampling without replacement provides 
#  a less biased use of all levels across the trees in the random forest.
# _The splitting rule to use during tree construction (used primarily to increase computational efficiency)
#  To increase computational efficiency, splitting rules can be randomized where only a random subset of 
#  possible splitting values is considered for a variable. 
#  If only a single random splitting value is randomly selected then we call this procedure extremely randomized trees. 
#  Due to the added #  randomness of split points, this method tends to have no improvement, 
# or often a negative impact, on predictive accuracy.

# Let's do Cartesian grid searches where we assess every combination of hyperparameters of interest:
hyper_grid <- expand.grid(
  mtry = floor(n_features * c(.05, .15, .25, .333, .4)),
  min.node.size = c(1, 3, 5, 10), 
  replace = c(TRUE, FALSE),                               
  sample.fraction = c(.5, .63, .8),                       
  rmse = NA                                               
)

# execute full cartesian grid search
for(i in seq_len(nrow(hyper_grid))) {
  # fit model for ith hyperparameter combination
  fit <- ranger(
    formula         = Direction ~ ., 
    data            = train.df, 
    num.trees       = n_features * 10,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$min.node.size[i],
    replace         = hyper_grid$replace[i],
    sample.fraction = hyper_grid$sample.fraction[i],
    verbose         = FALSE,
    seed            = 123,
    respect.unordered.factors = 'order',
  )
  # export OOB error 
  
  hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}

# assess top 10 models:
hyper_grid %>%
  arrange(rmse) %>%
  mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
  head(10)

# seems like sampleing only 50% of train.df with replacements consistently performs the best.
# Also mtry = 2 and min.node.size = 1 are good choices

# ------------------- Fit Random forest with h2o ---------------------
h2o.no_progress()
h2o.init(ip = "localhost", port = 54321, nthreads= -1, max_mem_size = "10g")
h2o.removeAll()

h2o.train <- as.h2o(train.df)
h2o.test <- as.h2o(test.df)
response <- "Direction"
predictors <- setdiff(colnames(train.df), response)

# fit default randomforest model with h2o
h2o.smarket_rf1 <- h2o.randomForest(
  x = predictors,
  y = response,
  training_frame = h2o.train,
  ntrees = n_features*10,
  seed = 123
)
h2o.smarket_rf1

# Let's perform a larger search with h2o

hyper_grid <- list(
  mtries = floor(n_features * c(.12,.15,.25,.333,.4)),
  min_rows = c(1,3,5,10),
  max_depth = c(10,20,30),
  sample_rate = c(.55, .632, .70, .80)
)

# random grid search strategy (you must use list)
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.001,  # stop if improvement is less than 0.05% in over all OOB error
  stopping_rounds = 10,       # over last 10 trees  
  max_runtime_secs = 60*5    # or stop search after 5 minutes  
)


# perform grid search 
grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_random_grid",
  x = predictors, 
  y = response,
  training_frame = h2o.train,
  hyper_params = hyper_grid,
  ntrees = n_features*10,
  stopping_metric = "RMSE",
  stopping_rounds = 10,
  stopping_tolerance = 0.005,
  search_criteria = search_criteria,
  seed = 123
)

# collect the results and sort by our model performance metric of choice
grid_perf <- h2o.getGrid(
  grid_id = "rf_random_grid", 
  sort_by = "mse", 
  decreasing = FALSE
)

# this is the best result:
# max_depth -> 20, 	min_rows -> 1.0,	mtries -> 3, sample_rate ->	0.7 achieved at RMSE of 0.0028
grid_perf


# permutation-based Feature Importance measure: 
# ----------------------------------------------  
#   In the permutation-based approach, for each tree, the OOB sample is passed down 
#   the tree and the prediction accuracy is recorded. Then the values for each variable 
#   (one at a time) are randomly permuted and the accuracy is again computed. 
#   The decrease in accuracy as a result of this randomly shuffling of feature values 
#   is averaged over all the trees for each predictor. 
#   The variables with the largest average decrease in accuracy are considered most important.

# For ranger, once youve identified the optimal parameter values from the grid search, 
# you will want to re-run your model with these hyperparameter values. 
# You can also crank up the number of trees, which will help create more stables 
# values of variable importance.

# re-run model with impurity-based variable importance
rf_impurity <- ranger(
  formula = Direction ~ ., 
  data = train.df, 
  num.trees = 2000,
  mtry = 3,
  min.node.size = 1, # i.e. min_rows
  sample.fraction = .80,
  replace = TRUE,
  importance = "impurity",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)

# re-run model with permutation-based variable importance
rf_permutation <- ranger(
  formula = Direction ~ ., 
  data = train.df, 
  num.trees = 2000,
  mtry = 3,
  min.node.size = 1, # i.e. min_rows
  sample.fraction = .80,
  replace = TRUE,
  importance = "permutation",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)

# variable importance plot
vip::vip(rf_impurity) 
vip::vip(rf_permutation) 

# seems like Today is most important feature in both plots

  

```


```{r }
library(tidyverse)
(t <- tibble(x = c("", " NA", "12", "34", " NA", "NA", NA), y = c(NA, "qsfg", "fbf", "12", "NA", "NA ", NA)))
(na.idx <- 
  t %>%
  pmap(~c(...)) %>%
  map(~sum(is.na(.) | str_trim(.) == "NA") > 0) %>%
  unlist %>%
  which()
)

rest.idx <- setdiff(seq_len(nrow(t)) , na.idx)

t[na.idx, ]
t[rest.idx, ]


# ------------------------

```

```{r}
# library("lme4")
# library("glmmADMB")      ## (not on CRAN)
# library("glmmTMB")
# library("MCMCglmm")
# library("blme")
# library("MASS")          ## for glmmPQL (base R)
# library("nlme")          ## for intervals(), tundra example (base R)
# ## auxiliary
# library("ggplot2")       ## for pretty plots generally
# ## ggplot customization:
# theme_set(theme_bw())
# scale_colour_discrete <- function(...,palette="Set1") {
#     scale_colour_brewer(...,palette=palette)
# }
# scale_colour_orig <- ggplot2::scale_colour_discrete
# scale_fill_discrete <- function(...,palette="Set1") {
#     scale_fill_brewer(...,palette=palette)
# }
# ## to squash facets together ...
# zmargin <- theme(panel.spacing=grid::unit(0,"lines"))
# library("gridExtra")     ## for grid.arrange()
# library("broom.mixed")
# ## n.b. as of 25 Sep 2018, need bbolker github version of dotwhisker ...
# library("dotwhisker")
# library("coda")      ## MCMC diagnostics
# library("aods3")     ## overdispersion diagnostics
# library("plotMCMC") ## pretty plots from MCMC fits
# library("bbmle")     ## AICtab
# library("pbkrtest")  ## parametric bootstrap
# library("Hmisc")
# ## for general-purpose reshaping and data manipulation:
# library("reshape2")
# library("plyr")
# ## for illustrating effects of observation-level variance in binary data:
# library("numDeriv")
# library("glmmADMB")
# bb <- glmmADMB:::get_bin_loc()[["bin_loc"]]
# bpath <- gsub("glmmadmb$","",bb)
# file.copy(bb,paste0(bpath,"glmmadmb.bak"))
# bburl <- "http://admb-project.org/buildbot/glmmadmb/"
# download.file(paste0(bburl,
#    "glmmadmb-mingw64-r2885-windows8-mingw64.exe"), dest=bb)
```

