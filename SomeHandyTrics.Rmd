---
title: "Tricks in R"
output: 
  pdf_document: default
  html_notebook: default
---

# Some clean up work using tidyverse
```{r Some tidy stuff and factor stuff}
suppressWarnings(suppressMessages(library(tidyverse)))
library(tidyverse)

wage.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Wage.csv", 
                      header=T, stringsAsFactors = F, na.strings = "?")
wage.df.original = tibble(wage.df)
wage.df = tibble(wage.df)


# convers maritl into an ordered factor
# find levels in maritl column
table(wage.df$maritl)[["1. Never Married"]]
marit_levels <- names(table(wage.df$maritl))
typeof(marit_levels)
# now convert it to order factor

# iris %>% select(starts_with(c("Petal", "Sepal")))

# iris %>%
#   group_by(Species) %>%
#   summarise(across(starts_with("Sepal"), mean))

# iris %>% summarise_at("Petal.Width", funs(min, anyNA))
# starwars %>% mutate_if(is.numeric, scale2, na.rm = TRUE)

wage.df %>%
  mutate_at("maritl", ~factor(.x,levels=(.x %>% table() %>% dimnames())))

turn.to.factor <- function(col, na.rm = FALSE) factor(col,levels=(col %>% table() %>% dimnames()))

wage.df %>%
  mutate_if(is.character, turn.to.factor, na.rm = TRUE)

```


# table and factor 
```{r table and factor}
# ------ table is used for counting records on one ore more factor columns -----
(x <- table(mtcars[c("vs", "cyl", "am")]))

# ---- for classification  use ordered factor to assign 1 to quality of interest --------# 
x<-factor(c("a","b","b","a"))
y<- ordered(c("a","b","b","a"), levels = c("c","b","a"))
str(attributes(y))

# -------------- modify levels of a factor ------- 

x <- ordered(c("a", "b", "a", "b"), levels = c("a", "b", "c"))
table(x)
levels(x) <- rev(levels(x))
table(x)

```

# Modify in place

* Modifying an R object usually creates a new copy. 
  + Use base::tracemem(x) to see if the object is copied
  
```{r}
#--------------------------------------------------------------------
# object with a single binding (object has a single name bound to it)
#--------------------------------------------------------------------

(v <- c(1,1,1))

# use base::tracemem(x) to see if the object is copied
cat(tracemem(v), "\n") # start tracing reference to v

v[[3]] <- 4
v

untracemem(v) # stop tracing reference to v
```



# Atomic Vector Subsettings and Factor Subsettings 
* Key Pints: 
  + avoid using: *is.atomic()*, *is.numeric()* and *is.vector()*, use *is.null(dim())*
  + applying *typeof()* to a vector returns type of its elements 
  + use *[ ]* to retrieve multiple values from a vector
    - using *[ [ ] ]* to get multiple values from a vector causes Error
  + recomanded to use *[ [ ] ]* to retrieve a single element from a vector
    - *[ [ ] ]* with zero length, NULL and out of bound index or name return error:
      + vector[ [NULL] ]       ==> Error
      + vector[ [integer(0)] ] ==> Error
      + vector[ [out_of_bound_integer ] ]   ==> Error
      + vector[ [out_of_bound_character ] ] ==> Error

    - *[ ]* with zero length, NULL and out of bound index or name return empty vector
    
  + only logical vector is recycled when used for subsetting  
  + *NA* has logical type thus when used for subsetting it will be recycled
  + coercion used by *c()* is: *logical* => *integer* => *double* => *character*
  + avoid automatic coercion inside *c()*, always do explicit coercion within *c()*
  + To create an empty vector of certain "type" and certain "length" 
    - vector("type", length = n) as: *vector("complex", length = 0)*
    - type(n) as: *numeric(3)*
    - type() is shorthand for type(0) as: *numeric()*
```{r atomic vector subsettings and factor subsettings}
#----------------------------------------------
# --- This is how R show an unnamed vector ---
#----------------------------------------------

(x <- c(2.1, 4.2, 5.3, 1.4))

names(x) <- c("one", "two", "three", "four")

#----------------------------------------------
# --- This is how R show a named vector ---
#----------------------------------------------

x

typeof(x)

#----------------------------------------------
# -------- subsetting by position (index) -----
#----------------------------------------------
x[c(3,1)]
# x[[c(3,1)]]  error: attempt to select more than one element vectorIndex

#--------------------------------------------------
# ---- duplicate index means duplicate values -----
#--------------------------------------------------
x[c(3,3,2,4,2,3,1,4)]
# x[[c(3,3,2,4,2,3,1,4)]] error: attempt to select more than one element vectorIndex
#-------------------------------------------------------------------------------
# --- order() returns indices such that the corresponding elements are ordered--
#-------------------------------------------------------------------------------
x[order(x)]
# x[[order(x)]] error: attempt to select more than one element vectorIndex
#----------------------------------------------------------------
# --- real numbers used as indices are truncated to integers ----
#----------------------------------------------------------------
x[c(3.1, 3.2, 3.3, 4.2)]
# x[[c(3.1, 3.2, 3.3, 4.2)]] error: attempt to select more than one element vectorIndex
#---------------------------------------------------------------
# use negative elements to exclude values at specified position
# --------------------------------------------------------------
x[-2]
# x[[-2]] error: attempt to select more than one element vectorIndex

x[c(-3:-1)]
# x[[c(-3:-1)]] error: attempt to select more than one element vectorIndex
#----------------------------------------------------------------------
# Logical vectors as index makes R chooses elements with TRUE as index
#----------------------------------------------------------------------

x[c(T,F,F,T)]
# x[[c(T,F,F,T)]] error: attempt to select more than one element vectorIndex

x[x>3]
# x[[x>3]] error: attempt to select more than one element vectorIndex
#--------------------------------  
# Only logical index is recycled 
#--------------------------------
x[T]
x[[T]] # recomended
#--------------------------------
# NA in index causes NA in output
#--------------------------------
x[c(2,3,NA,4)]
# x[[c(2,3,NA,4)]] error: attempt to select more than one element vectorIndex
# ---------------------------------------------
# Empty brackets [] will return original vector 
# ---------------------------------------------
x[]
# x[[]] error: subscript out of bound
# --------------------------------
# 0 will return zero length vector
# --------------------------------
x[0]
# x[[0]] error: attempt to select more than one element vectorIndex


#------------ Create an empty vector of given length ------------------
# vector("type", length = <<length>>) 
# <<type>>(<<length>>) like numeric(3)
# ---------------------------------------------------------------------

identical(numeric(0) , vector("double" , length = 0))
identical(logical(2) , vector("logical", length = 2))
identical(integer(0) , vector("integer", length = 0))
identical(complex(0) , vector("complex", length = 0))
identical(character(4) , vector("character", length = 4))


# ------------------------------------------------------------------
# [[]] with zero length, NULL and out of bound index or name return error
# ------------------------------------------------------------------
x[NULL] 

# x[[NULL]]  error: attempt to select less than one element in get1index

x[logical()]

# x[[logical()]] error: attempt to select less than one element in get1index

x[1000]
# x[[1000]] error: subscript out of bound

x["out_of_bound"]
# x[["out of bound"]] error: subscript out of bound

#----------------- NA as index ---------------------
# NA has logical type and logical vector is recycled 
#---------------------------------------------------
x[NA]
# x[[NA]] error: subscript out of bound
# --------------------------------------------
# named vectors can also be subset with names
# --------------------------------------------
x[c("one","three","one","four")]
# x[[c("one","three","one","four")]] # error: attempt to select more than one element vectorIndex
# -----------------------------------------------------------------------
# factor subsetting is based on underlying integer vector not the levels
# drop=T in operator [] controls if levels are dropped
# -----------------------------------------------------------------------
y <- ordered(c("a","b","b","a", "c"), levels = c("c","b","a"))
y[c(1,2,3)] 
# y[[c(1,2,3)]] # error: attempt to select more than one element vectorIndex 


y[1]
y[[1]] # recomended

y[1, drop=T]
y[[1, drop=T]] # recomended

```



# List Subsettings 
* Key Pints: 
  + *typeof()* applied on a list returns *list*
  + *[ ]* operator always returns a list whereas *[ [ ] ]* operator returns a single object
  + *$*col is a short form for *[ ["col"] ]* returns a single object bound to "col"
  + *$*col is translated to *[ ["col"] ]* by R
  + *[ [ ] ]* better be used with single positive integer or single string
  
  + Subsetting a non_empty list with *[ [ ] ]* returns:
    - list[ [NULL] ]       ==> Error
    - list[ [integer(0)] ] ==> Error
    - list[ [out_of_bound_integer ] ]   ==> NULL
    - list[ [out_of_bound_character ] ] ==> NULL

  + NULL is an empty list

  + Subsetting NULL always returns NULL 
    - NULL[ [NULL] ]       ==> NULL
    - NULL[ [integer(0)] ] ==> NULL
    - NULL[ [out_of_bound integer ] ]   ==> NULL
    - NULL[ [out_of_bound character ] ] ==> NULL

    - NULL[NULL]       ==> NULL
    - NULL[integer(0)] ==> NULL
    - NULL[out_of_bound_integer ]   ==> NULL
    - NULL[out_of_bound_character] ==> NULL

  + *[ [c(1,2)] ]* is equivalent to *[ [1] ][ [2] ]* (use *purrr::pluck(x, 1, 2)* instead)
  
  + *$* operator does partial matching but *[ [ ] ]* operator performs full matching
  + avoid silent partial matching by: options(warnPartialMatchDollar = T)
  + To create an empty list: *list()*
  + To create a list of length n full of NULL: *vector("list", length=n)*
  + *NULL == list() == list()[0] == list()[1]*
  + To convert a list to a vector use *unlist* (note *vector(list)* does not work!!)
  + Note that *unlist* strip off attributes of the object in the list
  + *list[NA]* returns *NA* 

```{r list subsetting is similar to atomic vector}
# ----------------------------------------------------------
# ----------- How R shows a named list --------------------
# ----------------------------------------------------------
(x <- list(a = 1, b=list(2,3,NULL), c=c(4,5,6) ))


# ----------------------------------------------------------
# ----------- How R shows an unnamed list ------------------
# ----------------------------------------------------------
(x <- list(1, list(2,3,NULL), c(4,5,6) ))


# -----------------------------------------------------------------------------
# ---- [] always returns a list, [[]] and $ returns elements in the list ----->
# -----------------------------------------------------------------------------
(x <- list(a=list(1,2,3,4,5,6,7,8), b = c(9,10,11), d = 13))


# -------------------------------
# -------- by position ----------
# -------------------------------
x[3]    


x[[3]]


x$d

feature.name <- "d"
x$feature.name # this returns NULL because it translates to x[["feature.name"]] 

x[[feature.name]]

x[c(1,2)]

x[[c(1,8)]] # == x[[1]][[8]]

purrr::pluck(x,1,8)

x[[1]][[8]]

x$a[[8]]



# ------------------------------------------------------------
# $ does the partial matching but [["col"]] does full maching
# to get a warning when R does partial matching always set 
#          options(warnPartialMatchDollar = T)
# ------------------------------------------------------------

options(warnPartialMatchDollar = T)
(x <- list(acd=list(1,2), adc=list(1,2), d = 13))
x$ac


(x <- list(a=list(1,2), b = c(9,10,11), d = 13))
# --------------------------------------
# duplicate index means duplicate values
# --------------------------------------
x[c(2,1,1)]


# ------------------------------------------------------------------------
# order() not implemented for list
# ------------------------------------------------------------------------


(x <- list(a=list(1,2), b = c(9,10,11), d = 13))
# ------------------------------------------------------
# real numbers used as indices are truncated to integers
# ------------------------------------------------------

x[c(3.1, 1.2)]

(x <- list(a=list(1,2), b = c(9,10,11), d = 13))
# -------------------------------------------------------------
# use negative elements to exclude values at specified position
# -------------------------------------------------------------
x[-2]

x[c(-3:-1)]


(x <- list(a=list(1,2), b = c(9,10,11), d = 13))
# --------------------------------------------------------------------
# Logical vectors as index makes R chooses elements with TRUE as index
# --------------------------------------------------------------------

x[c(T,F,F)]


# x[x>3] # # Error: 'list' object cannot be coerced to type 'double'


(x <- list(a=list(1,2), b = c(9,10,11), d = 13))
# ---------------------------------
# # Only logical index is recycled
# ---------------------------------
x[T]


(x <- list(a=list(1,2), b = c(9,10,11), d = 13))
# --------------------------------
# NA in index causes NA in output
# --------------------------------
x[c(2,3,NA,4)]

(x <- list(a=list(1,2), b = c(9,10,11), d = 13))
# ---------------------------------------------
# Empty brackets [] will return original list
# ---------------------------------------------
x[]

(x <- list(a=list(1,2), b = c(9,10,11), d = 13))
# --------------------------------
# 0 will return zero length list
# --------------------------------
x[0]


(x <- list(a=list(1,2), b = c(9,10,11), d = 13))


# ----------------------------------------------------------------------------
#  Subestting with NULL OOB (int), OOB (char) and NA
# ----------------------------------------------------------------------------

x[numeric()] 
#  x[[numeric()]] Error : attempt to select less than one element in get1index

x[NULL]
#  x[[NULL]] Error : attempt to select less than one element in get1index

x[1000]
#x[[1000]] Error : subscript out of bounds

x["out_of_bound"]
x[["out_of_bound"]]

#----------------- NA as index ---------------------
# NA has logical type and logical vector is recycled 
#---------------------------------------------------
x[NA]

x[[NA]]

# ----------------------------------------------------------------------
#  NULL is an empty list. Subsetting NULL with [[]] and [] always returns NULL
# ----------------------------------------------------------------------
NULL[[1]]
NULL[[]]
NULL[[numeric(0)]]
NULL[[NULL]]
NULL[["out_of_bound"]]
NULL[[1000]]

NULL[1]
NULL[]
NULL[numeric(0)]
NULL[NULL]
NULL["out_of_bound"]
NULL[1000]



(x <- list(a=list(1,2), b = c(9,10,11), d = 13))
# --------------------------------------------
# named vectors can also be subset with names
# --------------------------------------------
x[c("a","b","d")]

# --------------------------------------
# We can construct a factor from a list
# --------------------------------------
(y <- ordered(list("a","b","b","a", "c"), levels = c("c","b","a")))
y[c(1,2,3)] 


# ---------------------------------------------------------------
# purrr::pluck always returns NULL when OOB or empty index object
# ---------------------------------------------------------------
purrr::pluck(x, "a", 2)
purrr::pluck(x, 2, 3)
purrr::pluck(x, 3)
purrr::pluck(x, 3, 1)
purrr::pluck(x, "Out_of_Bound", 2)
purrr::pluck(x, "b", 1000)

(l <- list(x=list(1,2,3,4), y = list(5,6,7,8), z = list(9,10,11,12)))
# --------------------------------------
# get first element of each inner list
# --------------------------------------

lapply(l, function(inner.list) inner.list[[1]])

suppressWarnings(suppressMessages(library(tidyverse)))
library(tidyverse)
map(l, 1)


# ---------------------------------------------------------------------------
# ------ note unlist strips off attributes of the object inside the list ----
# ---------------------------------------------------------------------------
(l <- list(as.Date("1980-01-02")))
unlist(l)

(l1 <- rerun(2, sample(4)))
typeof(l1)
flatten_int(l1)



```
# Matrix Subsettings 
* Key Pints: 
  + *matrix* is just an atomic vector with three attributes:
    - dimensions
    - row names
    - column names
  + Subset a *matrix* by supplying a vector of indices for each dimension:
    - All rules for subsetting a vector is applied to subsetting each dimension
  + vector of values is used to fill a *matrix* up column by column or row by row
  + Dimension Dropping:
    - *[ ]* simplifies result to loweset possible dimensionality unless we set *drop=F*
    - *matrix[row #i, ]* retuns row #i as a vector
    - *matrix[ ,col# j]* retuns col #j as a vector
    - To preserve dimensionality set *drop=F* as last argument of operator *[ [ ] ]*
  + Subsetting a *matrix* with a single vector  
    - *matrix* is a vector so like vectors we can use vector of indices to subset it
    - Index of the elements in the *matrix* always goes column by column
  + Subsetting *matrix* with logical *matrix* which has same dimension as the original one
    - R selects those elements that correspond to TRUE in logical matrix


```{r matrices subsetting}


(a <- matrix(1:9, nrow = 3, byrow = F))



colnames(a) <- c("A", "B", "C")

str(a)


# --------------------------------------------------------------
# subsetting by supplying a vector of indices for each dimension
# --------------------------------------------------------------

a[0, -2]  # remember 0 as index returns an empty vector or list


a[1:2,]


a[c(T,T,F), c("A", "C")]


# ------------------------ Dimension Dropping -----------------------

a[2, ]

a[2, , drop=F]


a[, 3]

a[, 3, drop=F]

a[1,1] 

a[1,1, drop=F]

# ---------------- Subsetting a matrix with a single vector -------------------

(a <- matrix(1:4, nrow = 2, byrow = T))
colnames(a) <- c("A", "B")


a[1] # => 1
a[2] # => 3
a[3] # => 2
a[4] # => 4

a[c(2,4)]


(a <- matrix(1:4, nrow = 2, byrow = F))
colnames(a) <- c("A", "B")

a[1] # => 1
a[2] # => 2
a[3] # => 3
a[4] # => 4


a[c(2,4)]


# ------------ Subsetting with logical matrix ----------------------------
(a <- outer(1:5, 1:5, FUN="*"))

selection.matrix <- upper.tri(a)

a[selection.matrix]


# Another example of logical matrix
a <- outer(1:5, 1:5, FUN="*")
diagonal.selection <- matrix(rep(F, 25), c(5,5))
(for (i in 1:5)
  for (j in 1:5)
    diagonal.selection[i,j] <- (i==j)
)
diagonal.selection
a[diagonal.selection]

# Note:
# a$A Error in a$A : $ operator is invalid for atomic vectors

```


# Dataframe Subsettings as List of Lists and as Matrix
* Key Pints: 
  + Dataframe treated as a list of lists:
    - When subsetting with a single index or vector of indices  it behaves like a list of columns (i.e list of lists)
    - All rules for list subsetting using *[ ]* and *[ [ ] ]* are applied
    - Single index *[ ]* returns a dataframe (i.e a list, remember list subsetting rules)
    - Single index *[ [ ] ]* returns vector (i.e an object , remember list subsetting rules)
  + Dataframe treated as a *matrix*
    - Two sets of indices are used to subset a dataframe which returns a dataframe
    - Filter condition is always filter rows ,it is only on first index
    - dataframe with single column returns only that column unless we use drop=F (remember dimension dropping )
```{r dataframe subsettings}

df <- data.frame(x = 1:3, y=3:1, z = letters[1:3])

# ----------------- Dataframe: a list of lists perspective ---------------------

df[2]  # => dataframe containing second column 

df[[1]]  # =>  second column as a vector

df [c(1,3)] # dataframe containg first and third column

df [[c(1,3)]] # third element in first column (see the list subsetting)

df[c("x", "z")] # dataframe containg first and third column

df [c(2,2,1)] # dataframe contatining two times column #2 (named as Y and Y.1) and column #1 

# ----------------- Dataframe: a matrix perspective ---------------------

df[df$x == 2, ] # => dataframe containing a row(s) with value 2 in its x column 

df[c(1,3), ] # dataframe containing row# 1 and row # 3

df[c(2,2,1), ] # dataframe containing two times row #2 () and row # 1

df [, c(2,2,1)] # dataframe contatining two times column #2 (named as Y and Y.1) and column #1 

df[, 1] # remember from matrices that matrix[ ,col# j] retuns col #j as a vector

df[, 1, drop=F] # dataframe with single column #1

df[, "y"] 

df[, "y", drop=F] # dataframe with single column "y"


```
```{r dynamic column name example}
# Here is an example how to build a model dynamically in a function
library(grid)
library(gridExtra)
fit.step.model <- function(df, DV, IVs, seed=1113){
  frm <- as.formula(glue::glue(DV,"~", str_c(IVs, collapse = "+")))
  model.fit <- glm(frm, data = df)
  new.sample <- sample( levels(df [[IVs]]), 1000, replace=T)
  new.df = tibble(factor(new.sample))
  names(new.df) <- IVs
  preds <- predict(model.fit, newdata=new.df, se=T)
  

  se.bands <- cbind(preds$fit + 2 * preds$se.fit , preds$fit - 2 * preds$se.fit )
  
  data.to.draw <- tibble(x = factor(new.sample), y = preds$fit, 
                         y.se.1 = preds$fit + 2 * preds$se.fit, 
                         y.se.2 = preds$fit - 2 * preds$se.fit)
  
  # draw box plots for quality columns
  g1 <- ggplot(df, aes(factor(.data[[IVs]]), .data[[DV]], color = factor(.data[[IVs]]))) +
    geom_bar(aes(factor(.data[[IVs]]), .data[[DV]]),  stat = "summary", fun.y = mean) +
    stat_summary(fun.data = mean_sdl, width=0.05, geom = "errorbar",fun.y = mean) 

  
  #the sample we predict on
  g2 <- ggplot(data.to.draw, aes(x=x)) +
    geom_boxplot(aes(y = y, colour = DV)) +
    geom_boxplot(aes(y = y.se.1, colour = "standard error")) +
    geom_boxplot(aes(y = y.se.2, colour = "standard error"))

  grid.arrange(g1, g2, nrow = 1,
             top = textGrob("Step function fitted to a qualitative data"))
}


mtcars$cyl <- ordered(mtcars$cyl, levels = c(4,6,8))
fit.step.model(mtcars, "mpg", c("cyl"))
```


```{r filter_if mutte_if}
suppressWarnings(suppressMessages(library(tidyverse)))
library(tidyverse)

# ---------- Cool tidyverse stuff --------------# 
(df <- tibble(x = c("  "," 1  3", "12", "14  ",NA,"  "," 1  3", "12", "14  ", NA), 
              y = c(1,2,NA,4,5,1,2,NA,4,5)))

# First remove NA and empty spaces only from character columns
df %>%
  filter_if(is.character, all_vars(!is.na(.) & trimws(.) != ""))

# Next remove leading and trailing spaces from all elements in character columns
trim.f <- function(col, na.rm = F) { 
  isNA <- !reduce(col, ~ (is.na(.x) & is.na(.y)))
  if (na.rm && isNA) 
    unlist(map(col, ~ (if (is.na(.x)) "" else .x) ),use.names = F) %>%
      trimws(which = c("both")) # leading and trailing spaces
  else trimws(col, which = c("both")) # leading and trailing spaces
}

(df %>%
  mutate_if(is.character, trim.f, na.rm = T))


# pull(col) has the same effect as $col on dataframe
identical(df %>% pull(x), df$x)

# Finally convert character columns to factor

df %>%
  filter_if(is.character, all_vars(!is.na(.) & trimws(.) != "")) %>%
  mutate_if(is.character, trim.f, na.rm = T) %>%
  mutate_if(is.character, ~ ordered(.x, levels = unlist(.x %>% table %>% dimnames)))



# distinct values of certain columns
df %>% distinct(x,y)


# select only character columns
# df %>%
#   select_if(~is.character(.))
#   filter (!is.na(x) & trimws(x) != "")

# msleep %>% 
#   select(name:order, sleep_total:sleep_rem) %>% 
#   filter_if(is.character, all_vars(is.na(.)))

# ------------------------
```


```{r map reduce and all the fun stuff }
suppressWarnings(suppressMessages(library(tidyverse)))
library(tidyverse)
gapminder_orig <- 
  read.csv(
    "https://raw.githubusercontent.com/swcarpentry/r-novice-gapminder/gh-pages/_episodes_rmd/data/gapminder-FiveYearData.csv")

gapminder <-gapminder_orig
# get dataframe schema
gapminder %>%
  map_chr(class)

# get number of distince elements in each column
gapminder %>%
  map_dbl(n_distinct)

# get number of NA in each column
gapminder %>%
  map_dbl(~ sum(is.na(.)))

# get a summary of each column as a dataframe
gapminder %>%
  map_df(~ tibble(type=class(.),
                  no.of.elements = n_distinct(.), 
                  nas = sum(is.na(.))),
         .id="variable")

gapminder %>% sample_n(5) %>% pluck(1) # get the first column of dataframe

# map2 help us with zip 
map2(1:5, 1:5, ~ c(.x, .y))

plot_it <- function(df) {
  distincts.pairs <- df %>% distinct(continent, year)
  map2(distincts.pairs %>% pull(continent) %>% as.character,
       distincts.pairs %>% pull(year) ,
       ~ df %>% 
            filter(continent == .x, year == .y) %>%
            ggplot() +
            geom_point(aes(x = gdpPercap, y = lifeExp)) +
            ggtitle(glue::glue(.x, " ", .y)))
}

gapminder %>% plot_it


```




```{r nested tibbles (analogous to window functions)}
suppressWarnings(suppressMessages(library(tidyverse)))
library(tidyverse)
is_tibble(gapminder)

df_nested <- gapminder %>%
  group_by(continent) %>%
  nest()

is_tibble(df_nested$data[1])

str(df_nested$data[1])

# get the nested tibble easily
df_nested %>%
  pluck("data", 5)


# fit a separate linear regression model to each continent

df_models <- df_nested %>%
  mutate(lm_model = map(data, ~ lm(lifeExp ~ pop + gdpPercap + year, data=.)))

df_models %>%
  pluck("lm_model", 1)

# now predict on for each tibble (i.e each training data)

df_predict <- df_models %>%
  mutate(predict = map2(lm_model, data, ~ predict(.x, .y)))

# caclulate corresponding MSEs

df_predict %>%
  mutate (mse = map2_dbl(predict, data, ~  mean((.x - .y$lifeExp)^2)))

#fit a separate linear model for each continent without splitting up the data
helper <- function(df, con){
  model <- lm(lifeExp ~ pop + gdpPercap + year, data=df)
  s <- summary(model)
  print(s)
  cbind(tibble(continent = con),
        as_tibble(s$coefficients, rownames="term")) %>%
    rename("Std.Error" = "Std. Error", "statistics" = "t value", "pValue" = "Pr(>|t|)")
}

nested <- gapminder %>% group_by(continent) %>% nest()
  
df_list <- map2(nested$data, nested$continent, ~helper(.x, .y))

df_list %>% reduce(rbind)

# a better solution
gapminder %>% 
  group_by(continent) %>% 
  nest() %>%
  mutate(lm_obj = map(data, ~lm(lifeExp ~ pop + year + gdpPercap, data = .))) %>%
  mutate(lm_tidy = map(lm_obj, broom::tidy)) %>%
  ungroup() %>% # on next line we use "continent" column that we grouped on
  transmute(continent, lm_tidy) %>%
  unnest(cols = c(lm_tidy)) # explode nested tibbles rows into containing df

# To apply mutate functions to a list-column (i.e. a tibble column)
# you need to wrap the function you want to apply in a map function.

df_nested %>%
  mutate(dimensions = glue::glue(map_int(data,nrow), " X ", map_int(data,ncol)))

# another example, calculate average life expentency in each continit
df_nested %>%
  mutate(life_expect = map_dbl(data, ~ mean(.$lifeExp)))

```



```{r map_df, split, keep, discard, stringr, head_while, tail_while, rerun}
suppressWarnings(suppressMessages(library(tidyverse)))
library(tidyverse)
# map(element's index) extracts an element in the index from all nested lists

list(ids = 1:3, values=c("one", "two", "three")) %>%
  map(2)

# read list of csv files with the same schema and merge them
list.files("../open-data/", pattern = "^2017", full.names = TRUE) %>%
  map_df(read_csv)

# split simply group-by on given column and place them in separate tibbles
gapminder_list <- gapminder %>% 
  split(gapminder$continent)


# Sample 5 records from each continent tibble
samples <- gapminder %>% 
  split(gapminder$continent) %>%
  map(~(sample_n(., 5)))
samples

# keep those list elements (here datasets) that satisfy certain coditions
samples %>%
  keep(~(mean(.$lifeExp) > 70))

# discard with some magical stringr 
"absc61, sf22ve23,NA, wefc,wfverv,3rf3f344,rffr3$f446,,,rf11,NA,345fv,f3rf3" %>%
  str_split(",") %>% # list containing one vector that contains splitted strings
  pluck(1) %>% #get the first list in outer list
  map_chr(str_trim) %>% # remove white spaces
  discard(~ .=="" | .=="NA") %>% # discard empty strings or "NA"
  keep(~ str_extract(., ".$") %in% 0:9) %>% # keep those ends with a digit
  discard(~ as.numeric(str_extract(.x, "[:digit:]+$")) < 5)

# head_while and tail_while
sample(1:100) %>%
  head_while(~ . < 10)

sample(1:100) %>%
  tail_while(~ . > 20)

# run a function for a number of times
(m <- rerun(10, rnorm(10)) %>% # provides a list of 10 vectors each containing 10 iid
  reduce(cbind)) 
colnames(m) <- 1:10 %>% map_chr (~ str_c("col", .))
as_tibble(m)

# imap is indexed map
rerun(.n = 10, rnorm(10)) %>%
  imap_dfr(~ tibble(run = .y, 
                    mean = mean(.x),
                    sd = sd(.x),
                    median = median(.x)))


```


```{r reduce, accumulate, merge}
suppressWarnings(suppressMessages(library(tidyverse)))
library(tidyverse)
# reduce /  reduce_right is R's foldleft / foldRight

# -----------  foldLeft(List[Int]())((acc, x) => acc :+ (x + 1)) -----------
# Note #1: The First argument of the function is always zero of monoid.
# you can set zero of monoid using ".init" argument of reduce.
# If .init is not provided then first element of the list is used as zero.
# Note '.init = NULL' forces reduce to use NULL (i.e. an empty list) as 
# zero of the monoid and pass it as the first argument to the provided function 
# rather than using .x[[1]]. 
# reduce passes elements of the collection as function's second argument.

list(1,2,3,4,5) %>%
  reduce(~append (.x, .y + 1), .init = NULL)

list(1,2,3,4,5) %>%
  reduce_right (~append (.x, .y + 1), .init = NULL)

# reduce to find intersection between multiple lists



# use reduce to append multiple data frames 
df1 <- data.frame(Customer_ID = c(1, 2, 3), Name = c("John", "Sue", "Ann"))
df2 <- data.frame(Customer_ID = c(4, 5, 6), Name = c("Joe", "Suzy", "Annable"))

list(df1, df2) %>%
  reduce(rbind)

# accumulate does exactly the same thing as reduce but it also returns 
# accumulated value on each loop, provided .init is accessible by index
list(df1, df2) %>%
  accumulate(rbind)


# ----------- merge (join, left join, right join and ntural join) ----------
# First remember what "all" option of "merge" does:
# all = FALSE (default value) inner join where columns with the same name of 
#                             associated tables will appear once only.
# all.x = TRUE gives a left (outer) join, 
# all.y = TRUE a right (outer) join, 
# (all = TRUE) a (full) outer join. 
# DBMSes do not match NULL records, equivalent to incomparables = NA in R.

#  More data for merge 
df1 <- data.frame(Customer_ID = c(1, 2, 3), Name = c("John", "Sue", "Ann"))
df2 <- data.frame(Customer_ID = c(1, 3), Year_First_Order = c(2011, 2017))
df3 <- data.frame(Customer_ID = c(1, 2, 3), 
                  Date_Last_Order = c("2017-03-03", "2014-03-01", "2017-05-30"),
                  No_Items_Last_Order = c(3, 1, 1),
                  Total_Amount_Last_Order = c(49, 25,25))
df4 <- data.frame(Customer_ID = c(2, 3), Interested_In_Promo = c(TRUE, FALSE))

merge(df1 , df2, by="Customer_ID")
merge(df1 , df2, by="Customer_ID", all.x = T)

# now we use reduce to join the dataframes
list(df1,df2,df3,df4) %>%
  reduce(~ merge(.x, .y, all.x = T))


# another example of reduce : Sums of matrix powers
# we need package expm to do matrix power
library(expm)
(m <- rbind(c(0.9, 0.1), c(1, 0)))
1:20 %>% 
  map(~ (m %^% .)) %>% 
  reduce(~(.x+.y), .init = rbind(c(0,0),c(0.0)))


```




```{r}
suppressWarnings(suppressMessages(library(tidyverse)))
library(tidyverse)
## A general-purpose adder:
add <- function(x) Reduce("+", x)
add(list(1, 2, 3))
## Like sum(), but can also used for adding matrices etc., as it will
## use the appropriate '+' method in each reduction step.
## More generally, many generics meant to work on arbitrarily many
## arguments can be defined via reduction:
FOO <- function(...) Reduce(FOO2, list(...))
FOO2 <- function(x, y) UseMethod("FOO2")
## FOO() methods can then be provided via FOO2() methods.

## A general-purpose cumulative adder:
cadd <- function(x) Reduce("+", x, accumulate = TRUE)
cadd(seq_len(7))

## A simple function to compute continued fractions:
cfrac <- function(x) Reduce(function(u, v) u + 1 / v, x, right = TRUE)
## Continued fraction approximation for pi:
cfrac(c(3, 7, 15, 1, 292))
## Continued fraction approximation for Euler's number (e):
cfrac(c(2, 1, 2, 1, 1, 4, 1, 1, 6, 1, 1, 8))

## Iterative function application:
Funcall <- function(f, ...) f(...)
## Compute log(exp(acos(cos(0))))
Reduce(Funcall, list(log, exp, acos, cos), 0, right = TRUE)
## n-fold iterate of a function, functional style:
Iterate <- function(f, n = 1)
    function(x) Reduce(Funcall, rep.int(list(f), n), x, right = TRUE)
## Continued fraction approximation to the golden ratio:
Iterate(function(x) 1 + 1 / x, 30)(1)
## which is the same as
cfrac(rep.int(1, 31))
## Computing square root approximations for x as fixed points of the
## function t |-> (t + x / t) / 2, as a function of the initial value:
asqrt <- function(x, n) Iterate(function(t) (t + x / t) / 2, n)
asqrt(2, 30)(10) # Starting from a positive value => +sqrt(2)
asqrt(2, 30)(-1) # Starting from a negative value => -sqrt(2)

## A list of all functions in the base environment:
funs <- Filter(is.function, sapply(ls(baseenv()), get, baseenv()))
## Functions in base with more than 10 arguments:
names(Filter(function(f) length(formals(f)) > 10, funs))
## Number of functions in base with a '...' argument:
length(Filter(function(f)
              any(names(formals(f)) %in% "..."),
              funs))

## Find all objects in the base environment which are *not* functions:
Filter(Negate(is.function),  sapply(ls(baseenv()), get, baseenv()))

```

```{r raandom walk simulation}
suppressWarnings(suppressMessages(library(tidyverse)))
library(tidyverse)

rerun(5, rnorm(100)) %>%
  set_names(paste0("sim", 1:5)) %>%
  map(~ accumulate(., ~ .05 + .x + .y)) %>%
  map_dfr(~ tibble(value = .x, step = 1:100), .id = "simulation") %>%
  ggplot(aes(x = step, y = value)) +
    geom_line(aes(color = simulation)) +
    ggtitle("Simulations of a random walk with drift")

```
```{r Vectorize}
# We use rep.int as rep is primitive
rep.int(12,5)
vrep <- Vectorize(rep.int)
vrep(1:4, 4:1)
vrep(times = 1:4, x = 4:1)

vrep <- Vectorize(rep.int, "times")
vrep(times = 1:4, x = 42)

f <- function(x = 1:3, y) c(x, y)
vf <- Vectorize(f, SIMPLIFY = FALSE)
f(1:3, 1:3)
vf(1:3, 1:3)
vf(y = 1:3) # Only vectorizes y, not x

# Nonlinear regression contour plot, based on nls() example
require(graphics)
SS <- function(Vm, K, resp, conc) {
    pred <- (Vm * conc)/(K + conc)
    sum((resp - pred)^2 / pred)
}
vSS <- Vectorize(SS, c("Vm", "K"))
Treated <- subset(Puromycin, state == "treated")

Vm <- seq(140, 310, length.out = 50)
K <- seq(0, 0.15, length.out = 40)
SSvals <- outer(Vm, K, vSS, Treated$rate, Treated$conc)
contour(Vm, K, SSvals, levels = (1:10)^2, xlab = "Vm", ylab = "K")

# combn() has an argument named FUN
combnV <- Vectorize(function(x, m, FUNV = NULL) combn(x, m, FUN = FUNV),
                    vectorize.args = c("x", "m"))
combnV(4, 1:4)
combnV(4, 1:4, sum)
```

```{r some preprocessing}
suppressWarnings(suppressMessages(library(tidyverse)))
library(tidyverse)

wage.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Wage.csv", 
                      header=T, stringsAsFactors = F, na.strings = "?")
wage.df.original = tibble(wage.df)
(wage.df = tibble(wage.df))

# remove empty characters and NA helper
remove.empty.characters <- function(df) 
  df %>%
    select_all %>%
    filter_if(is.character, all_vars(!is.na(.) & trimws(.) != ""))

# Next remove leading and trailing spaces from all elements in character columns
trim.f <- function(col, na.rm = F) { 
  isNA <- !reduce(col, ~ (is.na(.x) & is.na(.y)))
  if (na.rm && isNA) 
    unlist(map(col, ~ (if (is.na(.x)) "" else .x) ),use.names = F)
  else trimws(col, which = c("both")) # leading and trailing spaces
}

trim.spaces <- function(df)
  (df %>%
    mutate_if(is.character, trim.f, na.rm = T))

# Finally convert character columns to factor

char.to.fctor <- function(df)
  df %>%
    mutate_if(is.character, ~ factor(.x, levels = (.x %>% table() %>% names())))


(wage.df <- 
  wage.df %>%
  na.omit() %>%
  trim.spaces() %>%
  remove.empty.characters() %>%
  char.to.fctor())


```
---
title: "R for Data science"
output: html_notebook
---


```{r Data filter}
library(nycflights13)
library(tidyverse)

str(flights)

# filter() are combined with “and”: every expression must be true in order
# for a row to be included in the output

(jan1 <- filter(flights, month==1 , day == 1))

# use near for double value equalities
near (sqrt(2) ^ 2 , 2)

(jan1 <- filter(flights, month==11 | month == 12))
# equivalently
jan1 <- filter(flights, month %in% c(11,12))

# filter excludes both FALSE and NA values
df <- tibble(x = c(1, NA, 3))
filter(df, x>1)
filter(df, is.na(x) | x > 1)

# Exercise 5.2.4
#Find all flights that:
  # Had an arrival delay of two or more hours
filter(flights, arr_delay >= 2)
  #Flew to Houston (IAH or HOU)
filter(flights, dest %in% c("IAH", "HOU"))
  #Were operated by United, American, or Delta
filter(flights, carrier %in% c("UA", "AM", "DEL"))
  # Departed in summer (July, August, and September)
filter(flights, month %in% c(7, 8, 9))
  # Arrived more than two hours late, but didn’t leave late
filter(flights, dep_delay <= 0 & arr_delay >= 120)
flights[between(flights$dep_delay,0, 120), ]
  # Were delayed by at least an hour, but made up over 30 minutes in flight
filter(flights, dep_delay > 60 & arr_delay <= 30)
  # Departed between midnight and 6am (inclusive)
filter(flights, dep_time <= 600)

#How many flights have a missing dep_time
paste0(nrow(filter(flights, is.na(dep_time))), " flights have missing dep_time value")

# complete.cases gives TRUE when all values in a row are not NA
flights[!complete.cases(flights), ]
# columns that have NA
colnames(flights[!complete.cases(flights), ])
?complete.cases

# Arrange rows by getting a set of column names (or more complicated expressions) to order by
arrange(flights, year, month, desc(day))

# Missing values are always sorted at the end:
df <- tibble(x = c(5,2,NA))
arrange(df, x)

# 5.3.1 Exercises

# Sort missing values in the start
arrange(df, desc(is.na(x)), x)

# Sort flights to find the most delayed flights. 
arrange(flights, desc(dep_delay))[1,]


# Find the flights that left earliest.
arrange(flights, dep_time)[1,]

# Sort flights to find the fastest flights.
arrange(flights, distance/air_time)[1,]

#  Which travelled the shortest?
arrange(flights, distance)[1,]

# Which flights travelled the farthest?
arrange(flights, desc(distance))[1,]

```

```{r Select columns }
library(nycflights13)
library(tidyverse)

select(flights, year, month, day)

# Select all columns between year and day (inclusive)
select(flights, year:day)

# Select all columns except those from year to day (inclusive)
select(flights, -(year:day))

# select all columns whose name contains string "ela"
select(flights, contains("ela"))

# select all columns whose name ends with "_time"
select(flights, ends_with("_time"))

# rename a column
rename(flights, departure_time=dep_time, arrival_time = arr_time)

#  handful of variables you’d like to move to the start of the data frame.
select(flights, dep_time, arr_time, sched_dep_time, sched_arr_time, everything())

# 5.4.1 Exercises

# Brainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights.

select(flights, dep_time, dep_delay, arr_time, arr_delay)
select(flights, starts_with("dep_") | starts_with("arr_"))
select(flights, starts_with("dep_")  & (ends_with("_delay") | ends_with("_time")) 
       | starts_with("arr_")  & (ends_with("_delay") | ends_with("_time")))

# What happens if you include the name of a variable multiple times in a select() call?
select(flights, dep_time, dep_time, dep_time, arr_delay)

# What does the one_of() function do? Why might it be helpful in conjunction with this vector?
cols <- c("dep_time","XXX","dep_delay","ZZZ","QQQ","arr_delay")
select(flights, one_of(cols))

# case insensetivity is surprising
select(flights, contains("TIME", ignore.case = F))
```


```{r Add new columns with mutate() using vectorized functions}
library(nycflights13)
library(tidyverse)

# mutate() always adds new columns at the end of your dataset so we’ll start by creating a narrower dataset so we can see the new variables.

flights_small <- select(flights, year:day, ends_with("delay"), distance, air_time)
# view(flights_small)

flights_small

mutate(flights, gain = dep_delay - arr_delay, gain_per_hour = gain/hour, 
       speed = distance/air_time *60)

# If you only want to keep the new variables, use transmute()
transmute(flights, gain = dep_delay - arr_delay, gain_per_hour = gain/hour, 
       speed = distance/air_time *60)

# function must be vectorised to be able to use in mutate

# Modular arithmetic: %/% (integer division) and %% (remainder) are vectorized
transmute(flights, hour = dep_time %/% 100, min = dep_time %% 100)

# log() functions are also vectorize
transmute(flights, gain = dep_delay - arr_delay, gain_per_hour = gain/hour, logOfGain = log2(gain_per_hour))

# lead() and lag() allows you to compute running differences (e.g. x - lag(x)) or 
# find when values change (x != lag(x)). 

(x <- 1:10)
lag(x)
lead(x)
transmute(flights, dest, lag(dest), lead(dest))

# Cumulative and rolling aggregates (i.e. a sum computed over a rolling window): 
# R => cumsum(), cumprod(), cummin(), cummax(); 
(x <- 1:10)
cumsum(x)
cumprod(x)
cummin(x)
cummax(x)

# dplyr => cummean() for cumulative means
cummean(x)

# For Rolling aggregates use RcppRoll package
x <- matrix(rnorm(100),nrow=50,ncol=2)
x
RcppRoll::roll_sum(x,12)

# ranking (basically report the indices of elements if they were sorted, NA and INF goes to the end)
x <- c(5, 1, 3,Inf, 2, 2, NA) # => (1,2,2,3,5,Inf,NA)
row_number(x)
min_rank(x)
dense_rank(x)
percent_rank(x) # a number between 0 and 1 computed by rescaling min_rank to [0, 1]
cume_dist(x)  #a cumulative distribution function. Proportion of all values less than or equal to the current rank

# ntile creates a rough rank, which breaks the input vector into n buckets
ntile(x, 2) 
ntile(runif(100), 10)


# 5.5.2 Exercises

# Convert dep_time and sched_dep_time to a more convenient representation of number of minutes since midnight

#flights[order(flights$dep_time, na.last = T,decreasing = T), ]
flights1 <- mutate(flights, dep_time_minute = (dep_time%/%100) * 60 + dep_time%%100, sched_dep_time_minute = (sched_dep_time%/%100) * 60 + sched_dep_time%%100)
select (flights1,year,month, day,dep_time, dep_time_minute, sched_dep_time, sched_dep_time_minute, everything())

# Compare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?
fixed_air_time_flights <- transmute (flights, arr_time, dep_time, air_time, fixed_air_time_minute = abs( ((arr_time%/%100)*60+arr_time%%100) - ((dep_time%/%100)*60+dep_time%%100) ), fixed_air_time =  (fixed_air_time_minute%/%60)*100 + (fixed_air_time_minute%%60))
fixed_air_time_flights[order(fixed_air_time_flights$fixed_air_time), ]
filter(fixed_air_time_flights, arr_time <= dep_time)

# Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?
transmute(flights, dep_time, sched_dep_time, dep_delay, dep_delay_fixed =  dep_time - sched_dep_time )

# Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank()
filter (flights , min_rank(desc(dep_delay)) <= 10)

flights[order(flights$dep_delay, decreasing = T), ]

```
```{r summarize with group_by}

library(nycflights13)
library(tidyverse)

# summarize() with group_by() changes the unit of analysis from the complete dataset to individual groups. 
# Then, when you use the dplyr verbs on a grouped data frame they’ll be automatically applied “by group”

(by_day <- group_by(flights, year, month, day))
str(attributes(by_day))

# delay by day
summarise(by_day, delay = mean(dep_delay, na.rm = T))


```

```{r Combining multiple operations with the pipe}
library(nycflights13)
library(tidyverse)


# Explore the relationship between the distance and average delay for each location
delay <- flights %>%
  group_by(dest) %>%
  summarise(count = n(), dist = mean(distance, na.rm = T), delay = mean(arr_delay, na.rm = T)) %>%
  filter(count > 20 & dest != "HNL")

ggplot(data = delay, mapping = aes(x = dist, y = delay))+
  geom_point(aes(size=count), alpha=1/3)+
  geom_smooth(se=F)

# compare it with:
# by_destination <- group_by(flights, dest)
# (avgDelayByDest <- summarise(by_destination, count = n(), dist = mean(distance, na.rm = T), delay = mean(arr_delay, na.rm = T)))
# (delay <- filter(avgDelayByDest, count > 20 & dest != "HNL"))

```

```{r count }
library(nycflights13)
library(tidyverse)

# count missing values in arr_delay
missing_arr_delay <- flights %>%
  filter(is.na(arr_delay) | is.na(dep_delay)) %>%
  group_by(arr_delay) %>%
  summarise(count = n())
  
sprintf("Number of records with missing arr_delay or dep_delay: %d",missing_arr_delay$count)

# Remove records with missing arr_delay or dep_delay
(not_cancelled <- flights %>%
  filter(!is.na(arr_delay) & !is.na(dep_delay))
)

# let’s look at the planes (identified by their tail number) that have the highest average delays:

delays <- not_cancelled %>%
  group_by(tailnum) %>%
  summarise(delay = mean(arr_delay)) 

ggplot(data = delays, mapping = aes(x = delay)) +
  geom_freqpoly(binwidth = 10)
  
# Wow, there are some planes that have an average delay of 5 hours (300 minutes)!
# Let's draw a scatterplot of number of flights vs. average delay:

(delays1 <- not_cancelled %>%
  group_by(tailnum) %>%
  summarise(count = n(), delay = mean (arr_delay))
)

# filter(delays1 , is.na(delay) | is.na(count))
delays1 %>%
  # it’s often useful to filter out the groups with the smallest numbers of observations to see patterns more clearly
  filter (count > 50) %>% 
  ggplot(mapping = aes(x=count, y=delay))+
  geom_point(alpha=1/10)

# ANORTHER EXAMPLE:
# When I plot the skill of the batter (measured by the batting average, ba) against the number of opportunities to hit the ball 
# (measured by at bat, ab), you see two patterns:

batting <- as_tibble(Lahman::Batting)
(batters <- batting %>%
  group_by(playerID) %>%
  summarise(ba = sum(H, na.rm = T)/sum(AB, na.rm = T), ab = sum(AB, na.rm = T))
)

# Variation in our aggregate decreases as we get more data points.
# There’s a positive correlation between skill (ba) and opportunities to hit the ball (ab).

batters %>% 
 filter (ab > 100) %>%
  ggplot(mapping = aes(x=ab, y = ba)) +
  geom_point()+
  geom_smooth(se=F)

# If you naively sort on desc(ba), the people with the best batting averages are clearly lucky, not skilled:

batters %>%
  arrange(desc(ba))
```

```{r Summary functions: mean(), n(), sum(), median(), sd(), IQR(), mad()}
# sum(x > 10) can take a logical expression that filters in certain records and then it adds them (sum of bunch of T and F values)
# mean (x > 60) can filters in records whose column 'x' value is greater than 60 and  then calculate the mean of x for filtered records
# Measures of location: mean(x), median(x) (Half of the values of x is less than median(x) and other half is greater)
# Measures of spread: sd(x), IQR(x), mad(x)
# Measures of rank: min(x), quantile(x, 0.25), max(x)
# Measures of position: first(x), nth(x, 2), last(x). These work similarly to x[1], x[2], and x[length(x)] but let you set a default value if that position does not exist (i.e. you’re trying to get the 3rd element from a group that only has two elements).

# Counts: 
#   n() : returns the size of the current group. 
#   count(x) : counts number of repeatitions of each element in a qualitative column x
#   sum(!is.na(x)) : number of non-missing values in current group
#   n_distinct(x) : number of distinct (unique) values in current group

# Counts and proportions of logical values: sum(x > 10), mean(y == 0)

# quantile(x, 0.25) will find a value of x that is greater than 25% of the values, and less than the remaining 75%.
# IQR is 3rd Quartile - 1st Quartile (i.e the box plot)
# mad is median absolute deviation mad(x) may be more useful if you have outliers


library(nycflights13)
library(tidyverse)

(not_cancelled <- flights %>%
  filter(!is.na(arr_delay) & !is.na(dep_delay))
)

(not_cancelled %>% 
  group_by(year, month, day) %>%
  summarise(avg_arr_delay = mean(arr_delay), avg_pos_arr_delay = mean(arr_delay[arr_delay > 0]))
)


not_cancelled %>%
  group_by(dest) %>%
  summarise(distance_sd = sd(distance)) %>%
  arrange(desc(distance_sd))

not_cancelled %>%
  group_by(year, month, day) %>%
  summarise(first = min(dep_time), last=max(dep_time))

not_cancelled %>%
  group_by(year, month, day) %>%
  summarise(first_dep = first(dep_time), last_dep = last(dep_time))

# Filtering on ranks gives you all variables, with each observation in a separate row:
not_cancelled %>%
  group_by(year, month, day) %>%
  mutate(rank = min_rank(desc(dep_time))) %>%
  filter(rank %in% range(rank))
  
# which destination have the most carriers
not_cancelled %>%
  filter(!is.na(dest) & !is.na(carrier)) %>%
  group_by(dest) %>%
  summarize(max_carriers = max(n_distinct(carrier))) %>%
  arrange(desc(max_carriers))

# Give a count of each destinations separately
not_cancelled %>%
  count(dest)


# You can optionally provide a weight variable. 
# “count” (sum) the total number of miles a plane flew:

not_cancelled %>%
  count(tailnum, wt=distance)

# Whcih is the same as 
not_cancelled %>%
  group_by(tailnum) %>%
  summarise(n=sum(distance))


# How many flights left before 5am?
not_cancelled %>% 
  group_by(year, month, day) %>% 
  summarise(n_early = sum(dep_time < 500))

# What proportion of flights are delayed by more than an hour?
not_cancelled %>% 
  group_by(year, month, day) %>%
  summarize(proportion = mean(arr_delay > 60))
```

```{r 5.6.5 When you group by multiple variables, each summary peels off one level of the grouping}
library(nycflights13)
library(tidyverse)

(per_day <- flights %>%
  group_by(year, month, day) %>%
   summarise(flights=n())
)

(per_month <- per_day %>%
  summarize(flights = sum(flights)))

(per_year <- per_month %>%
    summarize(flights = sum(flights)))
  
# Equivalently

(per_day <- flights %>%
  group_by(year, month, day) %>%
   summarise(per_day_flights=n()) %>%
    summarise(per_month_flights = sum(per_day_flights)) %>%
    summarise(per_year_flights = sum(per_month_flights))
)
# Be careful when progressively rolling up summaries: it’s OK for sums and counts, but you need to think about weighting means and variances, and it’s not possible to do it exactly for rank-based statistics like the median
# I.e. the sum of groupwise sums is the overall sum, but the median of groupwise medians is not the overall median.

# If you need to remove grouping, and return to operations on ungrouped data, use ungroup()
daily <- group_by(flights, year, month, day)

daily %>%
  ungroup() %>% # no longer grouped by year-month-day
  summarise(flights = n())

```

```{r 5.6.7 Exercises}
library(nycflights13)
library(tidyverse)

not_cancelled <- flights %>% 
  filter(!is.na(arr_delay) & (!is.na(dep_delay)))
# 1)
# A flight is 15 minutes early 50% of the time, and 15 minutes late 50% of the time.
(not_cancelled %>%
    group_by(flight) %>%
    summarize(total = n(), 
              early15 = sum(arr_delay == -15), 
              late15 = sum (arr_delay == 15)) %>%
    filter(total != 0 & early15 != 0 & late15 != 0 &  near(total / 2, 0.5))

  )


# Another way :
(not_cancelled %>%
    group_by(flight) %>%
    summarize(total = n(), 
              early15 = mean(arr_delay == -15, na.rm = T), 
              late15 = mean (arr_delay == 15, na.rm = T)) %>%
    filter(total != 0 & early15 == 0.5 & late15 == 0.5)

  )

# A flight is always 10 minutes late.
(not_cancelled %>%
    group_by(flight) %>%
    filter (arr_delay == 10)
)

# 99% of the time a flight is on time. 1% of the time it’s 2 hours late.

not_cancelled %>%
    group_by(flight) %>%
    summarize (total = n(), ontime = sum(arr_delay == 0), late = sum(arr_delay == 2)) %>% 
  filter((ontime %/% total)*100 == 99 && (late %/% total)*100 == 1)

# 2) 
#  not_cancelled %>% count(dest) 
not_cancelled %>% 
  group_by(dest) %>%
  summarise(n = n())


# not_cancelled %>% count(tailnum, wt = distance) 
not_cancelled %>%
  group_by(tailnum) %>%
  summarise(wt = sum(distance))
 
# 4) 
# Look at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the average delay?

flights %>%
  group_by(year, month, day) %>%
  summarize(cancelledFlights = sum(is.na(arr_delay) | (is.na(dep_delay))),
            avgArrDelay = mean(is.na(arr_delay), na.rm = T)) %>%
  arrange(desc(avgArrDelay)) %>%
  filter(cancelledFlights <= 75) %>%
  ggplot(mapping = aes(x=cancelledFlights, y = avgArrDelay)) +
  geom_point()+
  geom_smooth(se=F)
  
# 5)
# Which carrier has the worst delays? 
# Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? 
# (Hint: think about flights %>% group_by(carrier, dest) %>% summarise(n()))


# not_cancelled %>%
#   group_by(carrier, dest) %>%
#    summarise(max_delay_per_dest = max(arr_delay, na.rm = T)) %>%
#      summarise(max_delay_per_carr = max(max_delay_per_dest, na.rm = T)) %>%
#   arrange(desc(max_delay_per_carr))

not_cancelled %>%
  group_by(carrier, dest) %>%
  summarise(max_delay_per_dest = max(arr_delay, na.rm = T)) %>%
  group_by(dest) %>%
  mutate(rank = min_rank(desc(max_delay_per_dest))) %>%
  filter(rank %in% range(rank)) %>%
  arrange(carrier, dest)

# 6) What does the sort argument to count() do?
not_cancelled %>% 
  count(tailnum, wt = distance, sort = T) 

# assume we want to count (number, letter) pair
(data = tibble(
  letter = sample(LETTERS, 50000, replace = TRUE),
  number = sample (1:10, 50000, replace = TRUE)
  ))

data %>% 
  count(letter, number, sort = TRUE)

data %>% 
  group_by(letter, number) %>% 
  summarise(n = n()) %>% 
  ungroup() %>% 
  arrange(desc(n))

data %>% 
  count(letter, number) %>% 
  ungroup() %>% 
  arrange(desc(n))
```


```{r group_by with mutate() and filter()}
library(nycflights13)
library(tidyverse)

# Find the worst members of each group
flights %>%
  group_by(year, month, day) %>%
  filter(rank(desc(arr_delay)) < 4)

(r1 <- rank(x1 <- c(3, 1, 4, 15, 92)))
(r2 <- min_rank(x1 <- c(3, 1, 4, 15, 92)))

# Find all groups bigger than a threshold
(poular_dests <-
  flights %>%
  group_by(dest) %>%
  filter(n() > 365))

# A grouped filter is a grouped mutate followed by an ungrouped filter. I generally avoid them except for quick and dirty manipulations: otherwise it’s hard to check that you’ve done the manipulation correctly.

poular_dests %>%
  filter(arr_delay > 0) %>%
  mutate(prop_delay = arr_delay / sum(arr_delay)) %>%
  select(year:day, dest, arr_delay, prop_delay)



```


```{r Exercises 5.7.1}
# 1) 
# Filter function is appalied to each group and shrinks the elements of each group 
flights %>%
  group_by(year, month, day) %>%
  filter (air_time == 320 & carrier=="US")

# mutate function with group
# Arithmetic operators with group_by
flights %>%
  
 
```

```{r}

#Functions that work most naturally in grouped mutates and filters are known as window functions (vs. the summary functions used for summaries). You can learn more about useful window functions in the corresponding vignette: vignette("window-functions")

vignette("window-functions")
```

```{r Some data prepration hints for great goods using dataPreparation}
library(dataPreparation)


auto.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Auto.csv", header=T, stringsAsFactors = F, na.strings = "?")
auto.df.original = tibble(auto.df)
auto.df = tibble(auto.df)

# see if there is any NA in any records
df[which(is.na(auto.df)),]

# lets first split data to 80% train and the rest test 
set.seed(1113)
train.idx <- sample(1:nrow(auto.df), 0.8*nrow(auto.df))
test.idx <- setdiff(1:nrow(auto.df), train.idx)

train.df <- auto.df[train.idx, ]
test.df <- auto.df[test.idx, ]
# remove empty characters and NA helper
remove.empty.characters <- function(df) 
  df %>%
    select_all %>%
    filter_if(is.character, all_vars(!is.na(.) & trimws(.) != ""))

# Next remove leading and trailing spaces from all elements in character columns
trim.f <- function(col, na.rm = F) { 
  isNA <- !reduce(col, ~ (is.na(.x) & is.na(.y)))
  if (na.rm && isNA) 
    unlist(map(col, ~ (if (is.na(.x)) "" else .x) ),use.names = F)
  else trimws(col, which = c("both")) # leading and trailing spaces
}

trim.spaces <- function(df)
  df %>%
    mutate_if(is.character, trim.f, na.rm = T)

# Finally convert character columns to factor

char.to.fctor <- function(df)
  df %>%
    mutate_if(is.character, ~ factor(.x, levels = (.x %>% table() %>% names())))

# Since it is important to have the same columns in train and test first, compute the encoding:
encoding <- build_encoding(dataSet = train.df, cols = "auto", verbose = TRUE)

train.df <- 
  train.df %>%
  na.omit() %>%
  trim.spaces() %>%
  remove.empty.characters() %>%
  char.to.fctor()

# In case your model does not do automatic hot encoding (like xgboost):
encoding <- build_encoding(dataSet = train.df, cols = "auto", verbose = TRUE)

# remove constant variables
(constant_cols <- whichAreConstant(train.df))

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(train.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(train.df))

# discretize year (convert them into intervals)
bins <- build_bins(dataSet = train.df, cols = "year", n_bins = 10, type = "equal_freq")
print(bins)
(train.df <- fastDiscretization(dataSet = train.df,
                                bins = list(year = c(71, 73, 74, 75, 76, 77, 79, 80, 81))))
table(train.df$cylinders)

# Encode the train and test with same encoding object 
train.df <- one_hot_encoder(dataSet = train.df, encoding = encoding, drop = TRUE, verbose = TRUE)

test.df <- 
  test.df %>%
  na.omit() %>%
  trim.spaces() %>%
  remove.empty.characters() %>%
  char.to.fctor()

# remove constant variables
(constant_cols <- whichAreConstant(test.df))

# remove Variables that are in double (for example col1 == col2)
(double_cols <- whichAreInDouble(test.df))

# remove Variables that are exact bijections (for example col1 = A, B, B, A and col2 = 1, 2, 2, 1)
(bijections_cols <- whichAreBijection(test.df))

# discretize year (convert them into intervals)
bins <- build_bins(dataSet = test.df, cols = "year", n_bins = 10, type = "equal_freq")
print(bins)
(test.df <- fastDiscretization(dataSet = test.df,
                                bins = list(year = c(71, 73, 74, 75, 76, 77, 79, 80, 81))))
table(test.df$cylinders)

# Encode the train and test with same encoding object 
test.df <- one_hot_encoder(dataSet = test.df, encoding = encoding, drop = TRUE, verbose = TRUE)

# Let’s check the dimensions:
sprintf("dimension of train data: %d" , dim(train.df))
sprintf("dimension of test data: %d" , dim(test.df))

# Since a lot of columns have been created, a filtering could be relevant:
# we drop columns the this function reports.
bijections <- whichAreBijection(dataSet = train.df, verbose = TRUE)
bijections <- whichAreBijection(dataSet = test.df, verbose = TRUE)

# Make sure that train and test sets have the same shape:
X_test <- sameShape(test.df, referenceSet = test.df, verbose = TRUE)
```
```{r Dealing with missing values, imputate}
suppressWarnings(suppressMessages(library(tidyverse)))
library(tidyverse)

# First let's build some data with NA
df <- tibble(x = c(rnorm(1000), c(NA, 13.34,2673.1,4674,NA,4585.2,78.678,NA,2754,
                   NA, 16.34,2473.1,4854,NA,45.2,7468.8,NA,2534,
                   NA, 156.34,23.188,6744,NA,4576.2,748.478,NA,4724)), 
             y = c(as.character(rnorm(1000)) ,
                   c("a1df",NA,"t4grf", "sv7",NA, " NA", "13wdf", "e45rgg", "r546gbn",
                   "ad2f","NA"," NA", "s8v","NA", "wd11fwf", "wd46f", "er57gg", "rgb67n",
                   "adf3",NA,"tgrf6", "9sv",NA, "wdfw13f", "NA ", "ergg68", "rgb85n")))


print(object.size(df), units="KB")


# ------------------ select observation with missing values ------------------

na.idx <- 
  df %>%
  pmap(~c(...)) %>%
  map(~sum(is.na(.) | str_trim(.) == "NA") > 0) %>%
  unlist %>%
  which()

rest.idx <- setdiff(seq_len(nrow(df)) , na.idx)

df[na.idx, ]
df[rest.idx, ]

  

# ------------------ ratio of missing values ------------------
ratio.missing <- length(na.idx) / nrow(df)
print(glue::glue(round(ratio.missing * 100, 3), "%"))



# ------------------ which features are missing most often ------------------
df %>%
  # Gather columns into key-value pairs
  gather(col.name, col.value, x:y)%>%
  group_by(col.name) %>%
  summarise(NA.count = sum(is.na(col.value)))

# another way to do this:
df %>%
  pmap(~c(...)) %>%
  reduce(~.x + is.na(.y), .init = 0)

# old fashion way : 
apply(df, 2, function(x) length(which(is.na(x))))



# -------------- multiple features missing in one observation ------------------
df %>%
  pmap(~c(...) %>%
         reduce(~.x + is.na(.y) , .init = 0))
         
# old fashio way:
apply(df, 1, function(x) length(which(is.na(x))))




# ----------------- Use imputation to predict values for NAs ------------------
# Use predictive models using the known features in order to estimates the missing features.
# When imputing values, it is important that the test data are not used 
library(Hmisc)

# use aregImpute for multiple imputation
imputed <- aregImpute(y ~ x, df, group=df$y , pr = FALSE)

# Note that aregImpute makes several multiple imputations with different bootstrap samples, 
# which can be specified using the n.impute parameter. Since we want to use the imputations 
# from all runs rather than a single one, we will use the fit.mult.impute function to define our model:


# list.out: return a list; imputation: arbitrarily use the first imputation
imputed.data <- as.data.frame(impute.transcan(imputed, data = df,
                imputation = 1, list.out = TRUE,  pr = FALSE))

# compute new weights
weights <- get.weights(imputed.data)

# use imputations from all iterations rather than an arbitrary iteration:
fmi <- fit.mult.impute(y ~ x,
        fitter=glm, xtrans = imputed, family = "poisson", 
        data = df, subset = trainset, pr = FALSE) #, weights = weights)
        # weights arguments (weights = weights) not used due to error:
        # "..2 used in an incorrect context, no ... to look in"
fmi.preds <- predict.glm(fmi, newdata = df[testset,], type = "response") 
plot.linear.model(fmi, fmi.preds, df$y[testset])

# inspect imputed observations with respect to the outcome 
summary(as.numeric(imputed.data$df))


```
```{r}
# -------------  density: computes kernel density estimates. -------------
require(graphics)

plot(density(c(-20, rep(0,98), 20)), xlim = c(-4, 4))  # IQR = 0

# The Old Faithful geyser data
d <- density(faithful$eruptions, bw = "sj")

plot(d)

plot(d, type = "n")
polygon(d, col = "wheat")
```


```{r regression model analysis and diagnostics}
library(tidyverse)
library(graphics)

# make up some data
# ------------- 
carseats.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Carseats.csv", 
                      header=T, stringsAsFactors = T, na.strings = "?")
str(carseats.df)

carseats.df <- subset(na.omit(carseats.df), 
        select = c("Sales", "CompPrice", "Income", "Price"))

str(carseats.df)
set.seed(1113)

plot(carseats.df)

(no.of.train <- ceiling(0.7 * nrow(carseats.df)))
(no.of.test <- nrow(carseats.df) - no.of.train)

train.idx <- sample(seq_len(nrow(carseats.df)), no.of.train)
test.idx <- setdiff(seq_len(nrow(carseats.df)), train.idx)

# build ordinary least square model
model <- lm(Sales ~ CompPrice + Income + +Price ,
            data = carseats.df , subset = train.idx)

# first thing to do is to get confedence interval for each coefficient
# The wider confedence interval means model is more hesitate about the coefficient
confint(model)

# point to note: confedence interval for Income is too wide comparing with othres
# See model's performance

pred.values <- predict(model, newdata = carseats.df[test.idx, ])
obs.values <- carseats.df[test.idx, ]$Sales

# find the R^2 for test data
(R.squared <- round((cor(pred.values, obs.values) ^ 2), 3))

# R squared is 0.411 which is not great!
# lets plot residuals:

plot.data <- data.frame("Pred.values" = pred.values, "obs.values" = obs.values, 
                                "DataSet" = "test")
residuals <- obs.values - pred.values

plot.residuals <- data.frame("x" = obs.values, "y" = pred.values, 
                "x1" = obs.values, "y2" = pred.values + residuals,
                "DataSet" = "test")

ggplot() + 
        # plot test samples
        geom_point(data = plot.data, 
            aes(x = obs.values, y = pred.values, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.residuals, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "blue", slope = 1)

# From the plot we see that:
#  Salaries <= 4 are over estimated
#  Salaries >= 15 are under estimated

#let's see wht similar plot showson traing data

pred.values.train <- model$fitted.values
obs.values.train <- model$model[,1]

plot.data.train <- data.frame("Pred.values" = pred.values.train, "obs.values" = obs.values.train, 
                                "DataSet" = "train")
residuals.train <- obs.values.train - pred.values.train

plot.residuals.train <- data.frame("x" = obs.values.train, "y" = pred.values.train, 
                "x1" = obs.values.train, "y2" = pred.values.train + residuals.train,
                "DataSet" = "train")

ggplot() + 
        # plot test samples
        geom_point(data = plot.data.train, 
            aes(x = obs.values.train, y = pred.values.train, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.residuals.train, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "blue", slope = 1)

# looking at the above obcerved vs. predicted values in training data shows
# majority of obdevrd data has salary between 4 to 12 thus in this range 
# standard error is lower.

# let's draw the residuals pdf:
plot(density(model$residuals))

# Residuals are not normally distributed , so linear model is not the best model.
# let's look at diagnodtics plot to see the outliers and high leverage points
plot(model)

# Let's have look at the portions of full data that under estimations and over estimation occured

# observed data that is a border between salaries < 4.xxx and salaries > 4.xxx
(left.cut <- quantile(carseats.df$Sales , 0.15))

# observed data that is a border between salaries > 12.xxx and salaries < 10.xxx
(right.cut <- quantile(carseats.df$Sales , 0.95))

# observed dat with salaries < 4.xxx 
left.side.data.idx <- which(carseats.df$Sales < left.cut)

# summary of data in under estimated region
summary(carseats.df[left.side.data.idx, ])

# observed dat with salaries > 10.xxx 
right.side.data.idx <- which(carseats.df$Sales > right.cut)

# summary of data in over estimated region
summary(carseats.df[right.side.data.idx, ])

# summary of whole data
summary(carseats.df)

# comparing the mean of corresponding predictors of low (high) sale observations
# with other observations shows they are not large difference between
# means of predictors of low (high) sale observations and other observations
# So this could not be the cause of under and over estimation.
# looking at the plots shows majaority of records have Sales value 
# larger than 4 which makes model to choose intercept around 3.9


# ---------------- Dealing with under and over estimation -----------------

# To reduce under estimate and over estimated values let's 
# use GLM to be able to plugin Poisson pdf, we chose poisson 
# because plot of residuals showd it looks more poisson than Normal:

# GLM with Poisson does not use ordinary least square to fit the data
pois.model <- glm(Sales ~ CompPrice + Income + Price ,
            family = "poisson" ,data = carseats.df , subset = train.idx)

# first thing to do is to get confedence interval for each coefficient
# The wider confedence interval means model is more hesitate about coefficient
confint(pois.model)

# point to note: confedence interval of GLM for Comprice is too wide comparing with othres
# See model's performance
# need to set type to 'response' to get exponentiated predictions 
pred.values <- predict(pois.model, newdata = carseats.df[test.idx, ], 
                      type = "response") 
obs.values <- carseats.df[test.idx, ]$Sales

# find the R^2 for test data
(R.squared <- round((cor(pred.values, obs.values) ^ 2), 3))

# R squared is 0.426 which is slightly improved
# lets plot residuals:

plot.data <- data.frame("Pred.values" = pred.values, "obs.values" = obs.values, 
                                "DataSet" = "test")
residuals <- obs.values - pred.values

plot.residuals <- data.frame("x" = obs.values, "y" = pred.values, 
                "x1" = obs.values, "y2" = pred.values + residuals,
                "DataSet" = "test")

ggplot() + 
        # plot test samples
        geom_point(data = plot.data, 
            aes(x = obs.values, y = pred.values, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.residuals, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "blue", slope = 1)

# ------------------------------------------------------------------------------
# -------- Adjust the loss function by increasing impact of residulas ----------
# ------------------------------------------------------------------------------
# To rectify underestimation of Sales we adjust the loss function:
# We increase the impact of the residuals of the outliers by calculating 
# the z-scores of the sales and then use their exponential as the weight 
# for the regression model

sales.z.scores <- (carseats.df$Sales - mean(carseats.df$Sales)) / sd(carseats.df$Sales)
weights.exp <- 1/exp(sales.z.scores)^2
sales.weights <- weights.exp / mean(weights.exp) # normalize to mean 1

# Now build a new GLM model with weights
pois.model.with.weights  <- glm(Sales ~ CompPrice + Income + Price ,
            family = "poisson" ,data = carseats.df , subset = train.idx, 
            weights = sales.weights)

#Let's see the p-values
coef(summary(neg.bin.model.with.weights))
# p-values shows all predictors are statistically significant


# get confedence interval for each coefficient
confint(pois.model.with.weights)

# Clearly cofedence intervals of the predictor looks better now : No wide range is seen

# See model's performance
# need to set type to 'response' to get exponentiated predictions 
pred.values <- predict(pois.model.with.weights, newdata = carseats.df[test.idx, ], 
                      type = "response")
obs.values <- carseats.df[test.idx, ]$Sales

# find the R^2 for test data
(R.squared <- round((cor(pred.values, obs.values) ^ 2), 3))

# R squared is 0.425 which is back to what is was
# lets plot residuals:

plot.data <- data.frame("Pred.values" = pred.values, "obs.values" = obs.values, 
                                "DataSet" = "test")
residuals <- obs.values - pred.values

plot.residuals <- data.frame("x" = obs.values, "y" = pred.values, 
                "x1" = obs.values, "y2" = pred.values + residuals,
                "DataSet" = "test")

ggplot() + 
        # plot test samples
        geom_point(data = plot.data, 
            aes(x = obs.values, y = pred.values, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.residuals, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "blue", slope = 1)

# Remember we assumed that the mean of DV should be equal to its variance 
# to be able to use Poisson regression
# let's see ho is this true

sales.mean <- round(mean(carseats.df$Sales) ,3)
sales.var <- round(var(carseats.df$Sales),3)
print(glue::glue("mean of Sales: ",sales.mean, 
                 "\n variance of Sales: ",  sales.var))

# Variance is greater than the mean we have slight over dispersion
# we should try picking a model that is more suitable for overdispersion such as
# negative binomial

library(MASS)

neg.bin.model.with.weights  <- glm.nb(Sales ~ CompPrice + Income + Price ,
                                   data = carseats.df , subset = train.idx, 
                                   weights = sales.weights)

pred.values <- predict(neg.bin.model.with.weights, newdata = carseats.df[test.idx, ], 
                      type = "response")
obs.values <- carseats.df[test.idx, ]$Sales

#Let's see the p-values
coef(summary(neg.bin.model.with.weights))
# p-values shows all predictors are statistically significant

# get confedence interval for each coefficient
confint(pois.model.with.weights)


# See how model performs
# need to set type to 'response' to get exponentiated predictions 
pred.values <- predict(pois.model.with.weights, newdata = carseats.df[test.idx, ], 
                      type = "response")
obs.values <- carseats.df[test.idx, ]$Sales

# find the R^2 for test data
(R.squared <- round((cor(pred.values, obs.values) ^ 2), 3))

# R squared is 0.425 which is back to what is was
# lets plot residuals:

plot.data <- data.frame("Pred.values" = pred.values, "obs.values" = obs.values, 
                                "DataSet" = "test")
residuals <- obs.values - pred.values

plot.residuals <- data.frame("x" = obs.values, "y" = pred.values, 
                "x1" = obs.values, "y2" = pred.values + residuals,
                "DataSet" = "test")

ggplot() + 
        # plot test samples
        geom_point(data = plot.data, 
            aes(x = obs.values, y = pred.values, color = DataSet)) +
        # plot residuals
        geom_segment(data = plot.residuals, alpha = 0.2,
            aes(x = x, y = y, xend = x1, yend = y2, group = DataSet)) +
        # plot optimal regressor
        geom_abline(color = "blue", slope = 1)


# confidence bands for a negative binomial can be found in the following way:

# predict on the link level, which is more Gaussian:
preds.nb.ci <- predict(neg.bin.model.with.weights, 
                       newdata = carseats.df[test.idx, ], 
                       type = "link", se.fit = TRUE)

# compute confidence intervals
ilink <- family(neg.bin.model.with.weights)$linkinv # exponential

## parameters of the negative binomial
theta <- neg.bin.model.with.weights$theta # 'r' parameter of negative binomial (nbr of successes)

mu <- preds.nb.ci$fit # mean of negative binomial varies per fitted point

p <- theta / (theta + mu) # probability of success parametrization of negative binomial


# find 95% CI for normal distribution
# (we're not taking these values from the negative binomial because SEs are for normal distribution)

# ci.factor <- qnbinom(1 - (1 - CI.int)/2, size = theta, 
#                      mu = as.numeric(preds.nb.ci$fit)) # different factors per observation

ci.factor <- 1.96
CI.int <- 0.95

# calculate CIs:
df <- data.frame(carseats.df[test.idx, ], 
                 "PredictedSalary" = ilink(preds.nb.ci$fit), 
                 "Lower" = ilink(preds.nb.ci$fit - ci.factor * preds.nb.ci$se.fit),
                "Upper" = ilink(preds.nb.ci$fit + ci.factor * preds.nb.ci$se.fit))

df
# Using the constructed data frame containing the values of the features in the test set as well as the predictions with their confidence bands, we can plot how the estimates fluctuate in dependence on the independent variables:

# plot estimates vs individual features in the test set
plots <- list()
for (feature in colnames(carseats.df)) {
    if (feature == "Sales") {
        next
    }
    p <- ggplot(df, aes_string(x = feature, 
                       y = "PredictedSalary")) + 
      geom_line(colour = "blue") + 
      geom_point() + 
      geom_ribbon(aes(ymin = Lower, ymax = Upper), 
                    alpha = 0.5) 
      plots[[feature]] <- p
}
library(gridExtra)
grid.arrange(plots[[1]], plots[[2]], plots[[3]])

# These graphs shows high confedence when predicted salary is between 10 and 30
```


```{r function plot}

fun1 <- function(teta) {1 + sin(teta)}
p <- ggplot(tibble(teta = 0:2*pi), aes(x = teta)) + stat_function(fun = fun1)
p


fun2 <- function(teta){cos(2*teta)}
df <- tibble(teta=0:2*pi)
ggplot(df, aes(x = teta)) + stat_function(fun = fun2)
#----------- polar coordinate if you know how to transform polar equation into cartesian---------------

df <- tibble(r = seq(0, 2, length = 50), teta = seq(0, 2*pi, length = 50))
transformed <- transform(df,
  x = r * sin(teta),
  y = r * cos(teta)
)

ggplot(transformed, aes(x, y)) + 
  geom_path() + 
  geom_point(size = 2, colour = "red") + 
  coord_fixed()

```

* References :
    + Advanced R second edition by Hadley Wickham
    + http://www.rebeccabarter.com/blog/2019-08-19_purrr/
    + https://suzanbaert.netlify.app/2018/01/dplyr-tutorial-1/
    + https://suzanbaert.netlify.app/2018/01/dplyr-tutorial-2/
    + https://suzanbaert.netlify.app/2018/01/dplyr-tutorial-3/
    + https://suzanbaert.netlify.app/2018/01/dplyr-tutorial-4/
    + https://cran.r-project.org/web/packages/dataPreparation/vignettes/train_test_prep.html
    + https://www.datascienceblog.net/post/machine-learning/improving_ozone_prediction/
