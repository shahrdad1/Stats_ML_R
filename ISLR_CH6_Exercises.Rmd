---
title: "ISLR CH6 Exercises"
output:
  pdf_document: default
  html_notebook: default
---


```{r Lab 651 Best subset selection}
library(tidyverse)
library(leaps)
hitters.df = read.csv(
  "/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Hitters.csv", 
  header=T, stringsAsFactors = F, na.strings = "?")

# note that Salary is od type string and some of them are NA
sum(hitters.df$Salary=="NA")

# first remove character NAs
hitters.df <- hitters.df[hitters.df$Salary != "NA",]

# now convert Salary into numeric
hitters.df$Salary <- as.numeric(as.character(hitters.df$Salary))
  
# Now convert it to tibble
hitters.df <- tibble(hitters.df)

# regsubsets() part of lepas library chooses best subset using RSS


regfit.full <- regsubsets(Salary ~ ., hitters.df, nvmax = 19) 

#The summary shows the result of step 2 of algorithm 6.1 page 205 of the book 
summary <- summary(regfit.full)

names(summary)
summary

# coef(, n) returns coefficient estimates associated with best n variable model
coef(regfit.full,4)

# plot Rsq , Cp and BIC 
# par(mfrow=c(1,1))
plot(summary$rss, xlab = "Number of variables", ylab="RSS",type = "l")
plot(summary$adjr2,xlab = "Number of variables", ylab="Adjusted Rsquared",
     type = "l")

# which.max() returns location maximum point of the vector
(index <- which.max(summary$adjr2))
points(index, summary$adjr2[index], col="red", cex=2, pch=20)

# which.min() returns location minimum point of the vector
plot(summary$cp, xlab =" Numbers of variables", ylab="Cp", type="l")
(index <- which.min(summary$cp))
points(index, summary$cp[index], col="red", cex=2, pch=20)

# same for bic
plot(summary$bic, xlab =" Numbers of variables", ylab="Bic", type="l")
(index <- which.min(summary$bic))
points(index, summary$bic[index], col="red", cex=2, pch=20)

# regsubsets() has builtin plot() command that displays selected variables for 
# best model with a given number of predictors

plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")


```


```{r Lab 652 Forward and backward stepwise selection}
library(tidyverse)
library(leaps)
hitters.df = read.csv(
  "/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Hitters.csv", 
  header=T, stringsAsFactors = F, na.strings = "?")

# note that Salary is od type string and some of them are NA
sum(hitters.df$Salary=="NA")

# first remove character NAs
hitters.df <- hitters.df[hitters.df$Salary != "NA",]

# now convert Salary into numeric
hitters.df$Salary <- as.numeric(as.character(hitters.df$Salary))
  
# Now convert it to tibble
hitters.df <- tibble(hitters.df)

# we can use regsubsets() to perform forward / backward stepwise selection

regfit.fwd <- regsubsets(Salary ~ ., data = hitters.df, nvmax=ncol(hitters.df), 
                         method = "forward")

summary(regfit.fwd)

# coefficient for the best model with 3 coefficients
(coefs <- coef(regfit.fwd,3))
names(coefs)
```
```{r 653 use CV to choose one of set of models chosen in step 2 of the algorithms ) }
library(tidyverse)
library(leaps)
hitters.df = read.csv(
  "/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Hitters.csv", 
  header=T, stringsAsFactors = F, na.strings = "?")

# note that Salary is od type string and some of them are NA
sum(hitters.df$Salary=="NA")

# first remove character NAs
hitters.df <- hitters.df[hitters.df$Salary != "NA",]

# now convert Salary into numeric
hitters.df$Salary <- as.numeric(as.character(hitters.df$Salary))
  
# Now convert it to tibble
hitters.df <- tibble(hitters.df)
# first we create a vector that allocates each observation to one of K = 10 folds
k = 10
set.seed(1)

# create k folds
folds <- sample(1:k, nrow(hitters.df), replace = T)
# table(folds)
#  1  2  3  4  5  6  7  8  9 10 
# 35 25 33 31 34 31 32 29 39 33 

# number of features
noOfFeatures <- ncol(hitters.df) -1

# an empty accumulator to store MSE for each fold and each predictor
cv.errors <- matrix(NA, k, noOfFeatures, 
                    dimnames = list(NULL, paste(1:noOfFeatures)))

# Here is cv.errors:
# ====================
#       1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19
#  [1,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
#  [2,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
#  [3,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
#  [4,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
#  [5,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
#  [6,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
#  [7,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
#  [8,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
#  [9,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA
# [10,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA

# perform a cross validation on a for loop
for (j in 1:k){
  # step# 2 of algorithm is evaluated on all folds except one of them each time
  # it chooses best models with number of features 1,2,..., noOfFeatures 
  # on k-1 training folds
  best.fit <- regsubsets(Salary ~ ., data = hitters.df[folds != j, ], 
                         nvmax = noOfFeatures)

  # Now build X matrix from test data
  test.mat <- model.matrix(Salary ~ ., data = hitters.df[folds == j, ])
    
#model.matrix:
# ------------    
#(Intercept) AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns ...
#         1   202   53     4   31  26    27     9   1876   467     15   192  ...
#         1   239   60     0   30  11    22     6   1941   510      4   309  ...
#         1   472  116    16   60  62    74     6   1924   489     67   242  ...
  
  # now compute CV test error for each of  models that have best number of 
  # predictors on test fold # j 
  for(i in 1:noOfFeatures){
    # extract coefficients for model # i
    coefi <- coef(best.fit, id = i)
    
    # claculate cv test error for each row in test matrix()
    predicted_values <- test.mat [, names(coefi)] %*% coefi
    cv.errors[j,i] <- mean((predicted_values - hitters.df[folds ==j, ]$Salary)^2)
  }
}

# finally calculate mean of CV MSE error for each model
cv.error.means <- rep(NA, ncol(cv.errors))
for (l in 1:ncol(cv.errors)){
  cv.error.means[l] <- mean(cv.errors[ ,l])
}

# find the minimum of all cv-MSE means and corresponding coefficients
print("minimum of all cv-MSE means and corresponding coefficients: ")
which.min(cv.error.means)

# seems like model with following 10 predictors has least MSE error on test data
names(coef(best.fit, id = which.min(cv.error.means)))

```

```{r example of backward feature selection for LDA }
library(tidyverse)
library(class)
library(boot)
library(leaps)

set.seed(17)
weekly.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Weekly.csv", 
                      header=T, stringsAsFactors = F, na.strings = "?")
weekly.df = tibble(weekly.df)

#-------- Some usual cleaning on character columns ---------------- #

# First remove all recods with spaces in character column Direction
weekly.df$Direction <- gsub('\\s+', '', weekly.df$Direction)

# Second remove all leading and trailing spaces from a character column "Direction"
weekly.df$Direction <- trimws(weekly.df$Direction, which = c("both"))

# Remove all records with "NA" or empty string in character column "Direction"
weekly.df <- weekly.df[!(tolower(weekly.df$Direction) == "na" | 
                           weekly.df$Direction == ""), ]

# convert all character fields 
weekly.df[sapply(weekly.df, is.character)] <- 
  lapply(weekly.df[sapply(weekly.df, is.character)], as.factor)

#----------- Find and remove NA in all columns ------------- #
weekly.df <- na.omit(weekly.df)

# ---------- stepwise forward feature selectin with CV -----------#

# create k-fold 
k <- 10
threshold <- 0.5

set.seed(1)

# create k folds
folds <- sample(1:k, nrow(weekly.df), replace = T)

# For folds with same size do:
# sameSizefolds <- sample(rep(1:k, length.out = nrow(weekly.df)), 
#                                        size = nrow(weekly.df), replace = F)
# table(sameSizefolds)


# number of features
noOfFeatures <- ncol(weekly.df) -1

# an empty accumulator to store MSE for each fold and each predictor
cv.errors <- matrix(NA, k, noOfFeatures, 
                    dimnames = list(NULL, paste(1:noOfFeatures)))


# perform a cross validation on a for loop
for (j in 1:k){
  # step# 2 of algorithm 6.3 page 209 is evaluated on all folds except one of 
  # them each time it chooses best models with number of features 
  # 1,2,..., noOfFeatures on k-1 training folds.
  best.fit <- regsubsets(Direction ~ ., data = weekly.df[folds != j, ], 
                         nvmax = noOfFeatures, method = "backward")


  # Compute CV test error for each of  models that have best number of 
  # predictors on test fold # j 
  for(i in 1:noOfFeatures){
    
    # extract coefficients for model # i
    coefi <- coef(best.fit, id = i)

    # For classification we are interested in name 
    # of features for model # i (not their coefficients)
    # except intercept
    predictorsOfModel <- names(coefi)[-1]
    # fit the model on k-1 trainimng portion
    lda.fit <- MASS::lda(as.formula(paste("Direction~", paste(predictorsOfModel, collapse="+"))), 
                   data = weekly.df, family = binomial, subset = (folds != j))

    # predict on single validation fold
    lda.pred <- predict(lda.fit, weekly.df[folds == j, ], type =  "response")
    # since contrasts(weekly.df$Direction) shows dummy variable 1 asigned to 'Up'
    # and since P(y=1|x) is glm.probs what we get is prosterior of probability of 'Up' case
    stopifnot(length (lda.pred$class) == length(weekly.df[folds == j, ]$Direction))
    cv.errors[j,i] <- mean(lda.pred$class == weekly.df[folds == j, ]$Direction)
  }
}

# finally calculate mean of CV MSE error for each model
cv.error.means <- rep(NA, ncol(cv.errors))
for (i in 1:ncol(cv.errors)){
  cv.error.means[i] <- mean(cv.errors[, i])
}

# find the minimum of all cv-MSE means and corresponding coefficients
print("minimum of all cv-MSE means and corresponding coefficients: ")
which.min(cv.error.means)

# seems like model with following 5 predictors has least MSE error on test data
names(coef(best.fit, id = which.min(cv.error.means) ))

```



```{r Ridge regression }
library(tidyverse)
library(glmnet)

hitters.df = read.csv(
  "/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Hitters.csv", 
  header=T, stringsAsFactors = F, na.strings = "?")

# -------------- Usual clean up first ------------------#

# First remove all recods with spaces in character column Salary
hitters.df$Salary <- gsub('\\s+', '', hitters.df$Salary)

# Second remove all leading and trailing spaces from a character column "Salary"
hitters.df$Salary <- trimws(hitters.df$Salary, which = c("both"))

# Remove all records with "NA" or empty string in character column "Salary"
hitters.df <- hitters.df[!(tolower(hitters.df$Salary) == "na" | 
                           hitters.df$Salary == ""), ]

# convert Salary column to numberic
hitters.df$Salary <- as.numeric(as.character(hitters.df$Salary))

# convert all character fields 
hitters.df[sapply(hitters.df, is.character)] <- 
  lapply(hitters.df[sapply(hitters.df, is.character)], as.factor)

# Find and remove NA in all columns 
hitters.df <- na.omit(hitters.df)

# Use glmnet () for Ridge (glmnet only take numarical values) 
# glmnet automatically standardize predictors unless we set standardize = F

# first create a matrix of all predictors
# model.matrix automatically transforms any qualitative variable to factor

x <- model.matrix(Salary~., hitters.df)[, -1]
y <- hitters.df$Salary

# apply Ridge
grid <- 10 ^ seq(10, -2, length = 100)
# alpha = 0 causes Ridge to be applied
ridge.model <- glmnet(x, y, alpha=0, lambda = grid)

# there are 100 values for λ and associated to each we have 
# number of ncol(hitters.df) coefficients

ridge.model$lambda[50]

# get the coefficients corresponding to 50th λ:
rownames(coef(ridge.model))
coef(ridge.model)[,50]

# foe some reason there are two intercept coefficients at the begining
# we drop first one to calculate L2 norm of the coefficints
sqrt(sum(coef(ridge.model)[-1,50]^2))

# we use predict to get a new value for coefficients for any given value of λ
predict(ridge.model, s = 51, type="coefficients")[1:20]

# now split the samples into test and training:
set.seed(10)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]

# Fit ridge regression model to trainig data
ridge.model <- glmnet(x[train, ], y[train], alpha=0, 
                      lambda = ( 10 ^ seq(10, -2, length = 100)),
                      thresh = 1e-12)

# Evaluate MSE of the model on on the test set for λ = 4
# to do the prediction we set newx argument to test set
ridge.predict <- predict(ridge.model, s=1000, newx = x[test,])

print ("now find the MSE corresponding to λ = 4")
mean((ridge.predict-y.test)^2)

print ("just for comarison we use intercept to predict and calculate the MSE
       (λ is set to a very large value)")
ridge.predict <- predict(ridge.model, s=1e+10, newx = x[test,])

mean((ridge.predict-y.test)^2)

print ("now compare Ridge with usuall regression (i.e when λ = 0)")
# we set exact to T otherwise predict() function will 
# interpolate over the grid of λ values
ridge.predict <- predict(ridge.model, s=0, newx = x[test,],
                         exact = T, x = x[train,], y=y[train])

# calculate MSE
mean((ridge.predict - y.test)^2)

# if we want to fit a (unpenalized) least squares model, then we should use the
# lm() function, since that function provides more useful outputs, such as 
# standard errors and p-values for the coefficients.


```


```{r Ridge regression and finding best value of λ using CV}

library(tidyverse)
library(glmnet)

hitters.df = read.csv(
  "/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Hitters.csv", 
  header=T, stringsAsFactors = F, na.strings = "?")

# -------------- Usual clean up first ------------------#

# First remove all recods with spaces in character column Salary
hitters.df$Salary <- gsub('\\s+', '', hitters.df$Salary)

# Second remove all leading and trailing spaces from a character column "Salary"
hitters.df$Salary <- trimws(hitters.df$Salary, which = c("both"))

# Remove all records with "NA" or empty string in character column "Salary"
hitters.df <- hitters.df[!(tolower(hitters.df$Salary) == "na" | 
                           hitters.df$Salary == ""), ]

# convert Salary column to numeric
hitters.df$Salary <- as.numeric(as.character(hitters.df$Salary))

# convert all character fields to factor
hitters.df[sapply(hitters.df, is.character)] <- 
  lapply(hitters.df[sapply(hitters.df, is.character)], as.factor)

# Find and remove NA in all columns 
hitters.df <- na.omit(hitters.df)


x <- model.matrix(Salary~., hitters.df)[, -1] # -1 is to drop the Intercept
y <- hitters.df$Salary

set.seed(10)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)

# Built-in cross-validation function, cv.glmnet(). 
# By default, the function performs ten-fold cross-validation, 
# though this can be changed using the argument nfolds.
# we aaply ot on training portion of the data to find the λ
# then we run the final model with the lamda on test data to get MSE

cv.out=cv.glmnet(x[train ,],y[train],alpha=0)
plot(cv.out)
(bestlam=cv.out$lambda.min)

# What is the test MSE associated with this value of λ?

# Fit ridge regression model to trainig data
ridge.model <- glmnet(x[train, ], y[train], alpha=0, 
                      lambda = ( 10 ^ seq(10, -2, length = 100)),
                      thresh = 1e-12)

ridge.pred=predict(ridge.model,s=bestlam ,newx=x[test,])
mean((ridge.pred-y[test])^2)


# now let's split the data into train / test and train 
# the model on train data , get the best lambda and then run it on test portion 
# with the best lambda we got to see the testMSE:

cv.out <- cv.glmnet(x[train, ], y[train], alpha=0)
plot(cv.out)
(best.lambda <- cv.out$lambda.min)

ridge.pred <- predict(cv.out, s=bestlam, newx = x[test, ])
sprintf("mean error on test data using %s is %s", bestlam, mean((ridge.pred - y[test])^ 2))
mean((ridge.pred - y[test])^ 2)


# finally we refit the model on the whole data and use the best λ calculated 
# in CV
cv.out=cv.glmnet(x,y,alpha=0)
plot(cv.out)
predict(cv.out, type="coefficients" ,s=bestlam )

```


```{r Ridge for logistic regression using CV to find best λ }

library(tidyverse)
library(glmnet)

weekly.df = 
  read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Weekly.csv", 
                      header=T, stringsAsFactors = F, na.strings = "?")

str(weekly.df)

# -------------- Usual clean up first ------------------#

# First remove all recods with spaces in character column Direction
weekly.df$Direction <- gsub('\\s+', '', weekly.df$Direction)

# Second remove all leading and trailing spaces from a character column "Direction"
weekly.df$Direction <- trimws(weekly.df$Direction, which = c("both"))

# Remove all records with "NA" or empty string in character column "Direction"
weekly.df <- weekly.df[!(tolower(weekly.df$Direction) == "na" | 
                           weekly.df$Direction == ""), ]

# convert all character fields to factor
weekly.df[sapply(weekly.df, is.character)] <-
 lapply(weekly.df[sapply(weekly.df, is.character)], as.factor)

# Find and remove NA in all columns 
weekly.df <- na.omit(weekly.df)

str(weekly.df)

# contrasts(weekly.df$Direction)

# now we use cross validation to find the best λ and corresponding coeffs

# First construct matrix from dataframe (and drop intercept column)
x <- model.matrix(Direction~., weekly.df)[,-1] 
y <- ifelse(weekly.df$Direction == "Up", 1, 0)

set.seed(10)

# We’ll use the R function glmnet() [glmnet package] for computing penalized logistic regression.

# Built-in cross-validation function, cv.glmnet(). 
# By default, the function performs ten-fold cross-validation, 
# though this can be changed using the argument nfolds.

cv.out=cv.glmnet(x, y, family = "binomial", alpha=0, lambda = NULL)
plot(cv.out)
(bestlam=cv.out$lambda.min)

# get the coefficients:
predict(cv.out, type="coefficients" ,s=bestlam )

print("Here is value of λ for which the MSE is minimum")
cv.out$lambda.min

print("Here is one standard error value of λ for which the MSE is minimum")
cv.out$lambda.1se

```

```{r Lasso for logistic regression using CV to find best λ }

library(tidyverse)
library(glmnet)

weekly.df = 
  read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Weekly.csv", 
                      header=T, stringsAsFactors = F, na.strings = "?")

str(weekly.df)

# -------------- Usual clean up first ------------------#

# First remove all recods with spaces in character column Direction
weekly.df$Direction <- gsub('\\s+', '', weekly.df$Direction)

# Second remove all leading and trailing spaces from a character column "Direction"
weekly.df$Direction <- trimws(weekly.df$Direction, which = c("both"))

# Remove all records with "NA" or empty string in character column "Direction"
weekly.df <- weekly.df[!(tolower(weekly.df$Direction) == "na" | 
                           weekly.df$Direction == ""), ]

# convert all character fields to factor
weekly.df[sapply(weekly.df, is.character)] <-
 lapply(weekly.df[sapply(weekly.df, is.character)], as.factor)

# Find and remove NA in all columns 
weekly.df <- na.omit(weekly.df)

str(weekly.df)

# contrasts(weekly.df$Direction)

# now we use cross validation to find the best λ and corresponding coeffs

# First construct matrix from dataframe (and drop intercept column)
x <- model.matrix(Direction~., weekly.df)[,-1] 
y <- ifelse(weekly.df$Direction == "Up", 1, 0)

set.seed(10)

# We’ll use the R function glmnet() [glmnet package] 
# for computing penalized logistic regression.

# Built-in cross-validation function, cv.glmnet(). 
# By default, the function performs ten-fold cross-validation, 
# though this can be changed using the argument nfolds.

cv.out=cv.glmnet(x, y, family = "binomial", alpha=1, lambda = NULL)
plot(cv.out)
(bestlam=cv.out$lambda.min)

# gget the coefficients:
predict(cv.out, type="coefficients" ,s=bestlam )

names(cv.out)

print("Here is value of λ for which the MSE is minimum")
cv.out$lambda.min

print("Here is one standard error value of λ for which the MSE is minimum")
cv.out$lambda.1se
```




```{r 671 Principal component regression}

library(tidyverse)
library(pls)


hitters.df = read.csv(
  "/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Hitters.csv", 
  header=T, stringsAsFactors = F, na.strings = "?")

# -------------- Usual clean up first ------------------#

# First remove all recods with spaces in character column Salary
hitters.df$Salary <- gsub('\\s+', '', hitters.df$Salary)

# Second remove all leading and trailing spaces from a character column "Salary"
hitters.df$Salary <- trimws(hitters.df$Salary, which = c("both"))

# Remove all records with "NA" or empty string in character column "Salary"
hitters.df <- hitters.df[!(tolower(hitters.df$Salary) == "na" | 
                           hitters.df$Salary == ""), ]

# convert Salary column to numberic
hitters.df$Salary <- as.numeric(as.character(hitters.df$Salary))

# convert all character fields 
hitters.df[sapply(hitters.df, is.character)] <- 
  lapply(hitters.df[sapply(hitters.df, is.character)], as.factor)

# Find and remove NA in all columns 
hitters.df <- na.omit(hitters.df)

set.seed(2)

# fit the model using pcr
pcr.fit <- pcr(Salary ~ ., data = hitters.df, scale = T, validation="CV")

print ("Summary:")
summary(pcr.fit)


# Note that although the minimum value of RSME is associated with 
# M = 16 (which is very close to 19) but for M = 7 we get a drastic
# decrease in RMSE which is very close to that of M = 17
# This suggsts M=7 gives us good enough model
validationplot(pcr.fit, val.type = "MSEP")

# Now to see how model with 7 works on test data an compare it with 
# model with M = 17 we split data into test and train and fit the model
# on train 

set.seed(1)
train <- train <- sample(1:nrow(hitters.df), nrow(hitters.df)/2)
test <- (-train)
pcr.fit <- pcr(Salary ~ ., data=hitters.df, subset=train, scale=T, validation="CV")
validationplot(pcr.fit, val.type = "MSEP")

# now let's find the lowest cross validation error occurs M = 7 on the  model
test.y <- hitters.df[test,]$Salary
pcr.pred <- predict(pcr.fit, hitters.df[test,], ncomp = 7)

sprintf("lowest MSE corresponding to M = 7 is %s (Ridge was 143257.45)",mean((pcr.pred-test.y)^2))
mean((pcr.pred-test.y)^2)
```

```{r  Principal Component Analysis before LDA in CV}

library(tidyverse)
library(pls)

weekly.df = 
  read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Weekly.csv", 
                      header=T, stringsAsFactors = F, na.strings = "?")
# -------------- Usual clean up first ------------------#

# First remove all recods with spaces in character column Direction
weekly.df$Direction <- gsub('\\s+', '', weekly.df$Direction)

# Second remove all leading and trailing spaces from a character column "Direction"
weekly.df$Direction <- trimws(weekly.df$Direction, which = c("both"))

# Remove all records with "NA" or empty string in character column "Direction"
weekly.df <- weekly.df[!(tolower(weekly.df$Direction) == "na" | 
                           weekly.df$Direction == ""), ]

# convert all character fields to factor
weekly.df[sapply(weekly.df, is.character)] <-
 lapply(weekly.df[sapply(weekly.df, is.character)], as.factor)

# Find and remove NA in all columns 
weekly.df <- na.omit(weekly.df)

# -------------------  outmost loop must be CV loop ---------------------
# Note that as per "Tibshirani" cross validation must always be before
# dimension reduction or feature selection
# -----------------------------------------------------------------------
set.seed(1)
k <- 10
threshold <- 0.5

folds <- sample(1:k, size = nrow(weekly.df), replace = T) 
table(folds)

# folds with same size
# sameSizefolds <- sample(rep(1:k, length.out = nrow(weekly.df)), 
#   size = nrow(weekly.df), replace = F)
# table(sameSizefolds)

results <- lapply (1:k, function(x) {  # x is the index of test portion, the rest are for training
 
  #------------- calculate all PCA scores on k-1 trining fold -----------------
  # First create predictor matrix and response vector on test and train folds
  predictor.train.X <- model.matrix(Direction~., weekly.df[folds != x,])[, -1]
  response.train.Y <- weekly.df[folds != x,]$Direction
  
  # use PCA to find principal components on k-1 trining fold:
  pca.train <- princomp(predictor.train.X, cor=T) # PCA using correlation matrix
  
  # make a tibble from PCAs
  weekly.pca.train <- as_tibble(pca.train$scores[,]*-1) %>% 
    add_column(Direction=response.train.Y)

  # Do exact same thing for test fold:
  predictor.test.X <- model.matrix(Direction~., weekly.df[folds == x,])[, -1]
  response.test.Y <- weekly.df[folds == x,]$Direction
  
  # use PCA to find principal components on test fold:
  pca.test <- princomp(predictor.test.X, cor=T) # PCA using correlation matrix
  
  # make a tibble from PCAs
  weekly.pca.test <- as_tibble(pca.test$scores[,]*-1) %>% 
    add_column(Direction=response.test.Y)
  
  # get list of all pca column names
  pca.cols <- colnames(weekly.pca.train[ , !(names(weekly.pca.train) %in% c("Direction"))])
  
  # An empty tibble to collect all result of running LDA on given test fold
  pca.results <- tibble (no.of.pcas = NULL, 
                      posterior.up = NULL,
                      posterior.down = NULL,
                      predicted = NULL,
                      real = NULL)
  
  for (pc.col.idx in 1:length(pca.cols)){
    
    pca.chosen.cols <- pca.cols[1:pc.col.idx]
    partial.weekly.pca.train <- weekly.pca.train[ ,pca.chosen.cols] %>% 
      add_column(Direction=response.train.Y)
  
    # fit on train fold
    lda.fit <- 
      MASS::lda(as.formula(paste("Direction~", 
                                 paste(pca.chosen.cols, collapse="+"))), 
                data = partial.weekly.pca.train, family = binomial)

    # predict on test fold
    lda.pred <- predict(lda.fit, weekly.pca.test, type =  "response")
    stopifnot(length (lda.pred$class) == length(weekly.pca.test$Direction))
    pca.results <- rbind (pca.results, tibble (no.of.pcas = pc.col.idx, 
                      posterior.up = lda.pred$posterior[, "Up"],
                      posterior.down = lda.pred$posterior[, "Down"],
                      predicted = lda.pred$class,
                      real = weekly.pca.test$Direction))
  }
  return (pca.results)
})

# We have to find average of missclassification rate for each number of PCs 
# cross all test folds

# first calculte missclassification rate per each number of PCAs


rates <- lapply(results, function (result) {
  return(result %>% 
           group_by(no.of.pcas) %>%
           summarise(MSE = mean(predicted != real), 
            FP_rates = table(predicted, real)[2,1]/(table(predicted, real)[2,1]+ table(predicted, real)[1,1]),
            TP_rates = table(predicted, real)[2,2]/(table(predicted, real)[2,2]+ table(predicted, real)[1,2]),
            precisions = table(predicted, real)[2,2]/(table(predicted, real)[2,2]+ table(predicted, real)[2,1]),
            specificities = table(predicted, real)[2,1]/(table(predicted, real)[2,1]+ table(predicted, real)[1,1]),
            nullClassifier = max( ( table(predicted, real)[1,1] + table(predicted, real)[2,1])/
                                    (table(predicted, real)[1,1] + table(predicted, real)[2,1] + 
                                       table(predicted, real)[1,2] + table(predicted, real)[2,2]),
                                  table(predicted, real)[1,2] + table(predicted, real)[2,2])/
                                    (table(predicted, real)[1,1] + table(predicted, real)[2,1] + 
                                       table(predicted, real)[1,2] + table(predicted, real)[2,2])
            ))
})

# Place rates for all folds in one df
(all.rates = do.call(rbind, rates))

# get mean of rates cross all folds per each mumber of pcs (1, 2, ..., 8) 
(all.rates %>%
  group_by(no.of.pcas) %>%
  summarise(MSE = mean(MSE), 
            FP_rates = mean(FP_rates),
            TP_rates = mean(TP_rates),
            precisions = mean(precisions),
            specificities = mean(specificities),
            nullClassifier = mean(nullClassifier))
)

# result shows 1 pca is the best , 2 or 3 number of PCAS areacceptable
```
```{r 672 partial least square}
library(tidyverse)
library(pls)


hitters.df = read.csv(
  "/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/Hitters.csv", 
  header=T, stringsAsFactors = F, na.strings = "?")

# -------------- Usual clean up first ------------------#

# First remove all recods with spaces in character column Salary
hitters.df$Salary <- gsub('\\s+', '', hitters.df$Salary)

# Second remove all leading and trailing spaces from a character column "Salary"
hitters.df$Salary <- trimws(hitters.df$Salary, which = c("both"))

# Remove all records with "NA" or empty string in character column "Salary"
hitters.df <- hitters.df[!(tolower(hitters.df$Salary) == "na" | 
                           hitters.df$Salary == ""), ]

# convert Salary column to numberic
hitters.df$Salary <- as.numeric(as.character(hitters.df$Salary))

# convert all character fields 
hitters.df[sapply(hitters.df, is.character)] <- 
  lapply(hitters.df[sapply(hitters.df, is.character)], as.factor)

# Find and remove NA in all columns 
hitters.df <- na.omit(hitters.df)

set.seed(2)

# fit the model using pls
pls.fit <- plsr(Salary ~ ., data = hitters.df, scale = T, validation="CV")

print ("Summary:")
summary(pcr.fit)

validationplot(pls.fit, val.type = "MSEP")

# Now to see how model with 7 works on test data an compare it with 
# model with M = 17 we split data into test and train and fit the model
# on train 

set.seed(1)
train <- sample(1:nrow(hitters.df), nrow(hitters.df)/2)
test <- (-train)
pls.fit <- plsr(Salary ~ ., data=hitters.df, subset=train, scale=T, validation="CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")

# now let's find the lowest cross validation error occurs M = 7 on the  model
test.y <- hitters.df[test,]$Salary
pls.pred <- predict(pls.fit, hitters.df[test,] %>% select(-Salary), ncomp = 7)

sprintf("lowest MSE corresponding to M = 7 is %s (Ridge was 143257.45)",mean((pls.pred-test.y)^2))
# PLSR really does not add that much value to PCR
mean((pls.pred-test.y)^2)

```
```{r Exercise 8}
library(tidyverse)
library(leaps)


X <- rnorm(100)
e <- rnorm(100)


Y <- .02 + 1.6*X -2.2 * X^2 + 5.9*X^3 + e

print("Part c:-----------------------------------------------------")

df <- tibble(x1 = X, x2=X^2, x3=X^3, x4=X^4, x5=X^5, x6=X^6, x7=X^7, 
             x8=X^8, x9=X^9, x10=X^10, y = Y)

regfit.full <- regsubsets(y ~ ., df, nvmax = 10)

#The summary shows the result of step 2 of algorithm 6.1 page 205 of the book 
summary <- summary(regfit.full)

plot(summary$rss, xlab = "Number of variables", ylab="RSS",type = "l")
# which.max() returns location maximum point of the vector
index <- which.max(summary$adjr2)
plot(summary$adjr2,xlab = "Number of variables", ylab="Adjusted Rsquared",
     type = "l")
points(index, summary$adjr2[index], col="red", cex=2, pch=20)
print("coefficients of the best model (adjr2) : ")
coef(regfit.full,index)
# which.min() returns location minimum point of the vector
index <- which.min(summary$cp)
plot(summary$cp,xlab = "Number of variables", ylab="cp", type = "l")
points(index, summary$cp[index], col="red", cex=2, pch=20)
print("coefficients of the best model (cp) : ")
coef(regfit.full,index)
# same for bic
plot(summary$bic, xlab =" Numbers of variables", ylab="Bic", type="l")
(index <- which.min(summary$bic))
points(index, summary$bic[index], col="red", cex=2, pch=20)
print("coefficients of the best model (bic) : ")
coef(regfit.full,index)

# coef(, n) returns coefficient estimates associated with best n variable model



print ("------------- use CV with best subset selection ------------------" )
set.seed(1)
k <- 10

folds <- sample(1:k, size = nrow(df), replace = T) 
table(folds)

# folds with same size
# sameSizefolds <- sample(rep(1:k, length.out = nrow(weekly.df)), 
#   size = nrow(weekly.df), replace = F)
# table(sameSizefolds)

results <- lapply (1:k, function(j) {  # x is the index of test portion, the rest are for training

  # this is to collect the MSEs for each test fold
  mses <- tibble(no.of.coefs = NULL, MSE = NULL)
  df.train <- df[folds != j, ]
  df.test <- df[folds == j, ]
  df.train.X <- df.train %>% select (-y)
  df.test.X <- df.test %>% select (-y)
  mat.test.X <- model.matrix(y~., data=df.test)
  df.test.Y <- df.test$y
  
  # step 2 of algorithm 6.1 page 205 of the book 
  regfit.full.train <- regsubsets(y ~ ., df.train, nvmax = ncol(df.train.X))
  
  # apply the model with selected subsets on test set 
  # one at a time and cacluate the MSE
  for (i in 1:ncol(df.test.X)){
    (coefi <- coef(regfit.full.train, id = i))
    (pred <- mat.test.X[, names(coefi)] %*% coefi)
    (mse <- mean((pred - df.test.Y)^2))
     mses <- rbind (mses, tibble(no.of.coefs = i, MSE = mse))
  }
  return(mses)
})

allResults <- results[[1]]
for (i in 2 : length(results)){
  allResults <- rbind(allResults , results[[i]])
}

(allMse <- (allResults %>%
  group_by(no.of.coefs) %>%
  summarise(mse.mean = mean(MSE))) )
(idx <- which.min(allMse$mse.mean))
print ("The best subset of features selected correspnd to minimum CV_MSE")
# train on the whole training set now
regfit.full.train <- regsubsets(y ~ ., df %>% select (-y), nvmax = ncol(df %>% select (-y)))
coef(regfit.full.train, id = idx)

#- -------------------------------------------------------------------------
print ("part d : Use forward stepwise selection to find the best selected subsets  ------------------" )
regfit.fwd <- regsubsets(y ~ ., df, nvmax = 10, method="forward")

#The summary shows the result of step 2 of algorithm 6.2 page 207 of the book 
summary <- summary(regfit.fwd)


plot(summary$rss, xlab = "Number of variables", ylab="RSS",type = "l")

# which.max() returns location maximum point of the vector
index <- which.max(summary$adjr2)
plot(summary$adjr2,xlab = "Number of variables", ylab="Adjusted Rsquared",
     type = "l")
points(index, summary$adjr2[index], col="red", cex=2, pch=20)
print("coefficients of the best model (adjr2) : ")
coef(regfit.fwd,index)
# which.min() returns location minimum point of the vector
index <- which.min(summary$cp)
plot(summary$cp, xlab =" Numbers of variables", ylab="Cp", type="l")
points(index, summary$cp[index], col="red", cex=2, pch=20)
print("coefficients of the best model (cp) : ")
coef(regfit.fwd,index)
# same for bic
index <- which.min(summary$bic)
plot(summary$bic, xlab =" Numbers of variables", ylab="Bic", type="l")
points(index, summary$bic[index], col="red", cex=2, pch=20)
print("coefficients of the best model (bic) : ")
coef(regfit.fwd,index)


print ("------------- use CV to find best forward stepwise selected model ------------------" )
set.seed(1)
k <- 10

folds <- sample(1:k, size = nrow(df), replace = T) 
table(folds)

# folds with same size
# sameSizefolds <- sample(rep(1:k, length.out = nrow(weekly.df)), 
#   size = nrow(weekly.df), replace = F)
# table(sameSizefolds)

results <- lapply (1:k, function(x) {  # x is the index of test portion, the rest are for training
  
  # this is to collect the MSEs for each test fold
  mses <- tibble(no.of.coefs = NULL, MSE = NULL)
  
  df.train <- df[folds != x, ]
  df.test <- df[folds == x, ]
  (df.train.X <- df.train %>% select (-y))
  (df.test.X <- df.test %>% select (-y))
  (mat.test.X <- model.matrix(y~., data=df.test))
  (df.test.Y <- df.test$y)
  
  # step 2 of algorithm 6.2 page 207 of the book 
  regfit.fwd.train <- regsubsets(y ~ ., df.train, 
                                 nvmax = ncol(df.train.X), method="forward")
  
  # apply the model with selected subsets on test set 
  # one at a time and cacluate the MSE
  for (i in 1:ncol(df.test.X)){
    (coefi <- coef(regfit.fwd.train, id = i))
    (pred <- mat.test.X[, names(coefi)] %*% coefi)
    (mse <- mean((pred - df.test.Y)^2))
     mses <- rbind (mses, tibble(no.of.coefs = i, MSE = mse))
  }
  return(mses)
})

allResults <- results[[1]]
for (i in 2 : length(results)){
  allResults <- rbind(allResults , results[[i]])
}

(allMse <- (allResults %>%
  group_by(no.of.coefs) %>%
  summarise(mse.mean = mean(MSE))) )
(idx <- which.min(allMse$mse.mean))
print ("The forward features selected correspnd to minimum CV_MSE")
# train on the whole training set now
regfit.fwd.train <- regsubsets(y ~ ., df %>% select (-y), 
                               nvmax = ncol(df %>% select (-y)),
                               method = "forward")
coef(regfit.fwd.train, id = idx)


#----------------------------------------------------------------------------
print("Part e: Fit lasso model and use cv to find the best value for lamda --------------------")
#------------------------------------------------------------------------------

library(glmnet)
set.seed(1)

# First construct matrix from dataframe (and drop intercept column)
x <- model.matrix(y~., df)[,-1] 
y <- df$y

cv.out=cv.glmnet(x, y, alpha=1, lambda = NULL)
plot(cv.out)

print("Here is value of λ for which the MSE is minimum")
(bestlam=cv.out$lambda.min)

print("Here are the coefficients corresponding to best value of λ:")
predict(cv.out, type="coefficients" ,s=bestlam )

print ( "Lasso coefficients are not as close as that of all feature selections !!")

print("Here is one standard error value of λ for which the MSE is minimum")
one.SE.lam <- cv.out$lambda.1se

print("Here are the coefficients corresponding to one standard error value of λ:")
predict(cv.out, type="coefficients" ,s=one.SE.lam )


#----------------------------------------------------------------------
print("Part f: Perform Best subset selection and lasso on new data ------------")
#-----------------------------------------------------------------------

Y <- 12 - 45.3 * X^7
df <- tibble(x1 = X, x2=X^2, x3=X^3, x4=X^4, x5=X^5, x6=X^6, x7=X^7, 
             x8=X^8, x9=X^9, x10=X^10, y = Y)
library(glmnet)
set.seed(1)

print ("Perform best subset selection: ")
regfit.full <- regsubsets(y ~ ., df, nvmax = 10)

#The summary shows the result of step 2 of algorithm 6.1 page 205 of the book 
summary <- summary(regfit.full)

plot(summary$rss, xlab = "Number of variables", ylab="RSS",type = "l")
# which.max() returns location maximum point of the vector
index <- which.max(summary$adjr2)
plot(summary$adjr2,xlab = "Number of variables", ylab="Adjusted Rsquared",
     type = "l")
points(index, summary$adjr2[index], col="red", cex=2, pch=20)
print("coefficients of the best model (adjr2) : ")
coef(regfit.full,index)
# which.min() returns location minimum point of the vector
index <- which.min(summary$cp)
plot(summary$cp,xlab = "Number of variables", ylab="cp", type = "l")
points(index, summary$cp[index], col="red", cex=2, pch=20)
print("coefficients of the best model (cp) : ")
coef(regfit.full,index)
# same for bic
plot(summary$bic, xlab =" Numbers of variables", ylab="Bic", type="l")
(index <- which.min(summary$bic))
points(index, summary$bic[index], col="red", cex=2, pch=20)
print("coefficients of the best model (bic) : ")
coef(regfit.full,index)

# coef(, n) returns coefficient estimates associated with best n variable model



print ("------ use CV with best subset selection for new data --------------" )
set.seed(1)
k <- 10

folds <- sample(1:k, size = nrow(df), replace = T) 
table(folds)

# folds with same size
# sameSizefolds <- sample(rep(1:k, length.out = nrow(weekly.df)), 
#   size = nrow(weekly.df), replace = F)
# table(sameSizefolds)

results <- lapply (1:k, function(x) {  # x is the index of test portion, the rest are for training
  
  # this is to collect the MSEs for each test fold
  mses <- tibble(no.of.coefs = NULL, MSE = NULL)
  df.train <- df[folds != x, ]
  df.test <- df[folds == x, ]
  df.train.X <- df.train %>% select (-y)
  df.test.X <- df.test %>% select (-y)
  (mat.test.X <- model.matrix(y~., data=df.test))
  (df.test.Y <- df[folds == x, ]$y)
  
  # step 2 of algorithm 6.1 page 205 of the book 
  regfit.full.train <- regsubsets(y ~ ., df.train, nvmax = ncol(df.train.X))
  
  # apply the model with selected subsets on test set 
  # one at a time and cacluate the MSE
  for (i in 1:ncol(df.test.X)){
    (coefi <- coef(regfit.full.train, id = i))
    (pred <- mat.test.X[, names(coefi)] %*% coefi)
    (mse <- mean((pred - df.test.Y)^2))
     mses <- rbind (mses, tibble(no.of.coefs = i, MSE = mse))
  }
  return(mses)
})

allResults <- results[[1]]
for (i in 2 : length(results)){
  allResults <- rbind(allResults , results[[i]])
}

(allMse <- (allResults %>%
  group_by(no.of.coefs) %>%
  summarise(mse.mean = mean(MSE))) )
(idx <- which.min(allMse$mse.mean))
print ("The best subset of features selected correspnd to minimum CV_MSE")
# train on the whole training set now
regfit.full.train <- regsubsets(y ~ ., df %>% select (-y), nvmax = ncol(df %>% select (-y)))
coef(regfit.full.train, id = idx)

print(" Clearly applying CV on Best subset selection provides nonsensical result.")

print ("Apply Lasso  cross validation to find the best λ and corresponding coeffs")
library(glmnet)
set.seed(1)

# First construct matrix from dataframe (and drop intercept column)
x <- model.matrix(y~., df)[,-1] 
y <- df$y

cv.out=cv.glmnet(x, y, alpha=1, lambda = NULL)
plot(cv.out)

print("Here is value of λ for which the MSE is minimum")
(bestlam=cv.out$lambda.min)

print("Here are the coefficients corresponding to best value of λ:")
predict(cv.out, type="coefficients" ,s=bestlam )

print("Here is one standard error value of λ for which the MSE is minimum")
one.SE.lam <- cv.out$lambda.1se

print("Here are the coefficients corresponding to one standard error value of λ:")
predict(cv.out, type="coefficients" ,s=one.SE.lam )


```

```{r Exercise 9}
library(tidyverse)
library(glmnet)
library(pls)
library(leaps)

set.seed(1)
college.df = read.csv("/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/College.csv", 
                      header=T, stringsAsFactors = F, na.strings = "?")
college.df = tibble(college.df)

# str(college.df)

#-------- Some usual cleaning on character columns ---------------- #

# First remove all recods with spaces in character column Private
college.df$Private <- gsub('\\s+', '', college.df$Private)

# Second remove all leading and trailing spaces from a character column "Private"
college.df$Private <- trimws(college.df$Private, which = c("both"))

# Remove all records with "NA" or empty string in character column "Private"
college.df <- college.df[!(tolower(college.df$Private) == "na" | 
                           college.df$Private == ""), ]

# convert all character fields 
college.df[sapply(college.df, is.character)] <- 
  lapply(college.df[sapply(college.df, is.character)], as.factor)

#----------- Find and remove NA in all columns ------------- #
college.df <- na.omit(college.df)

# str(college.df)
set.seed(1)
print("a: Split into train / test data sets ---------------------")
train <- sample(1:nrow(college.df), nrow(college.df)/2)
test <- (-train)

train.df <- college.df[train, ]
test.df <- college.df[test, ]
y.train <- train.df$Apps
y.test <- test.df$Apps

print("b: fit a linear model ------------------------------------")
# fit a model
df.lm <- lm (Apps ~ ., data = train.df)

# Now predict  Apps for test data
pred.lm <- predict(df.lm, test.df)

print("Linear model test MSE:")
(lm.MSE <- mean( (pred.lm - y.test)^2))

print("c: fit Ridge regression model on training set and get test.mse-------------")

# use magic model.matrix to convert dataframe into a matrix for Ridge and Lasso
x.train <- model.matrix(Apps~., train.df)[, -1]
x.test <- model.matrix(Apps~., test.df)[, -1]


cv.out <- cv.glmnet(x.train, y.train, alpha=0) # Ridge
plot(cv.out)
best.lambda <- cv.out$lambda.min

# predict the model on test
pred.ridge <- predict(cv.out, s=best.lambda, newx=x.test)

sprintf("Ridge test mse for best lambda: %s", best.lambda)
(ridge.mse <- mean((pred.ridge - y.test)^2))

print("d: fit Lasso regression model on training set and get test.mse and coeffs")

cv.out <- cv.glmnet(x.train, y.train, alpha=1) # Lasso
plot(cv.out)
best.lambda <- cv.out$lambda.min

# predict the model on test
pred.lasso <- predict(cv.out, s=best.lambda, newx=x.test)

sprintf("Lasso test mse for best lambda: %s", best.lambda)
(lasso.mse <- mean((pred.lasso - y.test)^2))

print("Coeffs for Lasso: ")
(coefs.ridge <- predict (cv.out, type = "coefficients", s = best.lambda))

print("e: Fit a PCR model on training set with M chosen by CS and get test.mse")

pcr.fit <- pcr (Apps~., data=train.df, scale=T, validation="CV")


validationplot(pcr.fit, val.type="MSEP")

print("Minimum CV Root MSE is for M=17 components which is 100 so CV MSE is 10000")
print("Looking at validation plot we see that M = 15 or 16  should suffice")

# Now apply model 1ith M=17 on test data and calculate MSE'
M = 17
pcr.pred <- predict(pcr.fit, x.test, ncomp = M)
sprintf("pcr test cv_mse for when best number of component is: %s", M)
(pcr.mse <- mean((pcr.pred - y.test)^2))
sprintf("pcr test mse  for best number of component: %s is %s:", M, pcr.mse)


print("f: Fit a PLS model on training set with M chosen by CS and get test.mse")

pls.fit <- plsr(Apps ~ ., data = college.df, subset = train, scale=T, validation = "CV")
validationplot(pls.fit, val.type="MSEP")

print("Minimum CV Root MSE is for M=13 components which is 1118")

# Now apply model 1ith M=13 on test data and calculate MSE'
M = 13
pls.pred <- predict(pls.fit, x.test, ncomp = M)

(pls.mse <- mean((pls.pred - y.test)^2))
sprintf("pls test mse for for best number of component: %s is %s:", M, pls.mse)

```

```{r Exercise 10}
library(tidyverse)
library(glmnet)
library(pls)
library(leaps)

print("a) Generate a data set with p = 20 features, n = 1,000")

set.seed(10)
X <- matrix(rep(NA, 1000 * 20), c(1000,20))
for (i in 1: 20)
  X[, i] <- rnorm(1000)

e <- rnorm(1000)

beta <- c(12,0,2.6, -123,0,11.2,56,-7,0,0,0,13,-41,2.2,0,8.7, -18,0,19,0.03)

# name the features
feature.names <- c("x1", "x2", "x3", "x4", "x5", "x6", "x7", "x8", "x9", "x10", 
    "x11", "x12", "x13", "x14", "x15", "x16", "x17", "x18", "x19", "x20" )
colnames(X) <- feature.names
names(beta) <- feature.names  
beta

Y <- rep(NA, 1000)

for (i in 1 : 1000)
  Y[[i]] <- beta %*% X[i, ] + e[[i]]

# now split the samples into test and training:
print("b) Split the data into train setcontaining 100 and test set containing 900")

train <- sample(1:nrow(X), nrow(X)/10)
test <- (-train)

train.x <- X[train, ]
train.y <- Y[train]

test.x <- X[test, ]
test.y <- Y[test]

df.train.X <- as_tibble(train.x)
df.train <- df.train.X %>% add_column(y = train.y, .before = "x1")
df.test.X <- as_tibble(test.x)
df.test <- df.test.X %>% add_column(y = test.y, .before = "x1")

print("c) Perform best subset selection on training set:")
regfit.full <- regsubsets(y ~ ., df.train, nvmax = 20)

#The summary shows the result of step 2 of algorithm 6.1 page 205 of the book 
summary <- summary(regfit.full)

plot(summary$rss, xlab = "Number of variables", ylab="RSS",type = "l")

print("d) Plot test MSE for model of each size")

# First prepare test data as matrix, this is only for convinience 
# when using dot product %*%: coefficients  %*% test.X rows
df.test.mat <- model.matrix(y~., data = df.test)

#  loop through all models , get the coefficients and calculate MSE
mses <- rep(NA, 20)

for (i in 1:20){
  coeffs <- coef(regfit.full, i)
  yhat <- df.test.mat[, names(coeffs)] %*% coeffs
  mses[i]  <- mean((test.y - yhat)^2)
}

(mse_data <- tibble(comps = 1:20, mse=mses))
ggplot(mse_data, aes(comps, mse))+
  geom_line(color="blue")

print("e) For model sise with 12 components MSE is minimized: 12 --> 1.234475")


print("f) How does the model at which the test MSE is moinimized? compare the coeffs")

(coeffs <- coef(regfit.full, 12))
beta

print("comparision shows all zero coefficients successfully predicted and non zero
      coefficients are close approximation to real ones, only the 20th 
      coefficient is missing ")


print("
 (Intercept)        x1           x3           x4         
  0.1211867  11.9871248    2.6070986 -123.0928886      
                 12.00       2.60      -123.00                     
             
                 x6           x7           x8      
             10.9639610   56.0180802   -6.9507468  
                11.20       56.00         -7.00    
                
                 x12          x13          x14     
             13.1406636  -40.8673157    2.3919952  
                 13.00      -41.00         2.20        
                 
                 
                x16          x17          x19                
             8.6684323  -17.9409391   18.9294841         
               8.70       -18.00       19.00        0.03 
")


print("g) create a plot displaying ...")

# loop over all models
distance.squared <- rep(0, 20)

for (i in 1:20){
  (coeffs <- coef(regfit.full, i)[-1]) # drop the intercept
  for (j in 1:i)
    (distance.squared[i] <- distance.squared[i] + (coeffs [j] - beta[names(coeffs [j])])^2 )
}
dist <- sqrt(distance.squared)
(distance.vs.model <- tibble(model = 1:20, distance = dist))

ggplot(distance.vs.model, aes(model, distance))+
  geom_line(color="red")

print("As graph shows on model with 12 components (the model we found in part d) 
the distance between two sets of coefficients are minimum")
```

```{r Exercise 11}
library(tidyverse)
library(glmnet)
library(pls)
library(leaps)


boston.df = read.csv(
  "/Users/shahrdadshadab/env/my-R-project/ISLR/Data/datasets/BostonHousing.csv", 
  header=T, stringsAsFactors = F, na.strings = "?")

str(boston.df)

# Find and remove NA in all columns 
boston.df <- na.omit(boston.df)
set.seed(1)
print("Split into train / test data sets ---------------------")
train <- sample(1:nrow(boston.df), nrow(boston.df)/2)
test <- (-train)

df.train <- boston.df[train, ]
df.test <- boston.df[test, ]
train.y <- df.train$crim
test.y <- df.test$crim



print ("Perform best subset selection on train data: ")
regfit.full <- regsubsets(crim ~ ., df.train, nvmax = 10)

#The summary shows the result of step 2 of algorithm 6.1 page 205 of the book 
summary <- summary(regfit.full)

plot(summary$rss, xlab = "Number of variables", ylab="RSS",type = "l")
# which.max() returns location maximum point of the vector
index <- which.max(summary$adjr2)
plot(summary$adjr2,xlab = "Number of variables", ylab="Adjusted Rsquared",
     type = "l")
points(index, summary$adjr2[index], col="red", cex=2, pch=20)
print("coefficients of the best model (adjr2) : ")
coef(regfit.full,index)
# which.min() returns location minimum point of the vector
index <- which.min(summary$cp)
plot(summary$cp,xlab = "Number of variables", ylab="cp", type = "l")
points(index, summary$cp[index], col="red", cex=2, pch=20)
print("coefficients of the best model (cp) : ")
coef(regfit.full,index)
# same for bic
plot(summary$bic, xlab =" Numbers of variables", ylab="Bic", type="l")
(index <- which.min(summary$bic))
points(index, summary$bic[index], col="red", cex=2, pch=20)
print("coefficients of the best model (bic) : ")
coef(regfit.full,index)

print("looks a subset of three variables provides a model with acceptable performance")


print ("------ use CV with best subset selection  --------------" )

k <- 10

folds <- sample(1:k, size = nrow(df), replace = T) 
table(folds)

# folds with same size
# sameSizefolds <- sample(rep(1:k, length.out = nrow(weekly.df)), 
#   size = nrow(weekly.df), replace = F)
# table(sameSizefolds)

results <- lapply (1:k, function(x) {  # x is the index of test portion, the rest are for training
  
  # this is to collect the MSEs for each test fold
  mses <- tibble(no.of.coefs = NULL, MSE = NULL)
  df.train.cv <- df.train[folds != x, ]
  df.test.cv <- df.train[folds == x, ]
  df.train.cv.X <- df.train.cv %>% select (-crim)
  df.test.cv.X <- df.test.cv %>% select (-crim)
  (mat.test.cv.X <- model.matrix(crim~., data=df.test.cv))
  (df.test.cv.Y <- df.test.cv$crim)
  
  # step 2 of algorithm 6.1 page 205 of the book 
  regfit.full.train <- regsubsets(crim ~ ., df.train.cv, 
                                  nvmax = ncol(df.train.cv.X))
  
  # apply the model with selected subsets on test set 
  # one at a time and cacluate the MSE on test fold
  for (i in 1:ncol(df.test.cv.X)){
    (coefi <- coef(regfit.full.train, id = i))
    (pred <- mat.test.cv.X[, names(coefi)] %*% coefi)
    (mse <- mean((pred - df.test.cv.Y)^2))
     mses <- rbind (mses, tibble(no.of.coefs = i, MSE = mse))
  }
  return(mses)
})

allResults <- results[[1]]
for (i in 2 : length(results)){
  allResults <- rbind(allResults , results[[i]])
}

(allMse <- (allResults %>%
  group_by(no.of.coefs) %>%
  summarise(mse.mean = mean(MSE))) )

print ("The best subset of features selected correspnd to minimum CV_MSE:")
(idx <- which.min(allMse$mse.mean))

print("However looking at the data shows sebset of features with 9 components 
      is very close to the minimum we found so we use it")

M=9
sprintf(" Now apply the the model with %s No. of components to test data:", idx)
coefi <- coef(regfit.full.train, id = M)

mat.df.test <- model.matrix(crim~., df.test)
yhat <- mat.df.test[, names(coefi)] %*% coefi

mse.best.subset <- mean ((test.y - yhat)^2)
sprintf( "------------ MSE on test data for best model with 8 component: %s", 
         mse.best.subset)


print("c: fit Ridge regression model on training set and get test.mse-------------")

# use magic model.matrix to convert dataframe into a matrix for Ridge and Lasso
x.train <- model.matrix(crim~., df.train)[, -1]
y.train <- (df.train)$crim
x.test <- model.matrix(crim~., df.test)[, -1]
y.test <- df.test$crim

cv.out <- cv.glmnet(x.train, y.train, alpha=0) # Ridge
plot(cv.out)
best.lambda <- cv.out$lambda.min

# predict the model on test
pred.ridge <- predict(cv.out, s=best.lambda, newx=x.test)

sprintf("Ridge test cv_mse for best lambda: %s", best.lambda)
ridge.mse <- mean((pred.ridge - y.test)^2)

sprintf( "-----------Ridge MSE on test data for model with best lambda %s is %s", 
         best.lambda, ridge.mse)

print("fit Lasso regression model on training set and get test.mse ")

cv.out <- cv.glmnet(x.train, y.train, alpha=1) # Lasso
plot(cv.out)
best.lambda <- cv.out$lambda.min

# predict the model on test
pred.lasso <- predict(cv.out, s=best.lambda, newx=x.test)

sprintf("Lasso test cv_mse for best lambda: %s", best.lambda)
lasso.mse <- mean((pred.lasso - y.test)^2)

sprintf( "-----------Lasso MSE on test data for model with best lambda %s is %s", 
         best.lambda, lasso.mse)


print("Fit a PCR model on training set with M chosen by CS and get test.mse")

pcr.fit <- pcr (crim~., data=df.train, scale=T, validation="CV")

validationplot(pcr.fit, val.type="MSEP")

print("Minimum CV Root MSE is for M=4 components which is 50 so CV MSE is 2500")

# Now apply model 1ith M=17 on test data and calculate MSE'
M = 4
pcr.pred <- predict(pcr.fit, x.test, ncomp = M)
pcr.mse <- mean((pcr.pred - y.test)^2)
sprintf("pcr test mse  for best number of component: %s is %s:", M, pcr.mse)

sprintf("b) best subset selection has the best test MSE: %s 
        comparing with Ridge test MSE %s, Lasso test MSE %s 
        and PCR test %s MSEtherefore we prefer it to other three",
        mse.best.subset, ridge.mse, lasso.mse, pcr.mse)

sprintf("c) Best  subset selection only includes %s number of components
        probably because other features add noise",M)
```


```{r createDataPartition bootstrap sample and other samples}

# createDataPartition(
#   y,
#   times = 1,
#   p = 0.5,
#   list = TRUE,
#   groups = min(5, length(y))
# )
# createFolds(y, k = 10, list = TRUE, returnTrain = FALSE)
# 
# createMultiFolds(y, k = 10, times = 5)
# 
# createTimeSlices(y, initialWindow, horizon = 1, fixedWindow = TRUE, skip = 0)
# 
# groupKFold(group, k = length(unique(group)))
# 
# createResample(y, times = 10, list = TRUE)
# =============================================================================
# Arguments
# y: a vector of outcomes. For createTimeSlices, these should be in 
#    chronological order.
# times: the number of partitions to create
# p : the percentage of data that goes to training
# list : logical - should the results be in a list (TRUE) or a matrix with the 
#       number of rows equal to floor(p * length(y)) and times columns.
# groups: for numeric y, the number of breaks in the quantiles (see below)
# k: an integer for the number of folds.
# returnTrain : a logical. When true, the values returned are the sample 
#               positions corresponding to the data used during training. 
#               This argument only works in conjunction with list = TRUE
# initialWindow: Initial number of consecutive values in each training set sample
# horizon: Number of consecutive values in test set sample
# fixedWindow: logical, if FALSE, all training samples start at 1
# skip: integer, how many (if any) resamples to skip to thin the total amount
# group: a vector of groups whose length matches the number of rows in the 
#        overall data set.


library(tidyverse)
library(caret)
library(glmnet)


data(oil)
createDataPartition(oilType, 2)

x <- rgamma(50, 3, .5)
inA <- createDataPartition(x, list = FALSE)

plot(density(x[inA]))
rug(x[inA])

points(density(x[-inA]), type = "l", col = 4)
rug(x[-inA], col = 4)

createResample(oilType, 2)

createFolds(oilType, 10)
createFolds(oilType, 5, FALSE)

createFolds(rnorm(21))

createTimeSlices(1:9, 5, 1, fixedWindow = FALSE)
createTimeSlices(1:9, 5, 1, fixedWindow = TRUE)
createTimeSlices(1:9, 5, 3, fixedWindow = TRUE)
createTimeSlices(1:9, 5, 3, fixedWindow = FALSE)

createTimeSlices(1:15, 5, 3)
createTimeSlices(1:15, 5, 3, skip = 2)
createTimeSlices(1:15, 5, 3, skip = 3)

set.seed(131)
groups <- sort(sample(letters[1:4], size = 20, replace = TRUE))
table(groups)
folds <- groupKFold(groups)
lapply(folds, function(x, y) table(y[x]), y = groups)



```

